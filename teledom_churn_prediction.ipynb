{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект Телеком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, accuracy_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Извлечение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schemaname</th>\n",
       "      <th>tablename</th>\n",
       "      <th>tableowner</th>\n",
       "      <th>tablespace</th>\n",
       "      <th>hasindexes</th>\n",
       "      <th>hasrules</th>\n",
       "      <th>hastriggers</th>\n",
       "      <th>rowsecurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>steel</td>\n",
       "      <td>data_arc</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>steel</td>\n",
       "      <td>data_bulk</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steel</td>\n",
       "      <td>data_bulk_time</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>steel</td>\n",
       "      <td>data_gas</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steel</td>\n",
       "      <td>data_temp</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>steel</td>\n",
       "      <td>data_wire</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>steel</td>\n",
       "      <td>data_wire_time</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>telecom</td>\n",
       "      <td>contract</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>telecom</td>\n",
       "      <td>internet</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>telecom</td>\n",
       "      <td>personal</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>telecom</td>\n",
       "      <td>phone</td>\n",
       "      <td>praktikum_admin</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   schemaname       tablename       tableowner tablespace  hasindexes  \\\n",
       "0       steel        data_arc  praktikum_admin       None       False   \n",
       "1       steel       data_bulk  praktikum_admin       None       False   \n",
       "2       steel  data_bulk_time  praktikum_admin       None       False   \n",
       "3       steel        data_gas  praktikum_admin       None       False   \n",
       "4       steel       data_temp  praktikum_admin       None       False   \n",
       "5       steel       data_wire  praktikum_admin       None       False   \n",
       "6       steel  data_wire_time  praktikum_admin       None       False   \n",
       "7     telecom        contract  praktikum_admin       None       False   \n",
       "8     telecom        internet  praktikum_admin       None       False   \n",
       "9     telecom        personal  praktikum_admin       None       False   \n",
       "10    telecom           phone  praktikum_admin       None       False   \n",
       "\n",
       "    hasrules  hastriggers  rowsecurity  \n",
       "0      False        False        False  \n",
       "1      False        False        False  \n",
       "2      False        False        False  \n",
       "3      False        False        False  \n",
       "4      False        False        False  \n",
       "5      False        False        False  \n",
       "6      False        False        False  \n",
       "7      False        False        False  \n",
       "8      False        False        False  \n",
       "9      False        False        False  \n",
       "10     False        False        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Конфигурация базы данных\n",
    "db_config = {\n",
    "    'user': 'praktikum_student',\n",
    "    'pwd': 'Sdf4$2;d-d30pp',\n",
    "    'host': 'rc1b-wcoijxj3yxfsf3fs.mdb.yandexcloud.net',\n",
    "    'port': 6432,\n",
    "    'db': 'data-science-final'\n",
    "}\n",
    "\n",
    "# Создадим подключение к базе данных\n",
    "connection_string = (\n",
    "    f\"postgresql://{db_config['user']}:{db_config['pwd']}@\"\n",
    "    f\"{db_config['host']}:{db_config['port']}/{db_config['db']}\"\n",
    ")\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Посмотрим какие схемы и таблицы есть в этой базе данных\n",
    "query = \"SELECT * FROM pg_catalog.pg_tables WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema';\"\n",
    "available_tables = pd.read_sql_query(query, con=engine)\n",
    "display(available_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>BeginDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Type</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4472-LVYGI</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>None</td>\n",
       "      <td>Two year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>52.55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3115-CZMZD</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>None</td>\n",
       "      <td>Two year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>20.25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5709-LVOEQ</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>None</td>\n",
       "      <td>Two year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>80.85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID   BeginDate     EndDate            Type PaperlessBilling  \\\n",
       "0  4472-LVYGI  2020-02-01        None        Two year              Yes   \n",
       "1  3115-CZMZD  2020-02-01        None        Two year               No   \n",
       "2  3668-QPYBK  2018-08-09  2019-12-01  Month-to-month              Yes   \n",
       "3  5709-LVOEQ  2020-02-01        None        Two year               No   \n",
       "4  9237-HQITU  2019-01-26  2019-11-01  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod  MonthlyCharges  TotalCharges  \n",
       "0  Bank transfer (automatic)           52.55           NaN  \n",
       "1               Mailed check           20.25           NaN  \n",
       "2               Mailed check           53.85        108.15  \n",
       "3               Mailed check           80.85           NaN  \n",
       "4           Electronic check           70.70        151.65  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents\n",
       "0  7590-VHVEG  Female              0     Yes         No\n",
       "1  5575-GNVDE    Male              0      No         No\n",
       "2  3668-QPYBK    Male              0      No         No\n",
       "3  7795-CFOCW    Male              0      No         No\n",
       "4  9237-HQITU  Female              0      No         No"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customerID</td>\n",
       "      <td>InternetService</td>\n",
       "      <td>OnlineSecurity</td>\n",
       "      <td>OnlineBackup</td>\n",
       "      <td>DeviceProtection</td>\n",
       "      <td>TechSupport</td>\n",
       "      <td>StreamingTV</td>\n",
       "      <td>StreamingMovies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           c1               c2              c3            c4  \\\n",
       "0  customerID  InternetService  OnlineSecurity  OnlineBackup   \n",
       "1  7590-VHVEG              DSL              No           Yes   \n",
       "2  5575-GNVDE              DSL             Yes            No   \n",
       "3  3668-QPYBK              DSL             Yes           Yes   \n",
       "4  7795-CFOCW              DSL             Yes            No   \n",
       "\n",
       "                 c5           c6           c7               c8  \n",
       "0  DeviceProtection  TechSupport  StreamingTV  StreamingMovies  \n",
       "1                No           No           No               No  \n",
       "2               Yes           No           No               No  \n",
       "3                No           No           No               No  \n",
       "4               Yes          Yes           No               No  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customerID</td>\n",
       "      <td>MultipleLines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9305-CDSKC</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           c1             c2\n",
       "0  customerID  MultipleLines\n",
       "1  5575-GNVDE             No\n",
       "2  3668-QPYBK             No\n",
       "3  9237-HQITU             No\n",
       "4  9305-CDSKC            Yes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Функция загрузки данных из таблицы\n",
    "def load_data(schema, table_name):\n",
    "    query = f\"SELECT * FROM {schema}.{table_name}\"\n",
    "    return pd.read_sql_query(query, con=engine)\n",
    "\n",
    "# Загрузим данные из каждой таблицы в схеме «telecom».\n",
    "contract_data = load_data('telecom', 'contract')\n",
    "personal_data = load_data('telecom', 'personal')\n",
    "internet_data = load_data('telecom', 'internet')\n",
    "phone_data = load_data('telecom', 'phone')\n",
    "\n",
    "# Отобразим первые несколько строк одной из таблиц в качестве примера.\n",
    "display(contract_data.head())\n",
    "display(personal_data.head())\n",
    "display(internet_data.head())\n",
    "display(phone_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Исследовательский Анализ и Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропущенные значения в каждом наборе данных:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_data</th>\n",
       "      <th>contract_data</th>\n",
       "      <th>internet_data</th>\n",
       "      <th>phone_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BeginDate</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EndDate</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5174.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partner</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaymentMethod</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalCharges</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerID</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  personal_data  contract_data  internet_data  phone_data\n",
       "BeginDate                   NaN            0.0            NaN         NaN\n",
       "Dependents                  0.0            NaN            NaN         NaN\n",
       "EndDate                     NaN         5174.0            NaN         NaN\n",
       "MonthlyCharges              NaN            0.0            NaN         NaN\n",
       "PaperlessBilling            NaN            0.0            NaN         NaN\n",
       "Partner                     0.0            NaN            NaN         NaN\n",
       "PaymentMethod               NaN            0.0            NaN         NaN\n",
       "SeniorCitizen               0.0            NaN            NaN         NaN\n",
       "TotalCharges                NaN           11.0            NaN         NaN\n",
       "Type                        NaN            0.0            NaN         NaN\n",
       "c1                          NaN            NaN            0.0         0.0\n",
       "c2                          NaN            NaN            0.0         0.0\n",
       "c3                          NaN            NaN            0.0         NaN\n",
       "c4                          NaN            NaN            0.0         NaN\n",
       "c5                          NaN            NaN            0.0         NaN\n",
       "c6                          NaN            NaN            0.0         NaN\n",
       "c7                          NaN            NaN            0.0         NaN\n",
       "c8                          NaN            NaN            0.0         NaN\n",
       "customerID                  0.0            0.0            NaN         NaN\n",
       "gender                      0.0            NaN            NaN         NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Типы данных в каждом наборе данных:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_data</th>\n",
       "      <th>contract_data</th>\n",
       "      <th>internet_data</th>\n",
       "      <th>phone_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BeginDate</th>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EndDate</th>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <td>NaN</td>\n",
       "      <td>float64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partner</th>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PaymentMethod</th>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <td>int64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalCharges</th>\n",
       "      <td>NaN</td>\n",
       "      <td>float64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerID</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>object</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 personal_data contract_data internet_data phone_data\n",
       "BeginDate                  NaN        object           NaN        NaN\n",
       "Dependents              object           NaN           NaN        NaN\n",
       "EndDate                    NaN        object           NaN        NaN\n",
       "MonthlyCharges             NaN       float64           NaN        NaN\n",
       "PaperlessBilling           NaN        object           NaN        NaN\n",
       "Partner                 object           NaN           NaN        NaN\n",
       "PaymentMethod              NaN        object           NaN        NaN\n",
       "SeniorCitizen            int64           NaN           NaN        NaN\n",
       "TotalCharges               NaN       float64           NaN        NaN\n",
       "Type                       NaN        object           NaN        NaN\n",
       "c1                         NaN           NaN        object     object\n",
       "c2                         NaN           NaN        object     object\n",
       "c3                         NaN           NaN        object        NaN\n",
       "c4                         NaN           NaN        object        NaN\n",
       "c5                         NaN           NaN        object        NaN\n",
       "c6                         NaN           NaN        object        NaN\n",
       "c7                         NaN           NaN        object        NaN\n",
       "c8                         NaN           NaN        object        NaN\n",
       "customerID              object        object           NaN        NaN\n",
       "gender                  object           NaN           NaN        NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Проверка пропущенных значений и типов данных в каждой таблице\n",
    "missing_values = {\n",
    "    \"personal_data\": personal_data.isnull().sum(),\n",
    "    \"contract_data\": contract_data.isnull().sum(),\n",
    "    \"internet_data\": internet_data.isnull().sum(),\n",
    "    \"phone_data\": phone_data.isnull().sum()\n",
    "}\n",
    "\n",
    "data_types = {\n",
    "    \"personal_data\": personal_data.dtypes,\n",
    "    \"contract_data\": contract_data.dtypes,\n",
    "    \"internet_data\": internet_data.dtypes,\n",
    "    \"phone_data\": phone_data.dtypes\n",
    "}\n",
    "\n",
    "# Создание DataFrames для отсутствующих значений и типов данных\n",
    "missing_values_df = pd.DataFrame(missing_values)\n",
    "data_types_df = pd.DataFrame(data_types)\n",
    "\n",
    "# Отобразим DataFrames\n",
    "print(\"Пропущенные значения в каждом наборе данных:\")\n",
    "display(missing_values_df)\n",
    "print(\"\\nТипы данных в каждом наборе данных:\")\n",
    "display(data_types_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Отсутствующие значения**\n",
    "\n",
    "1. **Личные данные**: пропущенных значений нет.\n",
    "2. **Данные контракта**: отсутствуют значения в полях «**EndDate**» и «**TotalCharges**».  \n",
    "     - Отсутствующие значения в поле **EndDate**, скорее всего, указывают на клиентов, которые все еще активны (т. е. не ушли). Их следует обрабатывать соответствующим образом при создании целевой переменной.  \n",
    "     - Необходимо изучить и обработать недостающие значения в TotalCharges.\n",
    "\n",
    "\n",
    "3. **Данные Интернета**: пропущенных значений нет, но имена столбцов необходимо переименовать для ясности.\n",
    "4. **Данные телефона**: пропущенных значений нет, но названия столбцов необходимо переименовать для ясности.\n",
    "\n",
    "**Типы данных**\n",
    "\n",
    "- '**BeginDate**' и '**EndDate**' в данных контракта должны быть преобразованы в дату и время.\n",
    "- '**TotalCharges**' в данных контракта должен иметь числовой тип (float).\n",
    "- Столбец «**SeniorCitizen**» в личных данных, хотя и числовой, представляет собой двоичную категорию и может рассматриваться как категориальная переменная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>BeginDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Type</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4472-LVYGI</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Two year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>52.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3115-CZMZD</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Two year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>20.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5709-LVOEQ</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Two year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>80.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  BeginDate    EndDate            Type PaperlessBilling  \\\n",
       "0  4472-LVYGI 2020-02-01        NaT        Two year              Yes   \n",
       "1  3115-CZMZD 2020-02-01        NaT        Two year               No   \n",
       "2  3668-QPYBK 2018-08-09 2019-12-01  Month-to-month              Yes   \n",
       "3  5709-LVOEQ 2020-02-01        NaT        Two year               No   \n",
       "4  9237-HQITU 2019-01-26 2019-11-01  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod  MonthlyCharges  TotalCharges  Churn  \n",
       "0  Bank transfer (automatic)           52.55           NaN      0  \n",
       "1               Mailed check           20.25           NaN      0  \n",
       "2               Mailed check           53.85        108.15      1  \n",
       "3               Mailed check           80.85           NaN      0  \n",
       "4           Electronic check           70.70        151.65      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customerID</td>\n",
       "      <td>InternetService</td>\n",
       "      <td>OnlineSecurity</td>\n",
       "      <td>OnlineBackup</td>\n",
       "      <td>DeviceProtection</td>\n",
       "      <td>TechSupport</td>\n",
       "      <td>StreamingTV</td>\n",
       "      <td>StreamingMovies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  InternetService  OnlineSecurity  OnlineBackup  \\\n",
       "0  customerID  InternetService  OnlineSecurity  OnlineBackup   \n",
       "1  7590-VHVEG              DSL              No           Yes   \n",
       "2  5575-GNVDE              DSL             Yes            No   \n",
       "3  3668-QPYBK              DSL             Yes           Yes   \n",
       "4  7795-CFOCW              DSL             Yes            No   \n",
       "\n",
       "   DeviceProtection  TechSupport  StreamingTV  StreamingMovies  \n",
       "0  DeviceProtection  TechSupport  StreamingTV  StreamingMovies  \n",
       "1                No           No           No               No  \n",
       "2               Yes           No           No               No  \n",
       "3                No           No           No               No  \n",
       "4               Yes          Yes           No               No  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>MultipleLines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customerID</td>\n",
       "      <td>MultipleLines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9305-CDSKC</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  MultipleLines\n",
       "0  customerID  MultipleLines\n",
       "1  5575-GNVDE             No\n",
       "2  3668-QPYBK             No\n",
       "3  9237-HQITU             No\n",
       "4  9305-CDSKC            Yes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Обработка пропущенных значений в TotalCharges\n",
    "# Преобразование TotalCharges в числовое значение, установка error='coerce' для преобразования проблемных значений в NaN\n",
    "contract_data['TotalCharges'] = pd.to_numeric(contract_data['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Заполнение пропущенных значений в TotalCharges значением MonthlyCharges для клиентов с значением BeginDate, равным\n",
    "# дате извлечения данных (предполагается, что это новые клиенты)\n",
    "contract_data.loc[(contract_data['TotalCharges'].isnull()) & (contract_data['BeginDate'] == '2020-02-01'), \n",
    "                  'TotalCharges'] = contract_data['MonthlyCharges']\n",
    "\n",
    "# Преобразование 'BeginDate' и 'EndDate' в datetime\n",
    "contract_data['BeginDate'] = pd.to_datetime(contract_data['BeginDate'])\n",
    "\n",
    "# Для «EndDate» сначала заменим индикаторы неушедших клиентов на согласованный формат.\n",
    "contract_data['EndDate'] = contract_data['EndDate'].replace({'No': None})\n",
    "contract_data['EndDate'] = pd.to_datetime(contract_data['EndDate'])\n",
    "\n",
    "# Создайте двоичную целевую переменную для оттока\n",
    "contract_data['Churn'] = contract_data['EndDate'].notnull().astype(int)\n",
    "\n",
    "# Переименование столбцов в данных интернета и телефона для ясности\n",
    "internet_data.columns = ['customerID', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                         'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "phone_data.columns = ['customerID', 'MultipleLines']\n",
    "\n",
    "# Проверка преобразований\n",
    "display(contract_data.head())\n",
    "display(internet_data.head())\n",
    "display(phone_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Преобразования и обработка пропущенных значений успешно применены:***\n",
    "\n",
    "1. **Данные Контракта**:\n",
    "     - Преобразованы BeginDate и EndDate в datetime.\n",
    "     - Заполнены пропущенные значения TotalCharges для новых клиентов (чья BeginDate равна дате извлечения данных) их MonthlyCharges.\n",
    "     - Создана двоичная целевая переменная Churn (1 — отток, 0 — не отток).\n",
    "\n",
    "\n",
    "2. **Данные Интернета и Телефона**:\n",
    "     - Переименованы столбцы для ясности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>BeginDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Type</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>MultipleLines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>...</td>\n",
       "      <td>29.85</td>\n",
       "      <td>0</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>NaT</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>...</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>0</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>...</td>\n",
       "      <td>108.15</td>\n",
       "      <td>1</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>...</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>0</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>...</td>\n",
       "      <td>151.65</td>\n",
       "      <td>1</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  BeginDate    EndDate  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No 2019-04-29        NaT   \n",
       "1  5575-GNVDE    Male              0      No         No 2019-03-26        NaT   \n",
       "2  3668-QPYBK    Male              0      No         No 2018-08-09 2019-12-01   \n",
       "3  7795-CFOCW    Male              0      No         No 2018-12-22        NaT   \n",
       "4  9237-HQITU  Female              0      No         No 2019-01-26 2019-11-01   \n",
       "\n",
       "             Type PaperlessBilling              PaymentMethod  ...  \\\n",
       "0  Month-to-month              Yes           Electronic check  ...   \n",
       "1        One year               No               Mailed check  ...   \n",
       "2  Month-to-month              Yes               Mailed check  ...   \n",
       "3        One year               No  Bank transfer (automatic)  ...   \n",
       "4  Month-to-month              Yes           Electronic check  ...   \n",
       "\n",
       "   TotalCharges  Churn  InternetService OnlineSecurity OnlineBackup  \\\n",
       "0         29.85      0              DSL             No          Yes   \n",
       "1       1889.50      0              DSL            Yes           No   \n",
       "2        108.15      1              DSL            Yes          Yes   \n",
       "3       1840.75      0              DSL            Yes           No   \n",
       "4        151.65      1      Fiber optic             No           No   \n",
       "\n",
       "  DeviceProtection TechSupport StreamingTV StreamingMovies MultipleLines  \n",
       "0               No          No          No              No           NaN  \n",
       "1              Yes          No          No              No            No  \n",
       "2               No          No          No              No            No  \n",
       "3              Yes         Yes          No              No           NaN  \n",
       "4               No          No          No              No            No  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов: 0\n"
     ]
    }
   ],
   "source": [
    "# Объединение данных\n",
    "merged_data = pd.merge(personal_data, contract_data, on='customerID', how='left')\n",
    "merged_data = pd.merge(merged_data, internet_data, on='customerID', how='left')\n",
    "merged_data = pd.merge(merged_data, phone_data, on='customerID', how='left')\n",
    "\n",
    "# Обеспечение того, чтобы у каждого клиента была только одна запись\n",
    "merged_data_duplicates = merged_data.duplicated(subset=['customerID']).sum()\n",
    "\n",
    "# Проверка объединенных данных и наличия дубликатов.\n",
    "display(merged_data.head()) \n",
    "print(\"Количество дубликатов: {}\".format(merged_data_duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Данные из всех четырех таблиц были успешно объединены в один DataFrame. Каждый клиент представлен только одной записью, на что указывает отсутствие дубликатов.*\n",
    "\n",
    "**Обзор объединенных данных**\n",
    "\n",
    "- Объединенный набор данных содержит различные атрибуты клиентов, включая личные данные, информацию о контрактах, интернет-услугах и телефонных услугах, а также статус оттока."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeginDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>ContractDuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>NaT</td>\n",
       "      <td>9.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>15.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>13.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>9.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BeginDate    EndDate  ContractDuration\n",
       "0 2019-04-29        NaT          9.266667\n",
       "1 2019-03-26        NaT         10.400000\n",
       "2 2018-08-09 2019-12-01         15.966667\n",
       "3 2018-12-22        NaT         13.533333\n",
       "4 2019-01-26 2019-11-01          9.300000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задание эстетического стиля для графиков\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Проведение исследовательского анализа\n",
    "# 1. Распределение оттока\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.countplot(x='Churn', data=merged_data)\n",
    "plt.title('Распределение оттока клиентов')\n",
    "plt.show()\n",
    "\n",
    "# 2. Корреляционный анализ\n",
    "# Выбор числовых признаков для корреляционного анализа\n",
    "numerical_features = merged_data.select_dtypes(include=['int64', 'float64'])\n",
    "correlation_matrix = numerical_features.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Матрица корреляции')\n",
    "plt.show()\n",
    "\n",
    "# 3. Функция продолжительности контракта\n",
    "# Создание нового признака «ContractDuration» в месяцах\n",
    "merged_data['ContractDuration'] = (\n",
    "    (merged_data['EndDate'].fillna(pd.to_datetime('2020-02-01')) - merged_data['BeginDate'])\n",
    "    .dt.days / 30\n",
    ")\n",
    "\n",
    "merged_data[['BeginDate', 'EndDate', 'ContractDuration']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследовательский анализ позволил получить ценную информацию:\n",
    "\n",
    "1. **Распределение оттока клиентов**:\n",
    "    - График подсчета показывает распределение ушедших и не ушедших клиентов. Эта визуализация необходима для понимания баланса набора данных.\n",
    "\n",
    "\n",
    "2. **Корреляционный анализ**:\n",
    "    - Была создана тепловая карта корреляционной матрицы числовых признаков. Это помогает выявить любые существенные корреляции между признаками, особенно теми, которые могут иметь отношение к оттоку клиентов.\n",
    "\n",
    "\n",
    "3. **Срок действия контракта**:\n",
    "    - Был создан новый признак \"ContractDuration\". Он представляет собой продолжительность контракта в месяцах, рассчитанную как разница между BeginDate и EndDate (при условии, что дата извлечения данных — «01 февраля 2020 г.» для текущих клиентов). Этот признак может иметь решающее значение для прогнозирования оттока клиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>BeginDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Type</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>...</th>\n",
       "      <th>Churn</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>ContractDuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.439916</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.160323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.231078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.439916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-03-26</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.259629</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.439916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.362660</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.165711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.439916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.746535</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.295235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.439916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.197365</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.219153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  SeniorCitizen  Partner  Dependents  BeginDate    EndDate  Type  \\\n",
       "0       0      -0.439916        1           0 2019-04-29        NaT     0   \n",
       "1       1      -0.439916        0           0 2019-03-26        NaT     1   \n",
       "2       1      -0.439916        0           0 2018-08-09 2019-12-01     0   \n",
       "3       1      -0.439916        0           0 2018-12-22        NaT     1   \n",
       "4       0      -0.439916        0           0 2019-01-26 2019-11-01     0   \n",
       "\n",
       "   PaperlessBilling  PaymentMethod  MonthlyCharges  ...  Churn  \\\n",
       "0                 1              2       -1.160323  ...      0   \n",
       "1                 0              3       -0.259629  ...      0   \n",
       "2                 1              3       -0.362660  ...      1   \n",
       "3                 0              0       -0.746535  ...      0   \n",
       "4                 1              2        0.197365  ...      1   \n",
       "\n",
       "   InternetService  OnlineSecurity  OnlineBackup  DeviceProtection  \\\n",
       "0                0               0             1                 0   \n",
       "1                0               1             0                 1   \n",
       "2                0               1             1                 0   \n",
       "3                0               1             0                 1   \n",
       "4                1               0             0                 0   \n",
       "\n",
       "   TechSupport  StreamingTV  StreamingMovies  MultipleLines  ContractDuration  \n",
       "0            0            0                0              2         -0.231078  \n",
       "1            0            0                0              0          0.174349  \n",
       "2            0            0                0              0          2.165711  \n",
       "3            1            0                0              2          1.295235  \n",
       "4            0            0                0              0         -0.219153  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обработка категориальных переменных\n",
    "# Выбор категориальных столбцов\n",
    "categorical_cols = merged_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# # Применение label encoding к категориальным столбцам\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    if col != 'customerID':  # Исключение идентификатора клиента из кодирования\n",
    "        merged_data[col] = label_encoder.fit_transform(merged_data[col])\n",
    "\n",
    "# Масштабирование числовых признаков\n",
    "# Выбор числовых столбцов (за исключением двоичных столбцов, таких как «Отток»)\n",
    "if 'Churn' in merged_data.columns:\n",
    "    # Выбор числовых столбцов, исключая «Отток»\n",
    "    numerical_cols = [col for col in merged_data.select_dtypes(include=['int64', 'float64']).columns if col != 'Churn']\n",
    "else:\n",
    "    # Выбор всех числовых столбцов\n",
    "    numerical_cols = merged_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Применение стандартного масштабирования\n",
    "scaler = StandardScaler()\n",
    "merged_data[numerical_cols] = scaler.fit_transform(merged_data[numerical_cols])\n",
    "\n",
    "# Окончательный набор данных для обучения модели\n",
    "final_data = merged_data.drop(['customerID'], axis=1)\n",
    "\n",
    "# Отображение первых нескольких строк обработанных данных\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этапы предварительной обработки успешно завершены:\n",
    "\n",
    "1. **Обработка категориальных переменных:**\n",
    "    - К категориальным столбцам применено кодирование меток (Label encoding), преобразующее их в числовые значения. `customerID` был исключен из этой кодировки, поскольку это идентификатор.\n",
    "\n",
    "\n",
    "2. **Масштабирование числовых признаков:**\n",
    "    - К числовым столбцам применено стандартное масштабирование, нормализующее их распределение.\n",
    "\n",
    "\n",
    "3. **Окончательный набор данных для обучения модели:**\n",
    "    - Исключен столбец «customerID», который не используется для обучения модели.\n",
    "    - Окончательный набор данных готов к следующему этапу обучения модели.\n",
    "\n",
    "После завершения этих этапов предварительной обработки наш набор данных подготовлен для обучения моделей машинного обучения для прогнозирования оттока клиентов. Следующим шагом будет разделение данных на обучающий и тестовый наборы, выбор подходящих моделей и продолжение процесса обучения и проверки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение Моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4225, 18), (1409, 18), (1409, 18), (4225,), (1409,), (1409,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определение признаков (X) и таргета (y)\n",
    "X = final_data.drop(['BeginDate', 'EndDate', 'Churn'], axis=1)\n",
    "y = final_data['Churn']\n",
    "\n",
    "# Случайное состояние для воспроизводимости - дата начала проекта\n",
    "RANDOM_STATE = 150124\n",
    "\n",
    "# Первое разделение: временный обучающий набор и тестовый набор\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE)\n",
    "\n",
    "# Второе разделение: окончательный обучающий набор и набор проверки.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "# Подтверждение разделения\n",
    "(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор данных был успешно разделен на обучающий, валидационный и тестовый наборы:\n",
    "\n",
    "- Обучающий набор: 4225 образца.\n",
    "- Валидационный набор: 1409 образца.\n",
    "- Тестовый набор: 1409 образец.\n",
    "- Признаков в каждом наборе: 18 признаков.\n",
    "- Целевая переменная («Отток»): присутствует как в обучающих, так и в тестовых наборах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6530359636659389, 0.821585635324191)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим SimpleImputer со стратегией (например, среднее, медиана, самое частое)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Подгоним и преобразуем преобразователь к нашим обучающим данным\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "\n",
    "# Преобразуем тестовые данные, используя тот же преобразователь\n",
    "X_val = imputer.transform(X_val)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Модель Решающее Дерево (Decision Tree)\n",
    "dt_model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_cv_score = cross_val_score(dt_model, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "# Модель Случайный Лес (Random Forest)\n",
    "rf_model = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_cv_score = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "(dt_cv_score, rf_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение и перекрестная проверка моделей дерева решений и случайного леса успешно завершены. Вот оценки ROC-AUC по результатам 5-кратной перекрестной проверки:\n",
    "\n",
    "- Модель **Decision Tree** ≈ 0,65.\n",
    "- Модель **Random Forest** ≈ 0,82.\n",
    "\n",
    "Модель случайного леса демонстрирует значительно более высокий показатель ROC-AUC по сравнению с моделью дерева решений, что указывает на лучшую эффективность прогнозирования оттока клиентов и что уже очень близко к проектному требованию по метрике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8103737228465787"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучение модели XGBoost\n",
    "xgb_model = XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_cv_score = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "xgb_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, модель XGBoost показала результат послабее, чем Random Forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Преобразование данных в тензоры PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Создание наборов данных и загрузчиков данных\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.6712, Val Loss: 0.7006\n",
      "Epoch [2/1000], Loss: 0.6744, Val Loss: 0.6977\n",
      "Epoch [3/1000], Loss: 0.6686, Val Loss: 0.6948\n",
      "Epoch [4/1000], Loss: 0.6921, Val Loss: 0.6917\n",
      "Epoch [5/1000], Loss: 0.7060, Val Loss: 0.6884\n",
      "Epoch [6/1000], Loss: 0.6998, Val Loss: 0.6849\n",
      "Epoch [7/1000], Loss: 0.6444, Val Loss: 0.6810\n",
      "Epoch [8/1000], Loss: 0.6725, Val Loss: 0.6766\n",
      "Epoch [9/1000], Loss: 0.6437, Val Loss: 0.6716\n",
      "Epoch [10/1000], Loss: 0.5947, Val Loss: 0.6665\n",
      "Epoch [11/1000], Loss: 0.6655, Val Loss: 0.6611\n",
      "Epoch [12/1000], Loss: 0.6144, Val Loss: 0.6556\n",
      "Epoch [13/1000], Loss: 0.5238, Val Loss: 0.6500\n",
      "Epoch [14/1000], Loss: 0.7738, Val Loss: 0.6443\n",
      "Epoch [15/1000], Loss: 0.5868, Val Loss: 0.6386\n",
      "Epoch [16/1000], Loss: 0.5322, Val Loss: 0.6327\n",
      "Epoch [17/1000], Loss: 0.8114, Val Loss: 0.6267\n",
      "Epoch [18/1000], Loss: 0.6297, Val Loss: 0.6207\n",
      "Epoch [19/1000], Loss: 0.6007, Val Loss: 0.6145\n",
      "Epoch [20/1000], Loss: 0.5404, Val Loss: 0.6085\n",
      "Epoch [21/1000], Loss: 0.9085, Val Loss: 0.6025\n",
      "Epoch [22/1000], Loss: 0.5302, Val Loss: 0.5968\n",
      "Epoch [23/1000], Loss: 0.6207, Val Loss: 0.5912\n",
      "Epoch [24/1000], Loss: 0.7499, Val Loss: 0.5856\n",
      "Epoch [25/1000], Loss: 0.2375, Val Loss: 0.5803\n",
      "Epoch [26/1000], Loss: 0.5048, Val Loss: 0.5751\n",
      "Epoch [27/1000], Loss: 0.4492, Val Loss: 0.5702\n",
      "Epoch [28/1000], Loss: 0.8503, Val Loss: 0.5658\n",
      "Epoch [29/1000], Loss: 0.4880, Val Loss: 0.5618\n",
      "Epoch [30/1000], Loss: 1.2935, Val Loss: 0.5579\n",
      "Epoch [31/1000], Loss: 0.4389, Val Loss: 0.5544\n",
      "Epoch [32/1000], Loss: 0.5061, Val Loss: 0.5511\n",
      "Epoch [33/1000], Loss: 0.9281, Val Loss: 0.5481\n",
      "Epoch [34/1000], Loss: 0.5136, Val Loss: 0.5453\n",
      "Epoch [35/1000], Loss: 0.8507, Val Loss: 0.5427\n",
      "Epoch [36/1000], Loss: 0.1418, Val Loss: 0.5403\n",
      "Epoch [37/1000], Loss: 0.5032, Val Loss: 0.5381\n",
      "Epoch [38/1000], Loss: 0.5183, Val Loss: 0.5360\n",
      "Epoch [39/1000], Loss: 1.0536, Val Loss: 0.5341\n",
      "Epoch [40/1000], Loss: 1.1363, Val Loss: 0.5324\n",
      "Epoch [41/1000], Loss: 0.3161, Val Loss: 0.5307\n",
      "Epoch [42/1000], Loss: 0.5560, Val Loss: 0.5289\n",
      "Epoch [43/1000], Loss: 0.2879, Val Loss: 0.5274\n",
      "Epoch [44/1000], Loss: 0.1093, Val Loss: 0.5260\n",
      "Epoch [45/1000], Loss: 0.5163, Val Loss: 0.5246\n",
      "Epoch [46/1000], Loss: 1.0399, Val Loss: 0.5233\n",
      "Epoch [47/1000], Loss: 0.1702, Val Loss: 0.5221\n",
      "Epoch [48/1000], Loss: 0.4455, Val Loss: 0.5208\n",
      "Epoch [49/1000], Loss: 0.6135, Val Loss: 0.5196\n",
      "Epoch [50/1000], Loss: 0.2024, Val Loss: 0.5185\n",
      "Epoch [51/1000], Loss: 0.2315, Val Loss: 0.5174\n",
      "Epoch [52/1000], Loss: 0.0995, Val Loss: 0.5165\n",
      "Epoch [53/1000], Loss: 0.0703, Val Loss: 0.5154\n",
      "Epoch [54/1000], Loss: 0.4115, Val Loss: 0.5143\n",
      "Epoch [55/1000], Loss: 0.4389, Val Loss: 0.5133\n",
      "Epoch [56/1000], Loss: 0.2137, Val Loss: 0.5123\n",
      "Epoch [57/1000], Loss: 0.2668, Val Loss: 0.5114\n",
      "Epoch [58/1000], Loss: 0.2935, Val Loss: 0.5104\n",
      "Epoch [59/1000], Loss: 0.5513, Val Loss: 0.5094\n",
      "Epoch [60/1000], Loss: 0.2450, Val Loss: 0.5085\n",
      "Epoch [61/1000], Loss: 0.8561, Val Loss: 0.5076\n",
      "Epoch [62/1000], Loss: 0.4146, Val Loss: 0.5066\n",
      "Epoch [63/1000], Loss: 1.2563, Val Loss: 0.5058\n",
      "Epoch [64/1000], Loss: 0.1771, Val Loss: 0.5050\n",
      "Epoch [65/1000], Loss: 0.5001, Val Loss: 0.5041\n",
      "Epoch [66/1000], Loss: 0.3644, Val Loss: 0.5032\n",
      "Epoch [67/1000], Loss: 0.0207, Val Loss: 0.5023\n",
      "Epoch [68/1000], Loss: 0.0840, Val Loss: 0.5015\n",
      "Epoch [69/1000], Loss: 0.9732, Val Loss: 0.5007\n",
      "Epoch [70/1000], Loss: 0.2843, Val Loss: 0.4998\n",
      "Epoch [71/1000], Loss: 0.7362, Val Loss: 0.4990\n",
      "Epoch [72/1000], Loss: 0.7734, Val Loss: 0.4982\n",
      "Epoch [73/1000], Loss: 0.4343, Val Loss: 0.4974\n",
      "Epoch [74/1000], Loss: 0.1795, Val Loss: 0.4967\n",
      "Epoch [75/1000], Loss: 0.4661, Val Loss: 0.4959\n",
      "Epoch [76/1000], Loss: 0.1145, Val Loss: 0.4952\n",
      "Epoch [77/1000], Loss: 0.0501, Val Loss: 0.4945\n",
      "Epoch [78/1000], Loss: 0.2324, Val Loss: 0.4937\n",
      "Epoch [79/1000], Loss: 2.2530, Val Loss: 0.4930\n",
      "Epoch [80/1000], Loss: 0.2402, Val Loss: 0.4923\n",
      "Epoch [81/1000], Loss: 0.4833, Val Loss: 0.4916\n",
      "Epoch [82/1000], Loss: 0.1396, Val Loss: 0.4910\n",
      "Epoch [83/1000], Loss: 0.2575, Val Loss: 0.4904\n",
      "Epoch [84/1000], Loss: 0.4387, Val Loss: 0.4897\n",
      "Epoch [85/1000], Loss: 2.8889, Val Loss: 0.4891\n",
      "Epoch [86/1000], Loss: 0.7208, Val Loss: 0.4887\n",
      "Epoch [87/1000], Loss: 1.2622, Val Loss: 0.4881\n",
      "Epoch [88/1000], Loss: 0.3751, Val Loss: 0.4876\n",
      "Epoch [89/1000], Loss: 0.7511, Val Loss: 0.4870\n",
      "Epoch [90/1000], Loss: 0.1094, Val Loss: 0.4865\n",
      "Epoch [91/1000], Loss: 0.0838, Val Loss: 0.4859\n",
      "Epoch [92/1000], Loss: 0.2708, Val Loss: 0.4854\n",
      "Epoch [93/1000], Loss: 0.1014, Val Loss: 0.4849\n",
      "Epoch [94/1000], Loss: 0.1270, Val Loss: 0.4844\n",
      "Epoch [95/1000], Loss: 0.2775, Val Loss: 0.4839\n",
      "Epoch [96/1000], Loss: 0.8715, Val Loss: 0.4834\n",
      "Epoch [97/1000], Loss: 0.9287, Val Loss: 0.4829\n",
      "Epoch [98/1000], Loss: 0.0782, Val Loss: 0.4824\n",
      "Epoch [99/1000], Loss: 0.7029, Val Loss: 0.4819\n",
      "Epoch [100/1000], Loss: 0.1075, Val Loss: 0.4815\n",
      "Epoch [101/1000], Loss: 0.3182, Val Loss: 0.4810\n",
      "Epoch [102/1000], Loss: 0.2188, Val Loss: 0.4806\n",
      "Epoch [103/1000], Loss: 0.1761, Val Loss: 0.4801\n",
      "Epoch [104/1000], Loss: 0.0239, Val Loss: 0.4797\n",
      "Epoch [105/1000], Loss: 0.1601, Val Loss: 0.4793\n",
      "Epoch [106/1000], Loss: 0.1270, Val Loss: 0.4789\n",
      "Epoch [107/1000], Loss: 0.2243, Val Loss: 0.4784\n",
      "Epoch [108/1000], Loss: 0.1193, Val Loss: 0.4781\n",
      "Epoch [109/1000], Loss: 0.6697, Val Loss: 0.4777\n",
      "Epoch [110/1000], Loss: 0.4982, Val Loss: 0.4773\n",
      "Epoch [111/1000], Loss: 0.2490, Val Loss: 0.4770\n",
      "Epoch [112/1000], Loss: 0.7874, Val Loss: 0.4767\n",
      "Epoch [113/1000], Loss: 0.2594, Val Loss: 0.4763\n",
      "Epoch [114/1000], Loss: 1.1854, Val Loss: 0.4760\n",
      "Epoch [115/1000], Loss: 0.0042, Val Loss: 0.4757\n",
      "Epoch [116/1000], Loss: 0.0245, Val Loss: 0.4754\n",
      "Epoch [117/1000], Loss: 0.0519, Val Loss: 0.4751\n",
      "Epoch [118/1000], Loss: 0.9055, Val Loss: 0.4748\n",
      "Epoch [119/1000], Loss: 0.4639, Val Loss: 0.4745\n",
      "Epoch [120/1000], Loss: 0.5526, Val Loss: 0.4742\n",
      "Epoch [121/1000], Loss: 0.1128, Val Loss: 0.4740\n",
      "Epoch [122/1000], Loss: 0.9716, Val Loss: 0.4737\n",
      "Epoch [123/1000], Loss: 0.0803, Val Loss: 0.4734\n",
      "Epoch [124/1000], Loss: 1.1818, Val Loss: 0.4731\n",
      "Epoch [125/1000], Loss: 0.2468, Val Loss: 0.4729\n",
      "Epoch [126/1000], Loss: 0.7342, Val Loss: 0.4726\n",
      "Epoch [127/1000], Loss: 0.1132, Val Loss: 0.4724\n",
      "Epoch [128/1000], Loss: 0.0479, Val Loss: 0.4721\n",
      "Epoch [129/1000], Loss: 0.4254, Val Loss: 0.4719\n",
      "Epoch [130/1000], Loss: 0.3923, Val Loss: 0.4716\n",
      "Epoch [131/1000], Loss: 0.8616, Val Loss: 0.4714\n",
      "Epoch [132/1000], Loss: 0.4262, Val Loss: 0.4712\n",
      "Epoch [133/1000], Loss: 0.2459, Val Loss: 0.4710\n",
      "Epoch [134/1000], Loss: 0.5465, Val Loss: 0.4708\n",
      "Epoch [135/1000], Loss: 0.5282, Val Loss: 0.4706\n",
      "Epoch [136/1000], Loss: 0.0558, Val Loss: 0.4704\n",
      "Epoch [137/1000], Loss: 0.0857, Val Loss: 0.4702\n",
      "Epoch [138/1000], Loss: 0.5679, Val Loss: 0.4700\n",
      "Epoch [139/1000], Loss: 0.6541, Val Loss: 0.4698\n",
      "Epoch [140/1000], Loss: 0.1152, Val Loss: 0.4696\n",
      "Epoch [141/1000], Loss: 0.2448, Val Loss: 0.4695\n",
      "Epoch [142/1000], Loss: 0.3047, Val Loss: 0.4693\n",
      "Epoch [143/1000], Loss: 0.0858, Val Loss: 0.4691\n",
      "Epoch [144/1000], Loss: 0.6076, Val Loss: 0.4689\n",
      "Epoch [145/1000], Loss: 1.4917, Val Loss: 0.4688\n",
      "Epoch [146/1000], Loss: 0.6649, Val Loss: 0.4686\n",
      "Epoch [147/1000], Loss: 2.8375, Val Loss: 0.4685\n",
      "Epoch [148/1000], Loss: 0.5154, Val Loss: 0.4683\n",
      "Epoch [149/1000], Loss: 0.6796, Val Loss: 0.4682\n",
      "Epoch [150/1000], Loss: 0.6188, Val Loss: 0.4680\n",
      "Epoch [151/1000], Loss: 0.2513, Val Loss: 0.4679\n",
      "Epoch [152/1000], Loss: 0.6658, Val Loss: 0.4677\n",
      "Epoch [153/1000], Loss: 0.6042, Val Loss: 0.4676\n",
      "Epoch [154/1000], Loss: 0.3121, Val Loss: 0.4674\n",
      "Epoch [155/1000], Loss: 1.1812, Val Loss: 0.4673\n",
      "Epoch [156/1000], Loss: 0.6828, Val Loss: 0.4672\n",
      "Epoch [157/1000], Loss: 0.0660, Val Loss: 0.4671\n",
      "Epoch [158/1000], Loss: 0.6923, Val Loss: 0.4669\n",
      "Epoch [159/1000], Loss: 0.0932, Val Loss: 0.4668\n",
      "Epoch [160/1000], Loss: 0.0251, Val Loss: 0.4666\n",
      "Epoch [161/1000], Loss: 0.9715, Val Loss: 0.4665\n",
      "Epoch [162/1000], Loss: 0.0028, Val Loss: 0.4664\n",
      "Epoch [163/1000], Loss: 0.7504, Val Loss: 0.4663\n",
      "Epoch [164/1000], Loss: 0.6114, Val Loss: 0.4662\n",
      "Epoch [165/1000], Loss: 0.1839, Val Loss: 0.4661\n",
      "Epoch [166/1000], Loss: 0.2305, Val Loss: 0.4659\n",
      "Epoch [167/1000], Loss: 0.7082, Val Loss: 0.4658\n",
      "Epoch [168/1000], Loss: 0.3639, Val Loss: 0.4657\n",
      "Epoch [169/1000], Loss: 0.0688, Val Loss: 0.4656\n",
      "Epoch [170/1000], Loss: 0.2043, Val Loss: 0.4654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/1000], Loss: 0.6520, Val Loss: 0.4653\n",
      "Epoch [172/1000], Loss: 0.2542, Val Loss: 0.4652\n",
      "Epoch [173/1000], Loss: 1.1166, Val Loss: 0.4651\n",
      "Epoch [174/1000], Loss: 0.4117, Val Loss: 0.4650\n",
      "Epoch [175/1000], Loss: 0.6977, Val Loss: 0.4649\n",
      "Epoch [176/1000], Loss: 0.5998, Val Loss: 0.4648\n",
      "Epoch [177/1000], Loss: 0.8747, Val Loss: 0.4647\n",
      "Epoch [178/1000], Loss: 0.9524, Val Loss: 0.4646\n",
      "Epoch [179/1000], Loss: 0.6388, Val Loss: 0.4645\n",
      "Epoch [180/1000], Loss: 0.3910, Val Loss: 0.4644\n",
      "Epoch [181/1000], Loss: 0.2011, Val Loss: 0.4643\n",
      "Epoch [182/1000], Loss: 0.2280, Val Loss: 0.4641\n",
      "Epoch [183/1000], Loss: 0.0291, Val Loss: 0.4640\n",
      "Epoch [184/1000], Loss: 0.2732, Val Loss: 0.4639\n",
      "Epoch [185/1000], Loss: 0.0126, Val Loss: 0.4638\n",
      "Epoch [186/1000], Loss: 0.2647, Val Loss: 0.4638\n",
      "Epoch [187/1000], Loss: 0.7539, Val Loss: 0.4637\n",
      "Epoch [188/1000], Loss: 1.0361, Val Loss: 0.4636\n",
      "Epoch [189/1000], Loss: 0.1507, Val Loss: 0.4634\n",
      "Epoch [190/1000], Loss: 0.0591, Val Loss: 0.4633\n",
      "Epoch [191/1000], Loss: 0.4013, Val Loss: 0.4632\n",
      "Epoch [192/1000], Loss: 0.8312, Val Loss: 0.4631\n",
      "Epoch [193/1000], Loss: 0.5666, Val Loss: 0.4631\n",
      "Epoch [194/1000], Loss: 0.7439, Val Loss: 0.4630\n",
      "Epoch [195/1000], Loss: 0.8104, Val Loss: 0.4629\n",
      "Epoch [196/1000], Loss: 0.7459, Val Loss: 0.4628\n",
      "Epoch [197/1000], Loss: 0.0519, Val Loss: 0.4628\n",
      "Epoch [198/1000], Loss: 0.3349, Val Loss: 0.4627\n",
      "Epoch [199/1000], Loss: 0.8304, Val Loss: 0.4626\n",
      "Epoch [200/1000], Loss: 0.0396, Val Loss: 0.4625\n",
      "Epoch [201/1000], Loss: 0.1102, Val Loss: 0.4624\n",
      "Epoch [202/1000], Loss: 0.2242, Val Loss: 0.4623\n",
      "Epoch [203/1000], Loss: 0.2476, Val Loss: 0.4622\n",
      "Epoch [204/1000], Loss: 0.7410, Val Loss: 0.4621\n",
      "Epoch [205/1000], Loss: 0.0086, Val Loss: 0.4621\n",
      "Epoch [206/1000], Loss: 0.0777, Val Loss: 0.4620\n",
      "Epoch [207/1000], Loss: 0.5314, Val Loss: 0.4619\n",
      "Epoch [208/1000], Loss: 0.0220, Val Loss: 0.4618\n",
      "Epoch [209/1000], Loss: 0.3582, Val Loss: 0.4617\n",
      "Epoch [210/1000], Loss: 3.2424, Val Loss: 0.4616\n",
      "Epoch [211/1000], Loss: 0.2538, Val Loss: 0.4615\n",
      "Epoch [212/1000], Loss: 0.4730, Val Loss: 0.4614\n",
      "Epoch [213/1000], Loss: 0.3062, Val Loss: 0.4614\n",
      "Epoch [214/1000], Loss: 0.0482, Val Loss: 0.4613\n",
      "Epoch [215/1000], Loss: 0.0301, Val Loss: 0.4612\n",
      "Epoch [216/1000], Loss: 0.0477, Val Loss: 0.4611\n",
      "Epoch [217/1000], Loss: 0.5688, Val Loss: 0.4610\n",
      "Epoch [218/1000], Loss: 0.6264, Val Loss: 0.4610\n",
      "Epoch [219/1000], Loss: 0.4144, Val Loss: 0.4609\n",
      "Epoch [220/1000], Loss: 0.1256, Val Loss: 0.4608\n",
      "Epoch [221/1000], Loss: 0.6788, Val Loss: 0.4608\n",
      "Epoch [222/1000], Loss: 0.1261, Val Loss: 0.4607\n",
      "Epoch [223/1000], Loss: 0.1081, Val Loss: 0.4606\n",
      "Epoch [224/1000], Loss: 0.0875, Val Loss: 0.4605\n",
      "Epoch [225/1000], Loss: 0.4285, Val Loss: 0.4605\n",
      "Epoch [226/1000], Loss: 0.0788, Val Loss: 0.4604\n",
      "Epoch [227/1000], Loss: 0.7747, Val Loss: 0.4603\n",
      "Epoch [228/1000], Loss: 1.6578, Val Loss: 0.4602\n",
      "Epoch [229/1000], Loss: 0.0809, Val Loss: 0.4602\n",
      "Epoch [230/1000], Loss: 0.1802, Val Loss: 0.4601\n",
      "Epoch [231/1000], Loss: 2.9640, Val Loss: 0.4601\n",
      "Epoch [232/1000], Loss: 0.7497, Val Loss: 0.4600\n",
      "Epoch [233/1000], Loss: 1.1228, Val Loss: 0.4599\n",
      "Epoch [234/1000], Loss: 0.0660, Val Loss: 0.4599\n",
      "Epoch [235/1000], Loss: 0.7424, Val Loss: 0.4598\n",
      "Epoch [236/1000], Loss: 0.0054, Val Loss: 0.4597\n",
      "Epoch [237/1000], Loss: 0.0107, Val Loss: 0.4597\n",
      "Epoch [238/1000], Loss: 0.0872, Val Loss: 0.4596\n",
      "Epoch [239/1000], Loss: 0.5806, Val Loss: 0.4596\n",
      "Epoch [240/1000], Loss: 0.6124, Val Loss: 0.4595\n",
      "Epoch [241/1000], Loss: 0.4510, Val Loss: 0.4594\n",
      "Epoch [242/1000], Loss: 0.3644, Val Loss: 0.4594\n",
      "Epoch [243/1000], Loss: 3.6826, Val Loss: 0.4593\n",
      "Epoch [244/1000], Loss: 0.0738, Val Loss: 0.4592\n",
      "Epoch [245/1000], Loss: 0.0548, Val Loss: 0.4591\n",
      "Epoch [246/1000], Loss: 0.0029, Val Loss: 0.4591\n",
      "Epoch [247/1000], Loss: 0.0460, Val Loss: 0.4590\n",
      "Epoch [248/1000], Loss: 0.4736, Val Loss: 0.4590\n",
      "Epoch [249/1000], Loss: 0.2628, Val Loss: 0.4589\n",
      "Epoch [250/1000], Loss: 1.0931, Val Loss: 0.4589\n",
      "Epoch [251/1000], Loss: 0.4115, Val Loss: 0.4588\n",
      "Epoch [252/1000], Loss: 0.1633, Val Loss: 0.4588\n",
      "Epoch [253/1000], Loss: 0.6540, Val Loss: 0.4587\n",
      "Epoch [254/1000], Loss: 0.5269, Val Loss: 0.4587\n",
      "Epoch [255/1000], Loss: 0.3122, Val Loss: 0.4586\n",
      "Epoch [256/1000], Loss: 0.1204, Val Loss: 0.4586\n",
      "Epoch [257/1000], Loss: 0.1451, Val Loss: 0.4585\n",
      "Epoch [258/1000], Loss: 0.4271, Val Loss: 0.4584\n",
      "Epoch [259/1000], Loss: 0.4019, Val Loss: 0.4584\n",
      "Epoch [260/1000], Loss: 0.7201, Val Loss: 0.4583\n",
      "Epoch [261/1000], Loss: 0.6806, Val Loss: 0.4583\n",
      "Epoch [262/1000], Loss: 0.0726, Val Loss: 0.4582\n",
      "Epoch [263/1000], Loss: 0.5970, Val Loss: 0.4581\n",
      "Epoch [264/1000], Loss: 0.6635, Val Loss: 0.4581\n",
      "Epoch [265/1000], Loss: 0.6216, Val Loss: 0.4580\n",
      "Epoch [266/1000], Loss: 0.2421, Val Loss: 0.4580\n",
      "Epoch [267/1000], Loss: 0.5692, Val Loss: 0.4579\n",
      "Epoch [268/1000], Loss: 0.7020, Val Loss: 0.4578\n",
      "Epoch [269/1000], Loss: 0.6736, Val Loss: 0.4578\n",
      "Epoch [270/1000], Loss: 0.0030, Val Loss: 0.4577\n",
      "Epoch [271/1000], Loss: 2.2091, Val Loss: 0.4577\n",
      "Epoch [272/1000], Loss: 0.6555, Val Loss: 0.4576\n",
      "Epoch [273/1000], Loss: 0.7473, Val Loss: 0.4575\n",
      "Epoch [274/1000], Loss: 0.0076, Val Loss: 0.4575\n",
      "Epoch [275/1000], Loss: 0.6365, Val Loss: 0.4574\n",
      "Epoch [276/1000], Loss: 0.3954, Val Loss: 0.4574\n",
      "Epoch [277/1000], Loss: 0.9680, Val Loss: 0.4573\n",
      "Epoch [278/1000], Loss: 0.0985, Val Loss: 0.4573\n",
      "Epoch [279/1000], Loss: 0.9384, Val Loss: 0.4573\n",
      "Epoch [280/1000], Loss: 0.0320, Val Loss: 0.4572\n",
      "Epoch [281/1000], Loss: 0.5131, Val Loss: 0.4571\n",
      "Epoch [282/1000], Loss: 0.1129, Val Loss: 0.4571\n",
      "Epoch [283/1000], Loss: 1.4761, Val Loss: 0.4570\n",
      "Epoch [284/1000], Loss: 0.3369, Val Loss: 0.4570\n",
      "Epoch [285/1000], Loss: 0.0302, Val Loss: 0.4569\n",
      "Epoch [286/1000], Loss: 0.5935, Val Loss: 0.4569\n",
      "Epoch [287/1000], Loss: 0.0003, Val Loss: 0.4568\n",
      "Epoch [288/1000], Loss: 0.0902, Val Loss: 0.4568\n",
      "Epoch [289/1000], Loss: 0.0178, Val Loss: 0.4567\n",
      "Epoch [290/1000], Loss: 0.6726, Val Loss: 0.4567\n",
      "Epoch [291/1000], Loss: 0.8090, Val Loss: 0.4566\n",
      "Epoch [292/1000], Loss: 0.1083, Val Loss: 0.4566\n",
      "Epoch [293/1000], Loss: 0.5732, Val Loss: 0.4565\n",
      "Epoch [294/1000], Loss: 0.7006, Val Loss: 0.4565\n",
      "Epoch [295/1000], Loss: 0.5004, Val Loss: 0.4564\n",
      "Epoch [296/1000], Loss: 0.6777, Val Loss: 0.4564\n",
      "Epoch [297/1000], Loss: 0.1525, Val Loss: 0.4563\n",
      "Epoch [298/1000], Loss: 0.7390, Val Loss: 0.4563\n",
      "Epoch [299/1000], Loss: 0.4689, Val Loss: 0.4563\n",
      "Epoch [300/1000], Loss: 0.6073, Val Loss: 0.4562\n",
      "Epoch [301/1000], Loss: 0.2893, Val Loss: 0.4562\n",
      "Epoch [302/1000], Loss: 0.4774, Val Loss: 0.4561\n",
      "Epoch [303/1000], Loss: 0.5881, Val Loss: 0.4561\n",
      "Epoch [304/1000], Loss: 0.1644, Val Loss: 0.4560\n",
      "Epoch [305/1000], Loss: 0.4970, Val Loss: 0.4560\n",
      "Epoch [306/1000], Loss: 0.0006, Val Loss: 0.4559\n",
      "Epoch [307/1000], Loss: 0.0762, Val Loss: 0.4559\n",
      "Epoch [308/1000], Loss: 0.1858, Val Loss: 0.4558\n",
      "Epoch [309/1000], Loss: 0.4483, Val Loss: 0.4558\n",
      "Epoch [310/1000], Loss: 0.0835, Val Loss: 0.4557\n",
      "Epoch [311/1000], Loss: 0.0184, Val Loss: 0.4557\n",
      "Epoch [312/1000], Loss: 0.6684, Val Loss: 0.4556\n",
      "Epoch [313/1000], Loss: 0.5410, Val Loss: 0.4556\n",
      "Epoch [314/1000], Loss: 0.2731, Val Loss: 0.4555\n",
      "Epoch [315/1000], Loss: 0.7543, Val Loss: 0.4555\n",
      "Epoch [316/1000], Loss: 0.0890, Val Loss: 0.4554\n",
      "Epoch [317/1000], Loss: 0.6295, Val Loss: 0.4554\n",
      "Epoch [318/1000], Loss: 0.5558, Val Loss: 0.4553\n",
      "Epoch [319/1000], Loss: 0.1976, Val Loss: 0.4553\n",
      "Epoch [320/1000], Loss: 0.2763, Val Loss: 0.4552\n",
      "Epoch [321/1000], Loss: 0.1638, Val Loss: 0.4552\n",
      "Epoch [322/1000], Loss: 0.0342, Val Loss: 0.4552\n",
      "Epoch [323/1000], Loss: 0.0097, Val Loss: 0.4551\n",
      "Epoch [324/1000], Loss: 0.5838, Val Loss: 0.4551\n",
      "Epoch [325/1000], Loss: 0.7535, Val Loss: 0.4550\n",
      "Epoch [326/1000], Loss: 1.0221, Val Loss: 0.4550\n",
      "Epoch [327/1000], Loss: 1.1062, Val Loss: 0.4550\n",
      "Epoch [328/1000], Loss: 0.0272, Val Loss: 0.4549\n",
      "Epoch [329/1000], Loss: 0.4986, Val Loss: 0.4549\n",
      "Epoch [330/1000], Loss: 0.5750, Val Loss: 0.4549\n",
      "Epoch [331/1000], Loss: 0.0555, Val Loss: 0.4548\n",
      "Epoch [332/1000], Loss: 0.7061, Val Loss: 0.4548\n",
      "Epoch [333/1000], Loss: 0.1858, Val Loss: 0.4547\n",
      "Epoch [334/1000], Loss: 0.0073, Val Loss: 0.4547\n",
      "Epoch [335/1000], Loss: 0.6589, Val Loss: 0.4546\n",
      "Epoch [336/1000], Loss: 0.2170, Val Loss: 0.4546\n",
      "Epoch [337/1000], Loss: 0.9018, Val Loss: 0.4545\n",
      "Epoch [338/1000], Loss: 0.5923, Val Loss: 0.4545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [339/1000], Loss: 0.0047, Val Loss: 0.4545\n",
      "Epoch [340/1000], Loss: 0.2794, Val Loss: 0.4544\n",
      "Epoch [341/1000], Loss: 0.2619, Val Loss: 0.4544\n",
      "Epoch [342/1000], Loss: 0.3526, Val Loss: 0.4544\n",
      "Epoch [343/1000], Loss: 0.0097, Val Loss: 0.4543\n",
      "Epoch [344/1000], Loss: 0.1181, Val Loss: 0.4543\n",
      "Epoch [345/1000], Loss: 0.5049, Val Loss: 0.4543\n",
      "Epoch [346/1000], Loss: 0.8010, Val Loss: 0.4542\n",
      "Epoch [347/1000], Loss: 0.0011, Val Loss: 0.4542\n",
      "Epoch [348/1000], Loss: 0.1503, Val Loss: 0.4542\n",
      "Epoch [349/1000], Loss: 0.0911, Val Loss: 0.4541\n",
      "Epoch [350/1000], Loss: 0.0391, Val Loss: 0.4541\n",
      "Epoch [351/1000], Loss: 0.0567, Val Loss: 0.4541\n",
      "Epoch [352/1000], Loss: 0.2510, Val Loss: 0.4540\n",
      "Epoch [353/1000], Loss: 0.7069, Val Loss: 0.4540\n",
      "Epoch [354/1000], Loss: 0.2363, Val Loss: 0.4539\n",
      "Epoch [355/1000], Loss: 0.3522, Val Loss: 0.4539\n",
      "Epoch [356/1000], Loss: 0.0211, Val Loss: 0.4539\n",
      "Epoch [357/1000], Loss: 0.4249, Val Loss: 0.4538\n",
      "Epoch [358/1000], Loss: 0.5767, Val Loss: 0.4538\n",
      "Epoch [359/1000], Loss: 0.1091, Val Loss: 0.4538\n",
      "Epoch [360/1000], Loss: 1.4524, Val Loss: 0.4537\n",
      "Epoch [361/1000], Loss: 0.0114, Val Loss: 0.4537\n",
      "Epoch [362/1000], Loss: 0.8735, Val Loss: 0.4537\n",
      "Epoch [363/1000], Loss: 1.2606, Val Loss: 0.4536\n",
      "Epoch [364/1000], Loss: 0.1686, Val Loss: 0.4536\n",
      "Epoch [365/1000], Loss: 1.1579, Val Loss: 0.4535\n",
      "Epoch [366/1000], Loss: 0.5190, Val Loss: 0.4535\n",
      "Epoch [367/1000], Loss: 1.2028, Val Loss: 0.4535\n",
      "Epoch [368/1000], Loss: 0.4852, Val Loss: 0.4534\n",
      "Epoch [369/1000], Loss: 0.6151, Val Loss: 0.4534\n",
      "Epoch [370/1000], Loss: 0.2706, Val Loss: 0.4534\n",
      "Epoch [371/1000], Loss: 0.6097, Val Loss: 0.4533\n",
      "Epoch [372/1000], Loss: 0.9386, Val Loss: 0.4533\n",
      "Epoch [373/1000], Loss: 0.0312, Val Loss: 0.4533\n",
      "Epoch [374/1000], Loss: 0.0735, Val Loss: 0.4533\n",
      "Epoch [375/1000], Loss: 0.0290, Val Loss: 0.4532\n",
      "Epoch [376/1000], Loss: 0.2627, Val Loss: 0.4532\n",
      "Epoch [377/1000], Loss: 0.1701, Val Loss: 0.4532\n",
      "Epoch [378/1000], Loss: 0.2547, Val Loss: 0.4532\n",
      "Epoch [379/1000], Loss: 0.0860, Val Loss: 0.4531\n",
      "Epoch [380/1000], Loss: 0.0288, Val Loss: 0.4531\n",
      "Epoch [381/1000], Loss: 0.0140, Val Loss: 0.4530\n",
      "Epoch [382/1000], Loss: 0.2864, Val Loss: 0.4530\n",
      "Epoch [383/1000], Loss: 0.0143, Val Loss: 0.4530\n",
      "Epoch [384/1000], Loss: 0.5851, Val Loss: 0.4530\n",
      "Epoch [385/1000], Loss: 0.0065, Val Loss: 0.4529\n",
      "Epoch [386/1000], Loss: 0.0443, Val Loss: 0.4529\n",
      "Epoch [387/1000], Loss: 0.0416, Val Loss: 0.4529\n",
      "Epoch [388/1000], Loss: 0.0396, Val Loss: 0.4528\n",
      "Epoch [389/1000], Loss: 0.0193, Val Loss: 0.4528\n",
      "Epoch [390/1000], Loss: 0.0123, Val Loss: 0.4528\n",
      "Epoch [391/1000], Loss: 0.3153, Val Loss: 0.4527\n",
      "Epoch [392/1000], Loss: 0.8695, Val Loss: 0.4527\n",
      "Epoch [393/1000], Loss: 0.5398, Val Loss: 0.4527\n",
      "Epoch [394/1000], Loss: 0.1087, Val Loss: 0.4527\n",
      "Epoch [395/1000], Loss: 0.1334, Val Loss: 0.4526\n",
      "Epoch [396/1000], Loss: 0.0927, Val Loss: 0.4526\n",
      "Epoch [397/1000], Loss: 0.5920, Val Loss: 0.4526\n",
      "Epoch [398/1000], Loss: 0.1628, Val Loss: 0.4525\n",
      "Epoch [399/1000], Loss: 0.3709, Val Loss: 0.4525\n",
      "Epoch [400/1000], Loss: 0.1206, Val Loss: 0.4525\n",
      "Epoch [401/1000], Loss: 0.0468, Val Loss: 0.4524\n",
      "Epoch [402/1000], Loss: 0.7695, Val Loss: 0.4524\n",
      "Epoch [403/1000], Loss: 0.2756, Val Loss: 0.4524\n",
      "Epoch [404/1000], Loss: 0.2584, Val Loss: 0.4524\n",
      "Epoch [405/1000], Loss: 1.8911, Val Loss: 0.4523\n",
      "Epoch [406/1000], Loss: 0.1820, Val Loss: 0.4523\n",
      "Epoch [407/1000], Loss: 0.0563, Val Loss: 0.4523\n",
      "Epoch [408/1000], Loss: 0.7738, Val Loss: 0.4523\n",
      "Epoch [409/1000], Loss: 0.0364, Val Loss: 0.4522\n",
      "Epoch [410/1000], Loss: 0.0018, Val Loss: 0.4522\n",
      "Epoch [411/1000], Loss: 0.0413, Val Loss: 0.4522\n",
      "Epoch [412/1000], Loss: 0.6018, Val Loss: 0.4521\n",
      "Epoch [413/1000], Loss: 0.2107, Val Loss: 0.4521\n",
      "Epoch [414/1000], Loss: 3.1263, Val Loss: 0.4521\n",
      "Epoch [415/1000], Loss: 3.6008, Val Loss: 0.4520\n",
      "Epoch [416/1000], Loss: 0.0131, Val Loss: 0.4520\n",
      "Epoch [417/1000], Loss: 0.0607, Val Loss: 0.4519\n",
      "Epoch [418/1000], Loss: 0.0923, Val Loss: 0.4519\n",
      "Epoch [419/1000], Loss: 0.0029, Val Loss: 0.4519\n",
      "Epoch [420/1000], Loss: 0.5149, Val Loss: 0.4519\n",
      "Epoch [421/1000], Loss: 0.8168, Val Loss: 0.4518\n",
      "Epoch [422/1000], Loss: 0.0470, Val Loss: 0.4518\n",
      "Epoch [423/1000], Loss: 0.0165, Val Loss: 0.4518\n",
      "Epoch [424/1000], Loss: 0.0411, Val Loss: 0.4518\n",
      "Epoch [425/1000], Loss: 0.3978, Val Loss: 0.4518\n",
      "Epoch [426/1000], Loss: 0.1080, Val Loss: 0.4518\n",
      "Epoch [427/1000], Loss: 0.0668, Val Loss: 0.4517\n",
      "Epoch [428/1000], Loss: 0.0221, Val Loss: 0.4517\n",
      "Epoch [429/1000], Loss: 0.0001, Val Loss: 0.4517\n",
      "Epoch [430/1000], Loss: 0.8486, Val Loss: 0.4517\n",
      "Epoch [431/1000], Loss: 1.0082, Val Loss: 0.4517\n",
      "Epoch [432/1000], Loss: 0.0616, Val Loss: 0.4516\n",
      "Epoch [433/1000], Loss: 1.0843, Val Loss: 0.4516\n",
      "Epoch [434/1000], Loss: 0.6047, Val Loss: 0.4516\n",
      "Epoch [435/1000], Loss: 0.1531, Val Loss: 0.4516\n",
      "Epoch [436/1000], Loss: 0.3854, Val Loss: 0.4516\n",
      "Epoch [437/1000], Loss: 0.2629, Val Loss: 0.4515\n",
      "Epoch [438/1000], Loss: 2.2825, Val Loss: 0.4515\n",
      "Epoch [439/1000], Loss: 0.2069, Val Loss: 0.4515\n",
      "Epoch [440/1000], Loss: 0.8033, Val Loss: 0.4514\n",
      "Epoch [441/1000], Loss: 0.1135, Val Loss: 0.4514\n",
      "Epoch [442/1000], Loss: 1.0235, Val Loss: 0.4514\n",
      "Epoch [443/1000], Loss: 0.0874, Val Loss: 0.4515\n",
      "Epoch [444/1000], Loss: 0.0067, Val Loss: 0.4514\n",
      "Epoch [445/1000], Loss: 0.8884, Val Loss: 0.4514\n",
      "Epoch [446/1000], Loss: 0.6506, Val Loss: 0.4514\n",
      "Epoch [447/1000], Loss: 0.1802, Val Loss: 0.4514\n",
      "Epoch [448/1000], Loss: 0.0852, Val Loss: 0.4513\n",
      "Epoch [449/1000], Loss: 1.0633, Val Loss: 0.4513\n",
      "Epoch [450/1000], Loss: 0.0987, Val Loss: 0.4513\n",
      "Epoch [451/1000], Loss: 0.3170, Val Loss: 0.4513\n",
      "Epoch [452/1000], Loss: 0.4378, Val Loss: 0.4513\n",
      "Epoch [453/1000], Loss: 0.1602, Val Loss: 0.4512\n",
      "Epoch [454/1000], Loss: 0.9562, Val Loss: 0.4512\n",
      "Epoch [455/1000], Loss: 2.1904, Val Loss: 0.4512\n",
      "Epoch [456/1000], Loss: 0.0565, Val Loss: 0.4512\n",
      "Epoch [457/1000], Loss: 0.3487, Val Loss: 0.4512\n",
      "Epoch [458/1000], Loss: 0.3892, Val Loss: 0.4512\n",
      "Epoch [459/1000], Loss: 0.5619, Val Loss: 0.4511\n",
      "Epoch [460/1000], Loss: 1.1232, Val Loss: 0.4511\n",
      "Epoch [461/1000], Loss: 2.1086, Val Loss: 0.4511\n",
      "Epoch [462/1000], Loss: 0.0834, Val Loss: 0.4510\n",
      "Epoch [463/1000], Loss: 0.2849, Val Loss: 0.4510\n",
      "Epoch [464/1000], Loss: 0.6430, Val Loss: 0.4510\n",
      "Epoch [465/1000], Loss: 0.0069, Val Loss: 0.4509\n",
      "Epoch [466/1000], Loss: 0.0882, Val Loss: 0.4509\n",
      "Epoch [467/1000], Loss: 0.0227, Val Loss: 0.4509\n",
      "Epoch [468/1000], Loss: 2.4231, Val Loss: 0.4509\n",
      "Epoch [469/1000], Loss: 0.2879, Val Loss: 0.4509\n",
      "Epoch [470/1000], Loss: 0.4143, Val Loss: 0.4509\n",
      "Epoch [471/1000], Loss: 0.1333, Val Loss: 0.4508\n",
      "Epoch [472/1000], Loss: 0.0267, Val Loss: 0.4508\n",
      "Epoch [473/1000], Loss: 0.1641, Val Loss: 0.4508\n",
      "Epoch [474/1000], Loss: 0.1414, Val Loss: 0.4508\n",
      "Epoch [475/1000], Loss: 0.1852, Val Loss: 0.4508\n",
      "Epoch [476/1000], Loss: 0.3830, Val Loss: 0.4508\n",
      "Epoch [477/1000], Loss: 0.1160, Val Loss: 0.4507\n",
      "Epoch [478/1000], Loss: 0.6209, Val Loss: 0.4507\n",
      "Epoch [479/1000], Loss: 0.0807, Val Loss: 0.4507\n",
      "Epoch [480/1000], Loss: 1.0486, Val Loss: 0.4507\n",
      "Epoch [481/1000], Loss: 1.6694, Val Loss: 0.4507\n",
      "Epoch [482/1000], Loss: 0.2849, Val Loss: 0.4507\n",
      "Epoch [483/1000], Loss: 0.0102, Val Loss: 0.4507\n",
      "Epoch [484/1000], Loss: 0.0079, Val Loss: 0.4506\n",
      "Epoch [485/1000], Loss: 0.0818, Val Loss: 0.4506\n",
      "Epoch [486/1000], Loss: 0.0894, Val Loss: 0.4506\n",
      "Epoch [487/1000], Loss: 0.2769, Val Loss: 0.4506\n",
      "Epoch [488/1000], Loss: 1.1705, Val Loss: 0.4505\n",
      "Epoch [489/1000], Loss: 0.1067, Val Loss: 0.4505\n",
      "Epoch [490/1000], Loss: 0.5664, Val Loss: 0.4505\n",
      "Epoch [491/1000], Loss: 0.1835, Val Loss: 0.4505\n",
      "Epoch [492/1000], Loss: 0.4098, Val Loss: 0.4505\n",
      "Epoch [493/1000], Loss: 0.6746, Val Loss: 0.4505\n",
      "Epoch [494/1000], Loss: 0.2856, Val Loss: 0.4504\n",
      "Epoch [495/1000], Loss: 0.0352, Val Loss: 0.4504\n",
      "Epoch [496/1000], Loss: 0.0040, Val Loss: 0.4504\n",
      "Epoch [497/1000], Loss: 0.4990, Val Loss: 0.4504\n",
      "Epoch [498/1000], Loss: 0.0224, Val Loss: 0.4504\n",
      "Epoch [499/1000], Loss: 0.7504, Val Loss: 0.4504\n",
      "Epoch [500/1000], Loss: 1.8451, Val Loss: 0.4504\n",
      "Epoch [501/1000], Loss: 0.0431, Val Loss: 0.4504\n",
      "Epoch [502/1000], Loss: 1.7977, Val Loss: 0.4503\n",
      "Epoch [503/1000], Loss: 0.8921, Val Loss: 0.4503\n",
      "Epoch [504/1000], Loss: 0.8000, Val Loss: 0.4503\n",
      "Epoch [505/1000], Loss: 0.4038, Val Loss: 0.4503\n",
      "Epoch [506/1000], Loss: 0.0009, Val Loss: 0.4503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [507/1000], Loss: 0.1861, Val Loss: 0.4502\n",
      "Epoch [508/1000], Loss: 0.2917, Val Loss: 0.4502\n",
      "Epoch [509/1000], Loss: 0.2728, Val Loss: 0.4502\n",
      "Epoch [510/1000], Loss: 0.8183, Val Loss: 0.4502\n",
      "Epoch [511/1000], Loss: 0.3153, Val Loss: 0.4502\n",
      "Epoch [512/1000], Loss: 1.3206, Val Loss: 0.4502\n",
      "Epoch [513/1000], Loss: 0.7348, Val Loss: 0.4502\n",
      "Epoch [514/1000], Loss: 0.6350, Val Loss: 0.4501\n",
      "Epoch [515/1000], Loss: 0.0546, Val Loss: 0.4501\n",
      "Epoch [516/1000], Loss: 0.4213, Val Loss: 0.4501\n",
      "Epoch [517/1000], Loss: 0.0562, Val Loss: 0.4501\n",
      "Epoch [518/1000], Loss: 0.2485, Val Loss: 0.4501\n",
      "Epoch [519/1000], Loss: 0.0420, Val Loss: 0.4501\n",
      "Epoch [520/1000], Loss: 1.3992, Val Loss: 0.4501\n",
      "Epoch [521/1000], Loss: 0.0041, Val Loss: 0.4501\n",
      "Epoch [522/1000], Loss: 0.1234, Val Loss: 0.4501\n",
      "Epoch [523/1000], Loss: 0.3718, Val Loss: 0.4501\n",
      "Epoch [524/1000], Loss: 0.3870, Val Loss: 0.4501\n",
      "Epoch [525/1000], Loss: 0.0142, Val Loss: 0.4500\n",
      "Epoch [526/1000], Loss: 0.4230, Val Loss: 0.4501\n",
      "Epoch [527/1000], Loss: 0.9120, Val Loss: 0.4500\n",
      "Epoch [528/1000], Loss: 0.1069, Val Loss: 0.4500\n",
      "Epoch [529/1000], Loss: 0.3600, Val Loss: 0.4500\n",
      "Epoch [530/1000], Loss: 0.1300, Val Loss: 0.4500\n",
      "Epoch [531/1000], Loss: 0.0886, Val Loss: 0.4500\n",
      "Epoch [532/1000], Loss: 0.0154, Val Loss: 0.4500\n",
      "Epoch [533/1000], Loss: 0.1284, Val Loss: 0.4500\n",
      "Epoch [534/1000], Loss: 0.0609, Val Loss: 0.4500\n",
      "Epoch [535/1000], Loss: 0.0170, Val Loss: 0.4500\n",
      "Epoch [536/1000], Loss: 0.2093, Val Loss: 0.4500\n",
      "Epoch [537/1000], Loss: 0.0188, Val Loss: 0.4500\n",
      "Epoch [538/1000], Loss: 0.9401, Val Loss: 0.4499\n",
      "Epoch [539/1000], Loss: 0.0064, Val Loss: 0.4499\n",
      "Epoch [540/1000], Loss: 0.1319, Val Loss: 0.4499\n",
      "Epoch [541/1000], Loss: 0.6415, Val Loss: 0.4499\n",
      "Epoch [542/1000], Loss: 1.3889, Val Loss: 0.4499\n",
      "Epoch [543/1000], Loss: 0.5481, Val Loss: 0.4498\n",
      "Epoch [544/1000], Loss: 0.0300, Val Loss: 0.4498\n",
      "Epoch [545/1000], Loss: 0.1105, Val Loss: 0.4498\n",
      "Epoch [546/1000], Loss: 0.4763, Val Loss: 0.4498\n",
      "Epoch [547/1000], Loss: 0.0414, Val Loss: 0.4498\n",
      "Epoch [548/1000], Loss: 0.3084, Val Loss: 0.4498\n",
      "Epoch [549/1000], Loss: 0.6148, Val Loss: 0.4498\n",
      "Epoch [550/1000], Loss: 0.0065, Val Loss: 0.4498\n",
      "Epoch [551/1000], Loss: 0.2419, Val Loss: 0.4498\n",
      "Epoch [552/1000], Loss: 0.0287, Val Loss: 0.4498\n",
      "Epoch [553/1000], Loss: 0.8668, Val Loss: 0.4498\n",
      "Epoch [554/1000], Loss: 0.1873, Val Loss: 0.4498\n",
      "Epoch [555/1000], Loss: 0.4001, Val Loss: 0.4498\n",
      "Epoch [556/1000], Loss: 0.2242, Val Loss: 0.4497\n",
      "Epoch [557/1000], Loss: 0.0575, Val Loss: 0.4497\n",
      "Epoch [558/1000], Loss: 3.1231, Val Loss: 0.4497\n",
      "Epoch [559/1000], Loss: 0.0878, Val Loss: 0.4497\n",
      "Epoch [560/1000], Loss: 0.0286, Val Loss: 0.4497\n",
      "Epoch [561/1000], Loss: 0.2860, Val Loss: 0.4497\n",
      "Epoch [562/1000], Loss: 0.0895, Val Loss: 0.4496\n",
      "Epoch [563/1000], Loss: 0.2875, Val Loss: 0.4496\n",
      "Epoch [564/1000], Loss: 0.9633, Val Loss: 0.4496\n",
      "Epoch [565/1000], Loss: 0.0691, Val Loss: 0.4496\n",
      "Epoch [566/1000], Loss: 0.1875, Val Loss: 0.4496\n",
      "Epoch [567/1000], Loss: 0.3938, Val Loss: 0.4496\n",
      "Epoch [568/1000], Loss: 0.0403, Val Loss: 0.4496\n",
      "Epoch [569/1000], Loss: 0.3087, Val Loss: 0.4496\n",
      "Epoch [570/1000], Loss: 0.0783, Val Loss: 0.4496\n",
      "Epoch [571/1000], Loss: 0.4786, Val Loss: 0.4496\n",
      "Epoch [572/1000], Loss: 0.4148, Val Loss: 0.4495\n",
      "Epoch [573/1000], Loss: 1.4489, Val Loss: 0.4495\n",
      "Epoch [574/1000], Loss: 0.0327, Val Loss: 0.4495\n",
      "Epoch [575/1000], Loss: 0.2505, Val Loss: 0.4495\n",
      "Epoch [576/1000], Loss: 0.5362, Val Loss: 0.4495\n",
      "Epoch [577/1000], Loss: 0.0386, Val Loss: 0.4495\n",
      "Epoch [578/1000], Loss: 0.0134, Val Loss: 0.4494\n",
      "Epoch [579/1000], Loss: 1.0274, Val Loss: 0.4494\n",
      "Epoch [580/1000], Loss: 0.2284, Val Loss: 0.4494\n",
      "Epoch [581/1000], Loss: 0.8217, Val Loss: 0.4494\n",
      "Epoch [582/1000], Loss: 0.6497, Val Loss: 0.4494\n",
      "Epoch [583/1000], Loss: 0.2941, Val Loss: 0.4494\n",
      "Epoch [584/1000], Loss: 0.3339, Val Loss: 0.4494\n",
      "Epoch [585/1000], Loss: 0.2501, Val Loss: 0.4494\n",
      "Epoch [586/1000], Loss: 0.0222, Val Loss: 0.4494\n",
      "Epoch [587/1000], Loss: 0.2307, Val Loss: 0.4494\n",
      "Epoch [588/1000], Loss: 0.0621, Val Loss: 0.4494\n",
      "Epoch [589/1000], Loss: 0.9346, Val Loss: 0.4493\n",
      "Epoch [590/1000], Loss: 0.9482, Val Loss: 0.4493\n",
      "Epoch [591/1000], Loss: 0.7196, Val Loss: 0.4493\n",
      "Epoch [592/1000], Loss: 0.7866, Val Loss: 0.4493\n",
      "Epoch [593/1000], Loss: 0.5435, Val Loss: 0.4493\n",
      "Epoch [594/1000], Loss: 0.3212, Val Loss: 0.4493\n",
      "Epoch [595/1000], Loss: 0.0029, Val Loss: 0.4493\n",
      "Epoch [596/1000], Loss: 0.0210, Val Loss: 0.4493\n",
      "Epoch [597/1000], Loss: 0.6229, Val Loss: 0.4493\n",
      "Epoch [598/1000], Loss: 0.1263, Val Loss: 0.4493\n",
      "Epoch [599/1000], Loss: 0.8587, Val Loss: 0.4493\n",
      "Epoch [600/1000], Loss: 0.3462, Val Loss: 0.4493\n",
      "Epoch [601/1000], Loss: 0.3044, Val Loss: 0.4492\n",
      "Epoch [602/1000], Loss: 0.0363, Val Loss: 0.4492\n",
      "Epoch [603/1000], Loss: 0.1322, Val Loss: 0.4492\n",
      "Epoch [604/1000], Loss: 0.0301, Val Loss: 0.4492\n",
      "Epoch [605/1000], Loss: 1.0266, Val Loss: 0.4492\n",
      "Epoch [606/1000], Loss: 0.5201, Val Loss: 0.4492\n",
      "Epoch [607/1000], Loss: 1.6522, Val Loss: 0.4492\n",
      "Epoch [608/1000], Loss: 0.0653, Val Loss: 0.4492\n",
      "Epoch [609/1000], Loss: 0.1170, Val Loss: 0.4492\n",
      "Epoch [610/1000], Loss: 0.8257, Val Loss: 0.4491\n",
      "Epoch [611/1000], Loss: 1.0584, Val Loss: 0.4491\n",
      "Epoch [612/1000], Loss: 0.6540, Val Loss: 0.4491\n",
      "Epoch [613/1000], Loss: 0.0269, Val Loss: 0.4491\n",
      "Epoch [614/1000], Loss: 0.0164, Val Loss: 0.4491\n",
      "Epoch [615/1000], Loss: 0.0702, Val Loss: 0.4491\n",
      "Epoch [616/1000], Loss: 0.6796, Val Loss: 0.4491\n",
      "Epoch [617/1000], Loss: 0.8696, Val Loss: 0.4491\n",
      "Epoch [618/1000], Loss: 0.0242, Val Loss: 0.4491\n",
      "Epoch [619/1000], Loss: 0.0705, Val Loss: 0.4490\n",
      "Epoch [620/1000], Loss: 0.1752, Val Loss: 0.4490\n",
      "Epoch [621/1000], Loss: 0.8946, Val Loss: 0.4490\n",
      "Epoch [622/1000], Loss: 0.0695, Val Loss: 0.4490\n",
      "Epoch [623/1000], Loss: 1.4788, Val Loss: 0.4490\n",
      "Epoch [624/1000], Loss: 0.5123, Val Loss: 0.4490\n",
      "Epoch [625/1000], Loss: 0.1632, Val Loss: 0.4490\n",
      "Epoch [626/1000], Loss: 0.0039, Val Loss: 0.4490\n",
      "Epoch [627/1000], Loss: 0.0901, Val Loss: 0.4490\n",
      "Epoch [628/1000], Loss: 0.4631, Val Loss: 0.4489\n",
      "Epoch [629/1000], Loss: 0.0369, Val Loss: 0.4489\n",
      "Epoch [630/1000], Loss: 0.1939, Val Loss: 0.4489\n",
      "Epoch [631/1000], Loss: 0.0147, Val Loss: 0.4489\n",
      "Epoch [632/1000], Loss: 0.4543, Val Loss: 0.4489\n",
      "Epoch [633/1000], Loss: 0.4907, Val Loss: 0.4489\n",
      "Epoch [634/1000], Loss: 0.0832, Val Loss: 0.4489\n",
      "Epoch [635/1000], Loss: 0.0171, Val Loss: 0.4489\n",
      "Epoch [636/1000], Loss: 0.0112, Val Loss: 0.4489\n",
      "Epoch [637/1000], Loss: 0.0787, Val Loss: 0.4489\n",
      "Epoch [638/1000], Loss: 0.9385, Val Loss: 0.4489\n",
      "Epoch [639/1000], Loss: 0.9502, Val Loss: 0.4489\n",
      "Epoch [640/1000], Loss: 0.0674, Val Loss: 0.4489\n",
      "Epoch [641/1000], Loss: 0.0461, Val Loss: 0.4489\n",
      "Epoch [642/1000], Loss: 0.0569, Val Loss: 0.4489\n",
      "Epoch [643/1000], Loss: 0.5978, Val Loss: 0.4489\n",
      "Epoch [644/1000], Loss: 0.0285, Val Loss: 0.4489\n",
      "Epoch [645/1000], Loss: 0.6149, Val Loss: 0.4489\n",
      "Epoch [646/1000], Loss: 0.0010, Val Loss: 0.4489\n",
      "Epoch [647/1000], Loss: 0.0083, Val Loss: 0.4489\n",
      "Epoch [648/1000], Loss: 0.0831, Val Loss: 0.4489\n",
      "Epoch [649/1000], Loss: 1.5651, Val Loss: 0.4489\n",
      "Epoch [650/1000], Loss: 2.3309, Val Loss: 0.4489\n",
      "Epoch [651/1000], Loss: 0.9872, Val Loss: 0.4488\n",
      "Epoch [652/1000], Loss: 0.1831, Val Loss: 0.4489\n",
      "Epoch [653/1000], Loss: 0.0521, Val Loss: 0.4489\n",
      "Epoch [654/1000], Loss: 0.0142, Val Loss: 0.4488\n",
      "Epoch [655/1000], Loss: 0.3127, Val Loss: 0.4488\n",
      "Epoch [656/1000], Loss: 0.0239, Val Loss: 0.4488\n",
      "Epoch [657/1000], Loss: 0.8710, Val Loss: 0.4488\n",
      "Epoch [658/1000], Loss: 1.0023, Val Loss: 0.4488\n",
      "Epoch [659/1000], Loss: 0.0338, Val Loss: 0.4489\n",
      "Epoch [660/1000], Loss: 0.7309, Val Loss: 0.4489\n",
      "Early stopping triggered at epoch 661\n",
      "\n",
      "Оценка ROC-AUC для валидации (Baseline): 0.8215\n"
     ]
    }
   ],
   "source": [
    "# Определение модели нейронной сети\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Инициализация нейронной сети\n",
    "model = NeuralNetwork(X_train.shape[1]).to(device)\n",
    "\n",
    "# Потеря и оптимизатор с регуляризацией L2 (снижение веса)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "\n",
    "# Параметры ранней остановки\n",
    "early_stopping_patience = 10  # Количество эпох ожидания после того, как в последний раз улучшилась валидационная потеря.\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Обучение нейронной сети с ранней остановкой\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Валидационная потеря\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(X_val_tensor.to(device))\n",
    "        val_loss = criterion(y_val_pred, y_val_tensor.to(device).unsqueeze(1))\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Оценка\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val_pred = model(X_val_tensor.to(device))\n",
    "    y_val_pred = y_val_pred.cpu().numpy()\n",
    "    val_roc_score = roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "# Распечатаем или сохраним проверочный показатель ROC-AUC в качестве базовой метрики.\n",
    "print()\n",
    "print(f\"Оценка ROC-AUC для валидации (Baseline): {val_roc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель нейронной сети, обученная с помощью PyTorch, показала многообещающие результаты (**0.82 ROC-AUC**), однако обучение было приостановлено т.к. потери не улучшались после определенного количества эпох! Но для baseline модели довольно приличный результат, поэтому далее будем использовать Optuna для тюнинга гиперпараметров данной модели. Что было сделано:\n",
    "\n",
    "1. **Регуляризация:**\n",
    "    - Добавлены выпадающие слои («nn.Dropout») после некоторых линейных слоев в классе NeuralNetwork. Это случайным образом обнуляет часть входных признаков во время обучения, что помогает предотвратить переобучение.\n",
    "    - В оптимизаторе добавлена регуляризация L2 (снижение веса). Это наказывает большие веса и помогает предотвратить переобучение.\n",
    "\n",
    "2. **Ранняя остановка:**\n",
    "    - Введены параметры для мониторинга потерь при проверке и прекращения обучения, если потери не улучшаются после определенного количества эпох («early_stopping_patience»).\n",
    "    - В течение каждой эпохи после обучения модель оценивается на проверочном наборе. Если потери при проверке не уменьшаются в течение указанного количества последовательных эпох, обучение прекращается, чтобы предотвратить переобучение.\n",
    "\n",
    "Модель была обучена с добавленной регуляризацией отсева и реализована ранняя остановка на основе потерь при валидации. Этот подход часто приводит к более обобщаемой модели, которая лучше работает с невидимыми ранее данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:06:23,587] A new study created in memory with name: no-name-5b45d8d3-aa57-493d-bff7-dfc319af812f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.0623, Val Loss: 0.4679\n",
      "Epoch [2/1000], Loss: 0.1872, Val Loss: 0.4519\n",
      "Epoch [3/1000], Loss: 1.6159, Val Loss: 0.4534\n",
      "Epoch [4/1000], Loss: 0.0138, Val Loss: 0.4496\n",
      "Epoch [5/1000], Loss: 0.9760, Val Loss: 0.4551\n",
      "Epoch [6/1000], Loss: 0.2310, Val Loss: 0.4534\n",
      "Epoch [7/1000], Loss: 0.3264, Val Loss: 0.4596\n",
      "Epoch [8/1000], Loss: 0.0441, Val Loss: 0.4509\n",
      "Epoch [9/1000], Loss: 0.3563, Val Loss: 0.4618\n",
      "Epoch [10/1000], Loss: 0.0426, Val Loss: 0.4544\n",
      "Epoch [11/1000], Loss: 0.0243, Val Loss: 0.4516\n",
      "Epoch [12/1000], Loss: 0.8996, Val Loss: 0.4466\n",
      "Epoch [13/1000], Loss: 0.3263, Val Loss: 0.4485\n",
      "Epoch [14/1000], Loss: 0.7928, Val Loss: 0.4479\n",
      "Epoch [15/1000], Loss: 0.6268, Val Loss: 0.4479\n",
      "Epoch [16/1000], Loss: 0.0043, Val Loss: 0.4504\n",
      "Epoch [17/1000], Loss: 0.4201, Val Loss: 0.4599\n",
      "Epoch [18/1000], Loss: 0.5541, Val Loss: 0.4531\n",
      "Epoch [19/1000], Loss: 0.4758, Val Loss: 0.4583\n",
      "Epoch [20/1000], Loss: 0.5896, Val Loss: 0.4556\n",
      "Epoch [21/1000], Loss: 0.1766, Val Loss: 0.4603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:06:29,817] Trial 0 finished with value: 0.8198305439010588 and parameters: {'hidden_size': 26, 'dropout_prob': 0.38689108924783444, 'learning_rate': 0.007831404177489183, 'weight_decay': 5.739791571493828e-05}. Best is trial 0 with value: 0.8198305439010588.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 22\n",
      "Epoch [1/1000], Loss: 0.7050, Val Loss: 0.4555\n",
      "Epoch [2/1000], Loss: 0.4272, Val Loss: 0.4554\n",
      "Epoch [3/1000], Loss: 0.6105, Val Loss: 0.4662\n",
      "Epoch [4/1000], Loss: 0.2800, Val Loss: 0.4464\n",
      "Epoch [5/1000], Loss: 0.3104, Val Loss: 0.4500\n",
      "Epoch [6/1000], Loss: 0.0079, Val Loss: 0.4506\n",
      "Epoch [7/1000], Loss: 0.9012, Val Loss: 0.4508\n",
      "Epoch [8/1000], Loss: 1.0533, Val Loss: 0.4518\n",
      "Epoch [9/1000], Loss: 0.2181, Val Loss: 0.4455\n",
      "Epoch [10/1000], Loss: 0.3596, Val Loss: 0.4530\n",
      "Epoch [11/1000], Loss: 0.3083, Val Loss: 0.4561\n",
      "Epoch [12/1000], Loss: 0.0463, Val Loss: 0.4490\n",
      "Epoch [13/1000], Loss: 0.3624, Val Loss: 0.4514\n",
      "Epoch [14/1000], Loss: 2.2146, Val Loss: 0.4572\n",
      "Epoch [15/1000], Loss: 1.0226, Val Loss: 0.4542\n",
      "Epoch [16/1000], Loss: 0.2873, Val Loss: 0.4594\n",
      "Epoch [17/1000], Loss: 2.9656, Val Loss: 0.4868\n",
      "Epoch [18/1000], Loss: 0.1244, Val Loss: 0.4649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:06:35,129] Trial 1 finished with value: 0.8183546852776092 and parameters: {'hidden_size': 109, 'dropout_prob': 0.4177091843400278, 'learning_rate': 0.00824620935109586, 'weight_decay': 4.118397744055653e-05}. Best is trial 0 with value: 0.8198305439010588.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 19\n",
      "Epoch [1/1000], Loss: 0.1183, Val Loss: 0.4502\n",
      "Epoch [2/1000], Loss: 0.2326, Val Loss: 0.4501\n",
      "Epoch [3/1000], Loss: 0.2311, Val Loss: 0.4495\n",
      "Epoch [4/1000], Loss: 0.3995, Val Loss: 0.4512\n",
      "Epoch [5/1000], Loss: 0.5637, Val Loss: 0.4519\n",
      "Epoch [6/1000], Loss: 0.4134, Val Loss: 0.4495\n",
      "Epoch [7/1000], Loss: 0.7676, Val Loss: 0.4495\n",
      "Epoch [8/1000], Loss: 0.5175, Val Loss: 0.4528\n",
      "Epoch [9/1000], Loss: 0.0548, Val Loss: 0.4528\n",
      "Epoch [10/1000], Loss: 1.4342, Val Loss: 0.4473\n",
      "Epoch [11/1000], Loss: 0.0254, Val Loss: 0.4550\n",
      "Epoch [12/1000], Loss: 1.2450, Val Loss: 0.4520\n",
      "Epoch [13/1000], Loss: 0.1501, Val Loss: 0.4574\n",
      "Epoch [14/1000], Loss: 1.2408, Val Loss: 0.4488\n",
      "Epoch [15/1000], Loss: 0.2457, Val Loss: 0.4510\n",
      "Epoch [16/1000], Loss: 0.1712, Val Loss: 0.4480\n",
      "Epoch [17/1000], Loss: 0.7298, Val Loss: 0.4516\n",
      "Epoch [18/1000], Loss: 0.5798, Val Loss: 0.4474\n",
      "Epoch [19/1000], Loss: 0.0877, Val Loss: 0.4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:06:40,489] Trial 2 finished with value: 0.8211764046146709 and parameters: {'hidden_size': 62, 'dropout_prob': 0.3671135122865846, 'learning_rate': 0.004621296273856339, 'weight_decay': 6.745981058546085e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 20\n",
      "Epoch [1/1000], Loss: 0.4739, Val Loss: 0.5282\n",
      "Epoch [2/1000], Loss: 0.5273, Val Loss: 0.4978\n",
      "Epoch [3/1000], Loss: 0.1559, Val Loss: 0.4857\n",
      "Epoch [4/1000], Loss: 0.8650, Val Loss: 0.4824\n",
      "Epoch [5/1000], Loss: 0.5321, Val Loss: 0.4719\n",
      "Epoch [6/1000], Loss: 0.9770, Val Loss: 0.4824\n",
      "Epoch [7/1000], Loss: 0.0552, Val Loss: 0.4736\n",
      "Epoch [8/1000], Loss: 0.0812, Val Loss: 0.4798\n",
      "Epoch [9/1000], Loss: 0.7125, Val Loss: 0.4636\n",
      "Epoch [10/1000], Loss: 1.3560, Val Loss: 0.4609\n",
      "Epoch [11/1000], Loss: 0.0072, Val Loss: 0.4693\n",
      "Epoch [12/1000], Loss: 0.2566, Val Loss: 0.4717\n",
      "Epoch [13/1000], Loss: 0.5763, Val Loss: 0.4687\n",
      "Epoch [14/1000], Loss: 0.0221, Val Loss: 0.4743\n",
      "Epoch [15/1000], Loss: 1.0718, Val Loss: 0.4704\n",
      "Epoch [16/1000], Loss: 0.8530, Val Loss: 0.4734\n",
      "Epoch [17/1000], Loss: 0.6908, Val Loss: 0.4624\n",
      "Epoch [18/1000], Loss: 0.4215, Val Loss: 0.4595\n",
      "Epoch [19/1000], Loss: 0.0031, Val Loss: 0.4628\n",
      "Epoch [20/1000], Loss: 0.0923, Val Loss: 0.4595\n",
      "Epoch [21/1000], Loss: 0.0569, Val Loss: 0.4605\n",
      "Epoch [22/1000], Loss: 0.0877, Val Loss: 0.4676\n",
      "Epoch [23/1000], Loss: 1.5239, Val Loss: 0.4701\n",
      "Epoch [24/1000], Loss: 0.0925, Val Loss: 0.4684\n",
      "Epoch [25/1000], Loss: 0.0842, Val Loss: 0.4685\n",
      "Epoch [26/1000], Loss: 0.1844, Val Loss: 0.4712\n",
      "Epoch [27/1000], Loss: 0.0583, Val Loss: 0.4720\n",
      "Epoch [28/1000], Loss: 0.7425, Val Loss: 0.4804\n",
      "Epoch [29/1000], Loss: 1.7748, Val Loss: 0.4689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:06:48,067] Trial 3 finished with value: 0.818698797391885 and parameters: {'hidden_size': 19, 'dropout_prob': 0.7235404784839259, 'learning_rate': 0.00100409593627036, 'weight_decay': 5.384127404838972e-06}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 30\n",
      "Epoch [1/1000], Loss: 0.3664, Val Loss: 0.4803\n",
      "Epoch [2/1000], Loss: 0.1709, Val Loss: 0.4642\n",
      "Epoch [3/1000], Loss: 0.9967, Val Loss: 0.4649\n",
      "Epoch [4/1000], Loss: 0.0770, Val Loss: 0.4635\n",
      "Epoch [5/1000], Loss: 0.2689, Val Loss: 0.4542\n",
      "Epoch [6/1000], Loss: 1.2883, Val Loss: 0.4560\n",
      "Epoch [7/1000], Loss: 0.5744, Val Loss: 0.4699\n",
      "Epoch [8/1000], Loss: 3.5825, Val Loss: 0.4765\n",
      "Epoch [9/1000], Loss: 0.0584, Val Loss: 0.4518\n",
      "Epoch [10/1000], Loss: 0.6368, Val Loss: 0.4562\n",
      "Epoch [11/1000], Loss: 0.6445, Val Loss: 0.4572\n",
      "Epoch [12/1000], Loss: 0.1585, Val Loss: 0.4539\n",
      "Epoch [13/1000], Loss: 0.2833, Val Loss: 0.4714\n",
      "Epoch [14/1000], Loss: 0.1289, Val Loss: 0.4599\n",
      "Epoch [15/1000], Loss: 0.5125, Val Loss: 0.4560\n",
      "Epoch [16/1000], Loss: 0.2043, Val Loss: 0.4599\n",
      "Epoch [17/1000], Loss: 0.6804, Val Loss: 0.4622\n",
      "Epoch [18/1000], Loss: 0.2398, Val Loss: 0.4639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:06:53,414] Trial 4 finished with value: 0.8199783846612662 and parameters: {'hidden_size': 28, 'dropout_prob': 0.7815844004588097, 'learning_rate': 0.005560377686134362, 'weight_decay': 1.7352735013882365e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 19\n",
      "Epoch [1/1000], Loss: 0.4200, Val Loss: 0.4575\n",
      "Epoch [2/1000], Loss: 0.6280, Val Loss: 0.4567\n",
      "Epoch [3/1000], Loss: 0.0166, Val Loss: 0.4551\n",
      "Epoch [4/1000], Loss: 0.5054, Val Loss: 0.4561\n",
      "Epoch [5/1000], Loss: 0.0021, Val Loss: 0.4538\n",
      "Epoch [6/1000], Loss: 0.6079, Val Loss: 0.4493\n",
      "Epoch [7/1000], Loss: 0.3401, Val Loss: 0.4481\n",
      "Epoch [8/1000], Loss: 0.0175, Val Loss: 0.4482\n",
      "Epoch [9/1000], Loss: 0.1660, Val Loss: 0.4468\n",
      "Epoch [10/1000], Loss: 0.0171, Val Loss: 0.4483\n",
      "Epoch [11/1000], Loss: 0.0215, Val Loss: 0.4492\n",
      "Epoch [12/1000], Loss: 0.4285, Val Loss: 0.4498\n",
      "Epoch [13/1000], Loss: 1.0477, Val Loss: 0.4512\n",
      "Epoch [14/1000], Loss: 0.3647, Val Loss: 0.4508\n",
      "Epoch [15/1000], Loss: 0.0595, Val Loss: 0.4639\n",
      "Epoch [16/1000], Loss: 0.9850, Val Loss: 0.4551\n",
      "Epoch [17/1000], Loss: 0.0072, Val Loss: 0.4520\n",
      "Epoch [18/1000], Loss: 0.4094, Val Loss: 0.4524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:06:59,422] Trial 5 finished with value: 0.8176511671773121 and parameters: {'hidden_size': 102, 'dropout_prob': 0.2795778295058234, 'learning_rate': 0.004744692773074152, 'weight_decay': 3.798714971616688e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 19\n",
      "Epoch [1/1000], Loss: 0.0014, Val Loss: 0.4552\n",
      "Epoch [2/1000], Loss: 1.0594, Val Loss: 0.4556\n",
      "Epoch [3/1000], Loss: 0.3531, Val Loss: 0.4551\n",
      "Epoch [4/1000], Loss: 0.9780, Val Loss: 0.4546\n",
      "Epoch [5/1000], Loss: 0.0243, Val Loss: 0.4526\n",
      "Epoch [6/1000], Loss: 0.6994, Val Loss: 0.4585\n",
      "Epoch [7/1000], Loss: 0.2175, Val Loss: 0.4505\n",
      "Epoch [8/1000], Loss: 0.0644, Val Loss: 0.4534\n",
      "Epoch [9/1000], Loss: 0.4034, Val Loss: 0.4584\n",
      "Epoch [10/1000], Loss: 0.0009, Val Loss: 0.4510\n",
      "Epoch [11/1000], Loss: 0.6589, Val Loss: 0.4573\n",
      "Epoch [12/1000], Loss: 0.3440, Val Loss: 0.4510\n",
      "Epoch [13/1000], Loss: 0.2164, Val Loss: 0.4516\n",
      "Epoch [14/1000], Loss: 0.0841, Val Loss: 0.4507\n",
      "Epoch [15/1000], Loss: 0.1576, Val Loss: 0.4512\n",
      "Epoch [16/1000], Loss: 1.0063, Val Loss: 0.4564\n",
      "Epoch [17/1000], Loss: 0.5834, Val Loss: 0.4488\n",
      "Epoch [18/1000], Loss: 0.4039, Val Loss: 0.4492\n",
      "Epoch [19/1000], Loss: 0.0429, Val Loss: 0.4479\n",
      "Epoch [20/1000], Loss: 0.2219, Val Loss: 0.4504\n",
      "Epoch [21/1000], Loss: 0.4005, Val Loss: 0.4496\n",
      "Epoch [22/1000], Loss: 0.8122, Val Loss: 0.4576\n",
      "Epoch [23/1000], Loss: 0.0204, Val Loss: 0.4565\n",
      "Epoch [24/1000], Loss: 2.0156, Val Loss: 0.4488\n",
      "Epoch [25/1000], Loss: 0.0020, Val Loss: 0.4493\n",
      "Epoch [26/1000], Loss: 5.4995, Val Loss: 0.4513\n",
      "Epoch [27/1000], Loss: 0.1131, Val Loss: 0.4565\n",
      "Epoch [28/1000], Loss: 0.1087, Val Loss: 0.4551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:07:09,151] Trial 6 finished with value: 0.8145694520205754 and parameters: {'hidden_size': 122, 'dropout_prob': 0.431927567617601, 'learning_rate': 0.008129404992721318, 'weight_decay': 6.649290843136876e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 29\n",
      "Epoch [1/1000], Loss: 0.3600, Val Loss: 0.4621\n",
      "Epoch [2/1000], Loss: 0.8007, Val Loss: 0.4548\n",
      "Epoch [3/1000], Loss: 0.5003, Val Loss: 0.4568\n",
      "Epoch [4/1000], Loss: 0.8810, Val Loss: 0.4589\n",
      "Epoch [5/1000], Loss: 0.0160, Val Loss: 0.4522\n",
      "Epoch [6/1000], Loss: 0.0363, Val Loss: 0.4516\n",
      "Epoch [7/1000], Loss: 0.3988, Val Loss: 0.4488\n",
      "Epoch [8/1000], Loss: 1.2139, Val Loss: 0.4538\n",
      "Epoch [9/1000], Loss: 0.5086, Val Loss: 0.4521\n",
      "Epoch [10/1000], Loss: 0.2327, Val Loss: 0.4534\n",
      "Epoch [11/1000], Loss: 0.9392, Val Loss: 0.4501\n",
      "Epoch [12/1000], Loss: 0.3565, Val Loss: 0.4513\n",
      "Epoch [13/1000], Loss: 1.2467, Val Loss: 0.4515\n",
      "Epoch [14/1000], Loss: 0.0666, Val Loss: 0.4490\n",
      "Epoch [15/1000], Loss: 0.0006, Val Loss: 0.4527\n",
      "Epoch [16/1000], Loss: 0.3649, Val Loss: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:07:14,323] Trial 7 finished with value: 0.8208093516927767 and parameters: {'hidden_size': 119, 'dropout_prob': 0.715721714291994, 'learning_rate': 0.00630984012073332, 'weight_decay': 2.073129350428529e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 17\n",
      "Epoch [1/1000], Loss: 0.6188, Val Loss: 0.4606\n",
      "Epoch [2/1000], Loss: 0.0157, Val Loss: 0.4520\n",
      "Epoch [3/1000], Loss: 0.3714, Val Loss: 0.4561\n",
      "Epoch [4/1000], Loss: 1.0864, Val Loss: 0.4507\n",
      "Epoch [5/1000], Loss: 0.0533, Val Loss: 0.4505\n",
      "Epoch [6/1000], Loss: 0.6248, Val Loss: 0.4497\n",
      "Epoch [7/1000], Loss: 0.0636, Val Loss: 0.4489\n",
      "Epoch [8/1000], Loss: 1.7633, Val Loss: 0.4520\n",
      "Epoch [9/1000], Loss: 0.0177, Val Loss: 0.4495\n",
      "Epoch [10/1000], Loss: 0.2162, Val Loss: 0.4499\n",
      "Epoch [11/1000], Loss: 0.4467, Val Loss: 0.4504\n",
      "Epoch [12/1000], Loss: 0.6680, Val Loss: 0.4496\n",
      "Epoch [13/1000], Loss: 0.3643, Val Loss: 0.4564\n",
      "Epoch [14/1000], Loss: 0.6195, Val Loss: 0.4491\n",
      "Epoch [15/1000], Loss: 0.2201, Val Loss: 0.4501\n",
      "Epoch [16/1000], Loss: 0.0244, Val Loss: 0.4485\n",
      "Epoch [17/1000], Loss: 0.2837, Val Loss: 0.4535\n",
      "Epoch [18/1000], Loss: 1.3040, Val Loss: 0.4493\n",
      "Epoch [19/1000], Loss: 0.0576, Val Loss: 0.4486\n",
      "Epoch [20/1000], Loss: 1.4985, Val Loss: 0.4518\n",
      "Epoch [21/1000], Loss: 0.1327, Val Loss: 0.4545\n",
      "Epoch [22/1000], Loss: 0.0976, Val Loss: 0.4494\n",
      "Epoch [23/1000], Loss: 0.0864, Val Loss: 0.4500\n",
      "Epoch [24/1000], Loss: 0.5229, Val Loss: 0.4485\n",
      "Epoch [25/1000], Loss: 0.2677, Val Loss: 0.4517\n",
      "Epoch [26/1000], Loss: 0.0647, Val Loss: 0.4511\n",
      "Epoch [27/1000], Loss: 0.1003, Val Loss: 0.4490\n",
      "Epoch [28/1000], Loss: 0.0349, Val Loss: 0.4484\n",
      "Epoch [29/1000], Loss: 0.3545, Val Loss: 0.4485\n",
      "Epoch [30/1000], Loss: 0.6554, Val Loss: 0.4499\n",
      "Epoch [31/1000], Loss: 0.2779, Val Loss: 0.4513\n",
      "Epoch [32/1000], Loss: 0.2854, Val Loss: 0.4509\n",
      "Epoch [33/1000], Loss: 0.3379, Val Loss: 0.4492\n",
      "Epoch [34/1000], Loss: 0.4863, Val Loss: 0.4500\n",
      "Epoch [35/1000], Loss: 0.0476, Val Loss: 0.4516\n",
      "Epoch [36/1000], Loss: 0.3711, Val Loss: 0.4496\n",
      "Epoch [37/1000], Loss: 1.5974, Val Loss: 0.4485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:07:25,113] Trial 8 finished with value: 0.8188695789597108 and parameters: {'hidden_size': 113, 'dropout_prob': 0.6669191635480597, 'learning_rate': 0.002782273997795402, 'weight_decay': 8.748696508410506e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 38\n",
      "Epoch [1/1000], Loss: 0.7220, Val Loss: 0.4686\n",
      "Epoch [2/1000], Loss: 0.4731, Val Loss: 0.4560\n",
      "Epoch [3/1000], Loss: 2.3273, Val Loss: 0.4547\n",
      "Epoch [4/1000], Loss: 1.3786, Val Loss: 0.4523\n",
      "Epoch [5/1000], Loss: 0.0978, Val Loss: 0.4517\n",
      "Epoch [6/1000], Loss: 0.1136, Val Loss: 0.4496\n",
      "Epoch [7/1000], Loss: 0.0105, Val Loss: 0.4475\n",
      "Epoch [8/1000], Loss: 0.3214, Val Loss: 0.4474\n",
      "Epoch [9/1000], Loss: 0.2119, Val Loss: 0.4508\n",
      "Epoch [10/1000], Loss: 0.1004, Val Loss: 0.4483\n",
      "Epoch [11/1000], Loss: 0.4199, Val Loss: 0.4472\n",
      "Epoch [12/1000], Loss: 0.2202, Val Loss: 0.4470\n",
      "Epoch [13/1000], Loss: 0.0023, Val Loss: 0.4442\n",
      "Epoch [14/1000], Loss: 0.2947, Val Loss: 0.4451\n",
      "Epoch [15/1000], Loss: 0.0025, Val Loss: 0.4501\n",
      "Epoch [16/1000], Loss: 0.0757, Val Loss: 0.4458\n",
      "Epoch [17/1000], Loss: 0.0397, Val Loss: 0.4491\n",
      "Epoch [18/1000], Loss: 0.0130, Val Loss: 0.4455\n",
      "Epoch [19/1000], Loss: 0.0733, Val Loss: 0.4499\n",
      "Epoch [20/1000], Loss: 0.0053, Val Loss: 0.4481\n",
      "Epoch [21/1000], Loss: 1.1140, Val Loss: 0.4463\n",
      "Epoch [22/1000], Loss: 0.4434, Val Loss: 0.4478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:07:31,608] Trial 9 finished with value: 0.8194736868936617 and parameters: {'hidden_size': 86, 'dropout_prob': 0.20935671844558998, 'learning_rate': 0.0013822660808353213, 'weight_decay': 6.467808469079452e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 23\n",
      "Epoch [1/1000], Loss: 0.0229, Val Loss: 0.4589\n",
      "Epoch [2/1000], Loss: 0.5534, Val Loss: 0.4578\n",
      "Epoch [3/1000], Loss: 0.0223, Val Loss: 0.4543\n",
      "Epoch [4/1000], Loss: 0.4290, Val Loss: 0.4558\n",
      "Epoch [5/1000], Loss: 0.0124, Val Loss: 0.4545\n",
      "Epoch [6/1000], Loss: 0.3525, Val Loss: 0.4530\n",
      "Epoch [7/1000], Loss: 0.5933, Val Loss: 0.4554\n",
      "Epoch [8/1000], Loss: 0.0881, Val Loss: 0.4550\n",
      "Epoch [9/1000], Loss: 0.1292, Val Loss: 0.4509\n",
      "Epoch [10/1000], Loss: 0.9596, Val Loss: 0.4561\n",
      "Epoch [11/1000], Loss: 0.5032, Val Loss: 0.4586\n",
      "Epoch [12/1000], Loss: 0.0956, Val Loss: 0.4512\n",
      "Epoch [13/1000], Loss: 0.1287, Val Loss: 0.4558\n",
      "Epoch [14/1000], Loss: 0.3124, Val Loss: 0.4559\n",
      "Epoch [15/1000], Loss: 0.0105, Val Loss: 0.4564\n",
      "Epoch [16/1000], Loss: 0.2916, Val Loss: 0.4516\n",
      "Epoch [17/1000], Loss: 1.4081, Val Loss: 0.4540\n",
      "Epoch [18/1000], Loss: 0.3161, Val Loss: 0.4478\n",
      "Epoch [19/1000], Loss: 0.0533, Val Loss: 0.4582\n",
      "Epoch [20/1000], Loss: 1.5765, Val Loss: 0.4526\n",
      "Epoch [21/1000], Loss: 0.0587, Val Loss: 0.4578\n",
      "Epoch [22/1000], Loss: 0.4684, Val Loss: 0.4509\n",
      "Epoch [23/1000], Loss: 0.0001, Val Loss: 0.4509\n",
      "Epoch [24/1000], Loss: 1.9496, Val Loss: 0.4485\n",
      "Epoch [25/1000], Loss: 0.0628, Val Loss: 0.4485\n",
      "Epoch [26/1000], Loss: 0.2836, Val Loss: 0.4533\n",
      "Epoch [27/1000], Loss: 0.3582, Val Loss: 0.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:07:39,175] Trial 10 finished with value: 0.8136836819486433 and parameters: {'hidden_size': 55, 'dropout_prob': 0.571177143544363, 'learning_rate': 0.009915088839491423, 'weight_decay': 9.594124747316027e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 28\n",
      "Epoch [1/1000], Loss: 0.7441, Val Loss: 0.4604\n",
      "Epoch [2/1000], Loss: 0.6332, Val Loss: 0.4568\n",
      "Epoch [3/1000], Loss: 0.1418, Val Loss: 0.4542\n",
      "Epoch [4/1000], Loss: 0.0037, Val Loss: 0.4546\n",
      "Epoch [5/1000], Loss: 0.3559, Val Loss: 0.4622\n",
      "Epoch [6/1000], Loss: 0.1080, Val Loss: 0.4549\n",
      "Epoch [7/1000], Loss: 0.2403, Val Loss: 0.4532\n",
      "Epoch [8/1000], Loss: 0.7189, Val Loss: 0.4519\n",
      "Epoch [9/1000], Loss: 0.3532, Val Loss: 0.4516\n",
      "Epoch [10/1000], Loss: 0.3567, Val Loss: 0.4504\n",
      "Epoch [11/1000], Loss: 0.4805, Val Loss: 0.4517\n",
      "Epoch [12/1000], Loss: 0.0677, Val Loss: 0.4540\n",
      "Epoch [13/1000], Loss: 0.0194, Val Loss: 0.4529\n",
      "Epoch [14/1000], Loss: 0.1614, Val Loss: 0.4505\n",
      "Epoch [15/1000], Loss: 0.3329, Val Loss: 0.4537\n",
      "Epoch [16/1000], Loss: 1.5865, Val Loss: 0.4548\n",
      "Epoch [17/1000], Loss: 0.0101, Val Loss: 0.4499\n",
      "Epoch [18/1000], Loss: 0.5994, Val Loss: 0.4501\n",
      "Epoch [19/1000], Loss: 0.0943, Val Loss: 0.4517\n",
      "Epoch [20/1000], Loss: 3.1562, Val Loss: 0.4586\n",
      "Epoch [21/1000], Loss: 0.5528, Val Loss: 0.4548\n",
      "Epoch [22/1000], Loss: 0.7746, Val Loss: 0.4548\n",
      "Epoch [23/1000], Loss: 3.9276, Val Loss: 0.4532\n",
      "Epoch [24/1000], Loss: 0.4184, Val Loss: 0.4564\n",
      "Epoch [25/1000], Loss: 0.5974, Val Loss: 0.4515\n",
      "Epoch [26/1000], Loss: 0.3265, Val Loss: 0.4540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:07:46,829] Trial 11 finished with value: 0.8199771101719542 and parameters: {'hidden_size': 57, 'dropout_prob': 0.5731122085492689, 'learning_rate': 0.00509888168997392, 'weight_decay': 2.1422055854546444e-05}. Best is trial 2 with value: 0.8211764046146709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 27\n",
      "Epoch [1/1000], Loss: 0.1529, Val Loss: 0.4614\n",
      "Epoch [2/1000], Loss: 0.0075, Val Loss: 0.4556\n",
      "Epoch [3/1000], Loss: 0.4136, Val Loss: 0.4604\n",
      "Epoch [4/1000], Loss: 0.5475, Val Loss: 0.4617\n",
      "Epoch [5/1000], Loss: 0.0038, Val Loss: 0.4475\n",
      "Epoch [6/1000], Loss: 0.4090, Val Loss: 0.4616\n",
      "Epoch [7/1000], Loss: 0.4482, Val Loss: 0.4481\n",
      "Epoch [8/1000], Loss: 0.1428, Val Loss: 0.4496\n",
      "Epoch [9/1000], Loss: 1.1210, Val Loss: 0.4465\n",
      "Epoch [10/1000], Loss: 0.2012, Val Loss: 0.4541\n",
      "Epoch [11/1000], Loss: 0.1796, Val Loss: 0.4503\n",
      "Epoch [12/1000], Loss: 0.2920, Val Loss: 0.4482\n",
      "Epoch [13/1000], Loss: 0.6123, Val Loss: 0.4508\n",
      "Epoch [14/1000], Loss: 0.0084, Val Loss: 0.4511\n",
      "Epoch [15/1000], Loss: 0.1143, Val Loss: 0.4502\n",
      "Epoch [16/1000], Loss: 0.3520, Val Loss: 0.4622\n",
      "Epoch [17/1000], Loss: 0.4289, Val Loss: 0.4504\n",
      "Epoch [18/1000], Loss: 0.0004, Val Loss: 0.4477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:07:52,152] Trial 12 finished with value: 0.8213242453748782 and parameters: {'hidden_size': 77, 'dropout_prob': 0.540685735349942, 'learning_rate': 0.0037043330706918332, 'weight_decay': 7.57058007236341e-05}. Best is trial 12 with value: 0.8213242453748782.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 19\n",
      "Epoch [1/1000], Loss: 0.1130, Val Loss: 0.4630\n",
      "Epoch [2/1000], Loss: 0.4371, Val Loss: 0.4563\n",
      "Epoch [3/1000], Loss: 0.1121, Val Loss: 0.4538\n",
      "Epoch [4/1000], Loss: 0.5941, Val Loss: 0.4507\n",
      "Epoch [5/1000], Loss: 1.3119, Val Loss: 0.4610\n",
      "Epoch [6/1000], Loss: 1.8226, Val Loss: 0.4588\n",
      "Epoch [7/1000], Loss: 0.7764, Val Loss: 0.4501\n",
      "Epoch [8/1000], Loss: 0.0305, Val Loss: 0.4548\n",
      "Epoch [9/1000], Loss: 0.6808, Val Loss: 0.4491\n",
      "Epoch [10/1000], Loss: 3.2780, Val Loss: 0.4503\n",
      "Epoch [11/1000], Loss: 0.8534, Val Loss: 0.4455\n",
      "Epoch [12/1000], Loss: 1.8472, Val Loss: 0.4596\n",
      "Epoch [13/1000], Loss: 0.5416, Val Loss: 0.4480\n",
      "Epoch [14/1000], Loss: 0.0560, Val Loss: 0.4561\n",
      "Epoch [15/1000], Loss: 0.0050, Val Loss: 0.4511\n",
      "Epoch [16/1000], Loss: 0.3750, Val Loss: 0.4460\n",
      "Epoch [17/1000], Loss: 0.0234, Val Loss: 0.4518\n",
      "Epoch [18/1000], Loss: 0.6508, Val Loss: 0.4443\n",
      "Epoch [19/1000], Loss: 0.7960, Val Loss: 0.4462\n",
      "Epoch [20/1000], Loss: 1.1634, Val Loss: 0.4511\n",
      "Epoch [21/1000], Loss: 0.6373, Val Loss: 0.4642\n",
      "Epoch [22/1000], Loss: 0.3362, Val Loss: 0.4506\n",
      "Epoch [23/1000], Loss: 0.8395, Val Loss: 0.4534\n",
      "Epoch [24/1000], Loss: 0.0218, Val Loss: 0.4599\n",
      "Epoch [25/1000], Loss: 0.1100, Val Loss: 0.4517\n",
      "Epoch [26/1000], Loss: 0.2902, Val Loss: 0.4531\n",
      "Epoch [27/1000], Loss: 0.1246, Val Loss: 0.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:08:00,485] Trial 13 finished with value: 0.8200930886993582 and parameters: {'hidden_size': 73, 'dropout_prob': 0.5398300926860776, 'learning_rate': 0.003578224855652317, 'weight_decay': 7.981563739261573e-05}. Best is trial 12 with value: 0.8213242453748782.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 28\n",
      "Epoch [1/1000], Loss: 1.3231, Val Loss: 0.4657\n",
      "Epoch [2/1000], Loss: 0.7573, Val Loss: 0.4557\n",
      "Epoch [3/1000], Loss: 0.6958, Val Loss: 0.4517\n",
      "Epoch [4/1000], Loss: 0.7172, Val Loss: 0.4521\n",
      "Epoch [5/1000], Loss: 0.2067, Val Loss: 0.4500\n",
      "Epoch [6/1000], Loss: 0.5239, Val Loss: 0.4483\n",
      "Epoch [7/1000], Loss: 0.1560, Val Loss: 0.4474\n",
      "Epoch [8/1000], Loss: 0.0388, Val Loss: 0.4512\n",
      "Epoch [9/1000], Loss: 0.5766, Val Loss: 0.4479\n",
      "Epoch [10/1000], Loss: 0.0058, Val Loss: 0.4482\n",
      "Epoch [11/1000], Loss: 0.0592, Val Loss: 0.4473\n",
      "Epoch [12/1000], Loss: 0.0728, Val Loss: 0.4468\n",
      "Epoch [13/1000], Loss: 0.1706, Val Loss: 0.4482\n",
      "Epoch [14/1000], Loss: 1.5922, Val Loss: 0.4489\n",
      "Epoch [15/1000], Loss: 1.2032, Val Loss: 0.4608\n",
      "Epoch [16/1000], Loss: 0.6460, Val Loss: 0.4530\n",
      "Epoch [17/1000], Loss: 0.4456, Val Loss: 0.4552\n",
      "Epoch [18/1000], Loss: 0.2082, Val Loss: 0.4515\n",
      "Epoch [19/1000], Loss: 0.1412, Val Loss: 0.4495\n",
      "Epoch [20/1000], Loss: 0.7593, Val Loss: 0.4487\n",
      "Epoch [21/1000], Loss: 0.0261, Val Loss: 0.4466\n",
      "Epoch [22/1000], Loss: 1.4492, Val Loss: 0.4481\n",
      "Epoch [23/1000], Loss: 2.3795, Val Loss: 0.4469\n",
      "Epoch [24/1000], Loss: 0.2379, Val Loss: 0.4478\n",
      "Epoch [25/1000], Loss: 0.4809, Val Loss: 0.4456\n",
      "Epoch [26/1000], Loss: 3.0171, Val Loss: 0.4518\n",
      "Epoch [27/1000], Loss: 0.1640, Val Loss: 0.4454\n",
      "Epoch [28/1000], Loss: 0.0039, Val Loss: 0.4462\n",
      "Epoch [29/1000], Loss: 1.3440, Val Loss: 0.4467\n",
      "Epoch [30/1000], Loss: 0.0289, Val Loss: 0.4487\n",
      "Epoch [31/1000], Loss: 0.1376, Val Loss: 0.4494\n",
      "Epoch [32/1000], Loss: 0.5687, Val Loss: 0.4457\n",
      "Epoch [33/1000], Loss: 0.2421, Val Loss: 0.4511\n",
      "Epoch [34/1000], Loss: 1.8060, Val Loss: 0.4448\n",
      "Epoch [35/1000], Loss: 0.0217, Val Loss: 0.4453\n",
      "Epoch [36/1000], Loss: 1.3528, Val Loss: 0.4472\n",
      "Epoch [37/1000], Loss: 0.0086, Val Loss: 0.4477\n",
      "Epoch [38/1000], Loss: 0.0011, Val Loss: 0.4493\n",
      "Epoch [39/1000], Loss: 0.2262, Val Loss: 0.4433\n",
      "Epoch [40/1000], Loss: 1.9314, Val Loss: 0.4451\n",
      "Epoch [41/1000], Loss: 0.0087, Val Loss: 0.4469\n",
      "Epoch [42/1000], Loss: 0.2237, Val Loss: 0.4469\n",
      "Epoch [43/1000], Loss: 0.3374, Val Loss: 0.4503\n",
      "Epoch [44/1000], Loss: 0.4999, Val Loss: 0.4526\n",
      "Epoch [45/1000], Loss: 0.4095, Val Loss: 0.4504\n",
      "Epoch [46/1000], Loss: 0.0454, Val Loss: 0.4512\n",
      "Epoch [47/1000], Loss: 0.3490, Val Loss: 0.4477\n",
      "Epoch [48/1000], Loss: 0.0676, Val Loss: 0.4501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:08:15,467] Trial 14 finished with value: 0.8187395810498734 and parameters: {'hidden_size': 49, 'dropout_prob': 0.28836772886972506, 'learning_rate': 0.003359624413773498, 'weight_decay': 7.484520254196658e-05}. Best is trial 12 with value: 0.8213242453748782.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 49\n",
      "Epoch [1/1000], Loss: 0.7982, Val Loss: 0.4650\n",
      "Epoch [2/1000], Loss: 1.1493, Val Loss: 0.4537\n",
      "Epoch [3/1000], Loss: 0.5079, Val Loss: 0.4507\n",
      "Epoch [4/1000], Loss: 0.0086, Val Loss: 0.4510\n",
      "Epoch [5/1000], Loss: 0.5905, Val Loss: 0.4566\n",
      "Epoch [6/1000], Loss: 0.1689, Val Loss: 0.4470\n",
      "Epoch [7/1000], Loss: 0.0642, Val Loss: 0.4479\n",
      "Epoch [8/1000], Loss: 0.2455, Val Loss: 0.4456\n",
      "Epoch [9/1000], Loss: 2.0772, Val Loss: 0.4502\n",
      "Epoch [10/1000], Loss: 0.5671, Val Loss: 0.4482\n",
      "Epoch [11/1000], Loss: 0.0037, Val Loss: 0.4481\n",
      "Epoch [12/1000], Loss: 1.0285, Val Loss: 0.4455\n",
      "Epoch [13/1000], Loss: 0.0486, Val Loss: 0.4447\n",
      "Epoch [14/1000], Loss: 0.0516, Val Loss: 0.4432\n",
      "Epoch [15/1000], Loss: 0.8799, Val Loss: 0.4468\n",
      "Epoch [16/1000], Loss: 0.1547, Val Loss: 0.4451\n",
      "Epoch [17/1000], Loss: 0.0087, Val Loss: 0.4474\n",
      "Epoch [18/1000], Loss: 0.0474, Val Loss: 0.4442\n",
      "Epoch [19/1000], Loss: 0.4338, Val Loss: 0.4457\n",
      "Epoch [20/1000], Loss: 0.3475, Val Loss: 0.4464\n",
      "Epoch [21/1000], Loss: 0.2589, Val Loss: 0.4453\n",
      "Epoch [22/1000], Loss: 0.9278, Val Loss: 0.4458\n",
      "Epoch [23/1000], Loss: 2.0629, Val Loss: 0.4530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:08:23,053] Trial 15 finished with value: 0.8194558440432919 and parameters: {'hidden_size': 90, 'dropout_prob': 0.47975301035982376, 'learning_rate': 0.001995785943874576, 'weight_decay': 4.8758636119163024e-05}. Best is trial 12 with value: 0.8213242453748782.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 24\n",
      "Epoch [1/1000], Loss: 0.0158, Val Loss: 0.4665\n",
      "Epoch [2/1000], Loss: 0.9316, Val Loss: 0.4627\n",
      "Epoch [3/1000], Loss: 0.2387, Val Loss: 0.4520\n",
      "Epoch [4/1000], Loss: 0.4035, Val Loss: 0.4514\n",
      "Epoch [5/1000], Loss: 1.0281, Val Loss: 0.4502\n",
      "Epoch [6/1000], Loss: 1.0604, Val Loss: 0.4485\n",
      "Epoch [7/1000], Loss: 1.0624, Val Loss: 0.4600\n",
      "Epoch [8/1000], Loss: 0.0724, Val Loss: 0.4557\n",
      "Epoch [9/1000], Loss: 0.0019, Val Loss: 0.4493\n",
      "Epoch [10/1000], Loss: 0.1439, Val Loss: 0.4483\n",
      "Epoch [11/1000], Loss: 0.0376, Val Loss: 0.4491\n",
      "Epoch [12/1000], Loss: 0.1456, Val Loss: 0.4487\n",
      "Epoch [13/1000], Loss: 0.0322, Val Loss: 0.4529\n",
      "Epoch [14/1000], Loss: 0.0111, Val Loss: 0.4559\n",
      "Epoch [15/1000], Loss: 0.2955, Val Loss: 0.4514\n",
      "Epoch [16/1000], Loss: 0.2985, Val Loss: 0.4487\n",
      "Epoch [17/1000], Loss: 1.1127, Val Loss: 0.4491\n",
      "Epoch [18/1000], Loss: 0.7778, Val Loss: 0.4512\n",
      "Epoch [19/1000], Loss: 0.3273, Val Loss: 0.4524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:08:28,463] Trial 16 finished with value: 0.8211381699353069 and parameters: {'hidden_size': 69, 'dropout_prob': 0.3480600559871301, 'learning_rate': 0.004089286914786433, 'weight_decay': 7.769675484844895e-05}. Best is trial 12 with value: 0.8213242453748782.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 20\n",
      "Epoch [1/1000], Loss: 0.7127, Val Loss: 0.6554\n",
      "Epoch [2/1000], Loss: 0.5223, Val Loss: 0.6493\n",
      "Epoch [3/1000], Loss: 0.6160, Val Loss: 0.6436\n",
      "Epoch [4/1000], Loss: 0.5917, Val Loss: 0.6380\n",
      "Epoch [5/1000], Loss: 0.8463, Val Loss: 0.6327\n",
      "Epoch [6/1000], Loss: 0.5594, Val Loss: 0.6276\n",
      "Epoch [7/1000], Loss: 0.5051, Val Loss: 0.6225\n",
      "Epoch [8/1000], Loss: 0.5138, Val Loss: 0.6172\n",
      "Epoch [9/1000], Loss: 0.5213, Val Loss: 0.6119\n",
      "Epoch [10/1000], Loss: 0.4862, Val Loss: 0.6065\n",
      "Epoch [11/1000], Loss: 0.4979, Val Loss: 0.6008\n",
      "Epoch [12/1000], Loss: 0.4328, Val Loss: 0.5950\n",
      "Epoch [13/1000], Loss: 0.4883, Val Loss: 0.5893\n",
      "Epoch [14/1000], Loss: 0.2676, Val Loss: 0.5833\n",
      "Epoch [15/1000], Loss: 0.5186, Val Loss: 0.5773\n",
      "Epoch [16/1000], Loss: 0.4484, Val Loss: 0.5713\n",
      "Epoch [17/1000], Loss: 0.4125, Val Loss: 0.5657\n",
      "Epoch [18/1000], Loss: 0.2879, Val Loss: 0.5605\n",
      "Epoch [19/1000], Loss: 0.3263, Val Loss: 0.5558\n",
      "Epoch [20/1000], Loss: 0.3409, Val Loss: 0.5513\n",
      "Epoch [21/1000], Loss: 0.3507, Val Loss: 0.5476\n",
      "Epoch [22/1000], Loss: 1.0415, Val Loss: 0.5441\n",
      "Epoch [23/1000], Loss: 1.2651, Val Loss: 0.5410\n",
      "Epoch [24/1000], Loss: 0.4491, Val Loss: 0.5384\n",
      "Epoch [25/1000], Loss: 0.8937, Val Loss: 0.5357\n",
      "Epoch [26/1000], Loss: 0.3228, Val Loss: 0.5333\n",
      "Epoch [27/1000], Loss: 0.4196, Val Loss: 0.5312\n",
      "Epoch [28/1000], Loss: 1.3206, Val Loss: 0.5291\n",
      "Epoch [29/1000], Loss: 0.2065, Val Loss: 0.5272\n",
      "Epoch [30/1000], Loss: 0.3588, Val Loss: 0.5252\n",
      "Epoch [31/1000], Loss: 0.2406, Val Loss: 0.5235\n",
      "Epoch [32/1000], Loss: 0.9862, Val Loss: 0.5219\n",
      "Epoch [33/1000], Loss: 0.2369, Val Loss: 0.5203\n",
      "Epoch [34/1000], Loss: 1.4348, Val Loss: 0.5188\n",
      "Epoch [35/1000], Loss: 1.2908, Val Loss: 0.5175\n",
      "Epoch [36/1000], Loss: 0.9690, Val Loss: 0.5162\n",
      "Epoch [37/1000], Loss: 1.3978, Val Loss: 0.5148\n",
      "Epoch [38/1000], Loss: 0.0801, Val Loss: 0.5135\n",
      "Epoch [39/1000], Loss: 0.1421, Val Loss: 0.5123\n",
      "Epoch [40/1000], Loss: 0.0837, Val Loss: 0.5111\n",
      "Epoch [41/1000], Loss: 0.2524, Val Loss: 0.5097\n",
      "Epoch [42/1000], Loss: 1.1903, Val Loss: 0.5085\n",
      "Epoch [43/1000], Loss: 0.2667, Val Loss: 0.5073\n",
      "Epoch [44/1000], Loss: 0.2708, Val Loss: 0.5062\n",
      "Epoch [45/1000], Loss: 0.8760, Val Loss: 0.5050\n",
      "Epoch [46/1000], Loss: 0.0835, Val Loss: 0.5038\n",
      "Epoch [47/1000], Loss: 0.4536, Val Loss: 0.5028\n",
      "Epoch [48/1000], Loss: 0.2790, Val Loss: 0.5017\n",
      "Epoch [49/1000], Loss: 0.4053, Val Loss: 0.5007\n",
      "Epoch [50/1000], Loss: 0.9659, Val Loss: 0.4997\n",
      "Epoch [51/1000], Loss: 0.0402, Val Loss: 0.4987\n",
      "Epoch [52/1000], Loss: 0.4162, Val Loss: 0.4977\n",
      "Epoch [53/1000], Loss: 0.0877, Val Loss: 0.4968\n",
      "Epoch [54/1000], Loss: 0.9582, Val Loss: 0.4959\n",
      "Epoch [55/1000], Loss: 1.7596, Val Loss: 0.4949\n",
      "Epoch [56/1000], Loss: 0.8821, Val Loss: 0.4942\n",
      "Epoch [57/1000], Loss: 1.1208, Val Loss: 0.4933\n",
      "Epoch [58/1000], Loss: 0.5623, Val Loss: 0.4925\n",
      "Epoch [59/1000], Loss: 0.3279, Val Loss: 0.4917\n",
      "Epoch [60/1000], Loss: 0.0127, Val Loss: 0.4910\n",
      "Epoch [61/1000], Loss: 0.2439, Val Loss: 0.4902\n",
      "Epoch [62/1000], Loss: 0.3457, Val Loss: 0.4895\n",
      "Epoch [63/1000], Loss: 0.2892, Val Loss: 0.4888\n",
      "Epoch [64/1000], Loss: 0.2303, Val Loss: 0.4882\n",
      "Epoch [65/1000], Loss: 0.4428, Val Loss: 0.4875\n",
      "Epoch [66/1000], Loss: 0.4218, Val Loss: 0.4869\n",
      "Epoch [67/1000], Loss: 0.4301, Val Loss: 0.4862\n",
      "Epoch [68/1000], Loss: 0.2439, Val Loss: 0.4856\n",
      "Epoch [69/1000], Loss: 0.1429, Val Loss: 0.4850\n",
      "Epoch [70/1000], Loss: 0.6072, Val Loss: 0.4845\n",
      "Epoch [71/1000], Loss: 0.0314, Val Loss: 0.4840\n",
      "Epoch [72/1000], Loss: 0.8842, Val Loss: 0.4835\n",
      "Epoch [73/1000], Loss: 0.1160, Val Loss: 0.4830\n",
      "Epoch [74/1000], Loss: 0.3024, Val Loss: 0.4825\n",
      "Epoch [75/1000], Loss: 0.3131, Val Loss: 0.4820\n",
      "Epoch [76/1000], Loss: 0.5431, Val Loss: 0.4815\n",
      "Epoch [77/1000], Loss: 2.0690, Val Loss: 0.4810\n",
      "Epoch [78/1000], Loss: 0.7881, Val Loss: 0.4806\n",
      "Epoch [79/1000], Loss: 0.0617, Val Loss: 0.4802\n",
      "Epoch [80/1000], Loss: 0.2072, Val Loss: 0.4798\n",
      "Epoch [81/1000], Loss: 0.1716, Val Loss: 0.4794\n",
      "Epoch [82/1000], Loss: 1.0653, Val Loss: 0.4790\n",
      "Epoch [83/1000], Loss: 1.3965, Val Loss: 0.4786\n",
      "Epoch [84/1000], Loss: 0.1562, Val Loss: 0.4782\n",
      "Epoch [85/1000], Loss: 0.5698, Val Loss: 0.4779\n",
      "Epoch [86/1000], Loss: 0.3694, Val Loss: 0.4776\n",
      "Epoch [87/1000], Loss: 0.3063, Val Loss: 0.4773\n",
      "Epoch [88/1000], Loss: 0.2583, Val Loss: 0.4770\n",
      "Epoch [89/1000], Loss: 0.5736, Val Loss: 0.4766\n",
      "Epoch [90/1000], Loss: 0.8951, Val Loss: 0.4763\n",
      "Epoch [91/1000], Loss: 0.8193, Val Loss: 0.4760\n",
      "Epoch [92/1000], Loss: 0.7953, Val Loss: 0.4758\n",
      "Epoch [93/1000], Loss: 0.5373, Val Loss: 0.4755\n",
      "Epoch [94/1000], Loss: 0.2410, Val Loss: 0.4753\n",
      "Epoch [95/1000], Loss: 0.2125, Val Loss: 0.4750\n",
      "Epoch [96/1000], Loss: 1.1651, Val Loss: 0.4748\n",
      "Epoch [97/1000], Loss: 0.9254, Val Loss: 0.4745\n",
      "Epoch [98/1000], Loss: 0.0015, Val Loss: 0.4743\n",
      "Epoch [99/1000], Loss: 0.1210, Val Loss: 0.4740\n",
      "Epoch [100/1000], Loss: 0.2483, Val Loss: 0.4738\n",
      "Epoch [101/1000], Loss: 0.6267, Val Loss: 0.4736\n",
      "Epoch [102/1000], Loss: 0.5053, Val Loss: 0.4733\n",
      "Epoch [103/1000], Loss: 0.6752, Val Loss: 0.4732\n",
      "Epoch [104/1000], Loss: 0.0442, Val Loss: 0.4730\n",
      "Epoch [105/1000], Loss: 0.8224, Val Loss: 0.4728\n",
      "Epoch [106/1000], Loss: 0.0007, Val Loss: 0.4725\n",
      "Epoch [107/1000], Loss: 0.7871, Val Loss: 0.4723\n",
      "Epoch [108/1000], Loss: 0.6362, Val Loss: 0.4721\n",
      "Epoch [109/1000], Loss: 0.1143, Val Loss: 0.4720\n",
      "Epoch [110/1000], Loss: 0.1113, Val Loss: 0.4718\n",
      "Epoch [111/1000], Loss: 0.0063, Val Loss: 0.4716\n",
      "Epoch [112/1000], Loss: 0.7704, Val Loss: 0.4714\n",
      "Epoch [113/1000], Loss: 0.6840, Val Loss: 0.4712\n",
      "Epoch [114/1000], Loss: 1.1900, Val Loss: 0.4711\n",
      "Epoch [115/1000], Loss: 0.0852, Val Loss: 0.4709\n",
      "Epoch [116/1000], Loss: 0.1682, Val Loss: 0.4707\n",
      "Epoch [117/1000], Loss: 0.3086, Val Loss: 0.4705\n",
      "Epoch [118/1000], Loss: 2.6197, Val Loss: 0.4703\n",
      "Epoch [119/1000], Loss: 0.6140, Val Loss: 0.4702\n",
      "Epoch [120/1000], Loss: 0.2849, Val Loss: 0.4700\n",
      "Epoch [121/1000], Loss: 2.2912, Val Loss: 0.4698\n",
      "Epoch [122/1000], Loss: 0.1220, Val Loss: 0.4697\n",
      "Epoch [123/1000], Loss: 0.0971, Val Loss: 0.4695\n",
      "Epoch [124/1000], Loss: 0.5657, Val Loss: 0.4694\n",
      "Epoch [125/1000], Loss: 0.0245, Val Loss: 0.4692\n",
      "Epoch [126/1000], Loss: 0.0268, Val Loss: 0.4691\n",
      "Epoch [127/1000], Loss: 0.1130, Val Loss: 0.4690\n",
      "Epoch [128/1000], Loss: 0.0786, Val Loss: 0.4688\n",
      "Epoch [129/1000], Loss: 0.0230, Val Loss: 0.4686\n",
      "Epoch [130/1000], Loss: 0.7510, Val Loss: 0.4685\n",
      "Epoch [131/1000], Loss: 0.5564, Val Loss: 0.4683\n",
      "Epoch [132/1000], Loss: 0.7073, Val Loss: 0.4682\n",
      "Epoch [133/1000], Loss: 0.7272, Val Loss: 0.4680\n",
      "Epoch [134/1000], Loss: 0.0530, Val Loss: 0.4679\n",
      "Epoch [135/1000], Loss: 0.7034, Val Loss: 0.4678\n",
      "Epoch [136/1000], Loss: 0.0413, Val Loss: 0.4677\n",
      "Epoch [137/1000], Loss: 0.7090, Val Loss: 0.4675\n",
      "Epoch [138/1000], Loss: 0.6378, Val Loss: 0.4674\n",
      "Epoch [139/1000], Loss: 0.1576, Val Loss: 0.4673\n",
      "Epoch [140/1000], Loss: 0.0057, Val Loss: 0.4671\n",
      "Epoch [141/1000], Loss: 0.4800, Val Loss: 0.4670\n",
      "Epoch [142/1000], Loss: 0.0675, Val Loss: 0.4669\n",
      "Epoch [143/1000], Loss: 0.3721, Val Loss: 0.4667\n",
      "Epoch [144/1000], Loss: 0.0241, Val Loss: 0.4666\n",
      "Epoch [145/1000], Loss: 0.1022, Val Loss: 0.4665\n",
      "Epoch [146/1000], Loss: 1.1876, Val Loss: 0.4664\n",
      "Epoch [147/1000], Loss: 0.3450, Val Loss: 0.4662\n",
      "Epoch [148/1000], Loss: 0.4917, Val Loss: 0.4661\n",
      "Epoch [149/1000], Loss: 0.4649, Val Loss: 0.4659\n",
      "Epoch [150/1000], Loss: 0.5171, Val Loss: 0.4658\n",
      "Epoch [151/1000], Loss: 0.2142, Val Loss: 0.4656\n",
      "Epoch [152/1000], Loss: 0.1285, Val Loss: 0.4655\n",
      "Epoch [153/1000], Loss: 0.1728, Val Loss: 0.4653\n",
      "Epoch [154/1000], Loss: 0.4119, Val Loss: 0.4652\n",
      "Epoch [155/1000], Loss: 0.0497, Val Loss: 0.4651\n",
      "Epoch [156/1000], Loss: 0.2548, Val Loss: 0.4650\n",
      "Epoch [157/1000], Loss: 0.7715, Val Loss: 0.4648\n",
      "Epoch [158/1000], Loss: 1.4157, Val Loss: 0.4647\n",
      "Epoch [159/1000], Loss: 0.0036, Val Loss: 0.4645\n",
      "Epoch [160/1000], Loss: 0.0984, Val Loss: 0.4644\n",
      "Epoch [161/1000], Loss: 0.5262, Val Loss: 0.4643\n",
      "Epoch [162/1000], Loss: 0.4697, Val Loss: 0.4641\n",
      "Epoch [163/1000], Loss: 1.0034, Val Loss: 0.4640\n",
      "Epoch [164/1000], Loss: 0.8786, Val Loss: 0.4639\n",
      "Epoch [165/1000], Loss: 1.6305, Val Loss: 0.4638\n",
      "Epoch [166/1000], Loss: 0.1201, Val Loss: 0.4636\n",
      "Epoch [167/1000], Loss: 0.9133, Val Loss: 0.4635\n",
      "Epoch [168/1000], Loss: 0.4202, Val Loss: 0.4634\n",
      "Epoch [169/1000], Loss: 0.2030, Val Loss: 0.4633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 0.3459, Val Loss: 0.4632\n",
      "Epoch [171/1000], Loss: 0.2065, Val Loss: 0.4632\n",
      "Epoch [172/1000], Loss: 0.3599, Val Loss: 0.4630\n",
      "Epoch [173/1000], Loss: 0.5837, Val Loss: 0.4629\n",
      "Epoch [174/1000], Loss: 0.0193, Val Loss: 0.4628\n",
      "Epoch [175/1000], Loss: 0.2699, Val Loss: 0.4627\n",
      "Epoch [176/1000], Loss: 0.0334, Val Loss: 0.4626\n",
      "Epoch [177/1000], Loss: 0.0950, Val Loss: 0.4625\n",
      "Epoch [178/1000], Loss: 0.1136, Val Loss: 0.4623\n",
      "Epoch [179/1000], Loss: 0.9344, Val Loss: 0.4623\n",
      "Epoch [180/1000], Loss: 0.7512, Val Loss: 0.4622\n",
      "Epoch [181/1000], Loss: 0.5001, Val Loss: 0.4620\n",
      "Epoch [182/1000], Loss: 0.8573, Val Loss: 0.4619\n",
      "Epoch [183/1000], Loss: 0.5764, Val Loss: 0.4618\n",
      "Epoch [184/1000], Loss: 0.0073, Val Loss: 0.4617\n",
      "Epoch [185/1000], Loss: 0.0026, Val Loss: 0.4616\n",
      "Epoch [186/1000], Loss: 0.7545, Val Loss: 0.4615\n",
      "Epoch [187/1000], Loss: 0.1605, Val Loss: 0.4614\n",
      "Epoch [188/1000], Loss: 0.1815, Val Loss: 0.4613\n",
      "Epoch [189/1000], Loss: 2.3790, Val Loss: 0.4612\n",
      "Epoch [190/1000], Loss: 1.5108, Val Loss: 0.4611\n",
      "Epoch [191/1000], Loss: 0.3057, Val Loss: 0.4610\n",
      "Epoch [192/1000], Loss: 0.0119, Val Loss: 0.4609\n",
      "Epoch [193/1000], Loss: 0.6824, Val Loss: 0.4608\n",
      "Epoch [194/1000], Loss: 0.1557, Val Loss: 0.4607\n",
      "Epoch [195/1000], Loss: 0.0520, Val Loss: 0.4606\n",
      "Epoch [196/1000], Loss: 0.0059, Val Loss: 0.4606\n",
      "Epoch [197/1000], Loss: 0.0218, Val Loss: 0.4605\n",
      "Epoch [198/1000], Loss: 0.3313, Val Loss: 0.4604\n",
      "Epoch [199/1000], Loss: 0.0008, Val Loss: 0.4603\n",
      "Epoch [200/1000], Loss: 0.1492, Val Loss: 0.4602\n",
      "Epoch [201/1000], Loss: 0.7542, Val Loss: 0.4601\n",
      "Epoch [202/1000], Loss: 0.2437, Val Loss: 0.4600\n",
      "Epoch [203/1000], Loss: 0.0014, Val Loss: 0.4600\n",
      "Epoch [204/1000], Loss: 0.3118, Val Loss: 0.4599\n",
      "Epoch [205/1000], Loss: 1.0323, Val Loss: 0.4599\n",
      "Epoch [206/1000], Loss: 2.1822, Val Loss: 0.4598\n",
      "Epoch [207/1000], Loss: 0.4244, Val Loss: 0.4596\n",
      "Epoch [208/1000], Loss: 0.3823, Val Loss: 0.4596\n",
      "Epoch [209/1000], Loss: 0.7144, Val Loss: 0.4595\n",
      "Epoch [210/1000], Loss: 0.4460, Val Loss: 0.4594\n",
      "Epoch [211/1000], Loss: 0.6136, Val Loss: 0.4593\n",
      "Epoch [212/1000], Loss: 0.0062, Val Loss: 0.4593\n",
      "Epoch [213/1000], Loss: 0.5084, Val Loss: 0.4592\n",
      "Epoch [214/1000], Loss: 0.6173, Val Loss: 0.4591\n",
      "Epoch [215/1000], Loss: 0.2184, Val Loss: 0.4590\n",
      "Epoch [216/1000], Loss: 0.0160, Val Loss: 0.4589\n",
      "Epoch [217/1000], Loss: 0.4248, Val Loss: 0.4589\n",
      "Epoch [218/1000], Loss: 0.0708, Val Loss: 0.4587\n",
      "Epoch [219/1000], Loss: 1.0348, Val Loss: 0.4587\n",
      "Epoch [220/1000], Loss: 0.1707, Val Loss: 0.4586\n",
      "Epoch [221/1000], Loss: 0.0294, Val Loss: 0.4585\n",
      "Epoch [222/1000], Loss: 0.2161, Val Loss: 0.4585\n",
      "Epoch [223/1000], Loss: 0.0820, Val Loss: 0.4584\n",
      "Epoch [224/1000], Loss: 0.1148, Val Loss: 0.4583\n",
      "Epoch [225/1000], Loss: 0.0855, Val Loss: 0.4583\n",
      "Epoch [226/1000], Loss: 1.5333, Val Loss: 0.4582\n",
      "Epoch [227/1000], Loss: 0.0034, Val Loss: 0.4582\n",
      "Epoch [228/1000], Loss: 0.8294, Val Loss: 0.4581\n",
      "Epoch [229/1000], Loss: 0.4743, Val Loss: 0.4581\n",
      "Epoch [230/1000], Loss: 0.9980, Val Loss: 0.4580\n",
      "Epoch [231/1000], Loss: 0.3588, Val Loss: 0.4580\n",
      "Epoch [232/1000], Loss: 2.4471, Val Loss: 0.4579\n",
      "Epoch [233/1000], Loss: 0.0069, Val Loss: 0.4578\n",
      "Epoch [234/1000], Loss: 0.0443, Val Loss: 0.4578\n",
      "Epoch [235/1000], Loss: 0.9174, Val Loss: 0.4578\n",
      "Epoch [236/1000], Loss: 0.7530, Val Loss: 0.4577\n",
      "Epoch [237/1000], Loss: 0.0040, Val Loss: 0.4576\n",
      "Epoch [238/1000], Loss: 0.0199, Val Loss: 0.4576\n",
      "Epoch [239/1000], Loss: 0.0813, Val Loss: 0.4574\n",
      "Epoch [240/1000], Loss: 0.0130, Val Loss: 0.4574\n",
      "Epoch [241/1000], Loss: 0.2366, Val Loss: 0.4573\n",
      "Epoch [242/1000], Loss: 1.6327, Val Loss: 0.4573\n",
      "Epoch [243/1000], Loss: 0.2177, Val Loss: 0.4572\n",
      "Epoch [244/1000], Loss: 0.5456, Val Loss: 0.4572\n",
      "Epoch [245/1000], Loss: 0.6748, Val Loss: 0.4571\n",
      "Epoch [246/1000], Loss: 0.7805, Val Loss: 0.4571\n",
      "Epoch [247/1000], Loss: 0.0457, Val Loss: 0.4570\n",
      "Epoch [248/1000], Loss: 0.3390, Val Loss: 0.4570\n",
      "Epoch [249/1000], Loss: 1.0342, Val Loss: 0.4569\n",
      "Epoch [250/1000], Loss: 0.0812, Val Loss: 0.4569\n",
      "Epoch [251/1000], Loss: 0.0457, Val Loss: 0.4568\n",
      "Epoch [252/1000], Loss: 0.5275, Val Loss: 0.4568\n",
      "Epoch [253/1000], Loss: 0.1476, Val Loss: 0.4567\n",
      "Epoch [254/1000], Loss: 0.5242, Val Loss: 0.4566\n",
      "Epoch [255/1000], Loss: 0.2126, Val Loss: 0.4566\n",
      "Epoch [256/1000], Loss: 0.8274, Val Loss: 0.4565\n",
      "Epoch [257/1000], Loss: 0.2149, Val Loss: 0.4564\n",
      "Epoch [258/1000], Loss: 0.0211, Val Loss: 0.4564\n",
      "Epoch [259/1000], Loss: 0.8919, Val Loss: 0.4563\n",
      "Epoch [260/1000], Loss: 0.0285, Val Loss: 0.4563\n",
      "Epoch [261/1000], Loss: 0.0232, Val Loss: 0.4563\n",
      "Epoch [262/1000], Loss: 0.0346, Val Loss: 0.4562\n",
      "Epoch [263/1000], Loss: 1.2771, Val Loss: 0.4561\n",
      "Epoch [264/1000], Loss: 1.0904, Val Loss: 0.4560\n",
      "Epoch [265/1000], Loss: 0.0996, Val Loss: 0.4560\n",
      "Epoch [266/1000], Loss: 0.0793, Val Loss: 0.4559\n",
      "Epoch [267/1000], Loss: 1.7778, Val Loss: 0.4559\n",
      "Epoch [268/1000], Loss: 0.6954, Val Loss: 0.4558\n",
      "Epoch [269/1000], Loss: 0.1936, Val Loss: 0.4558\n",
      "Epoch [270/1000], Loss: 0.0272, Val Loss: 0.4557\n",
      "Epoch [271/1000], Loss: 0.7095, Val Loss: 0.4557\n",
      "Epoch [272/1000], Loss: 0.0738, Val Loss: 0.4556\n",
      "Epoch [273/1000], Loss: 0.0144, Val Loss: 0.4556\n",
      "Epoch [274/1000], Loss: 0.1524, Val Loss: 0.4555\n",
      "Epoch [275/1000], Loss: 0.6471, Val Loss: 0.4554\n",
      "Epoch [276/1000], Loss: 0.3985, Val Loss: 0.4554\n",
      "Epoch [277/1000], Loss: 0.6590, Val Loss: 0.4553\n",
      "Epoch [278/1000], Loss: 0.6153, Val Loss: 0.4553\n",
      "Epoch [279/1000], Loss: 0.0928, Val Loss: 0.4553\n",
      "Epoch [280/1000], Loss: 0.1887, Val Loss: 0.4552\n",
      "Epoch [281/1000], Loss: 0.0959, Val Loss: 0.4552\n",
      "Epoch [282/1000], Loss: 0.0182, Val Loss: 0.4551\n",
      "Epoch [283/1000], Loss: 0.5874, Val Loss: 0.4551\n",
      "Epoch [284/1000], Loss: 0.0273, Val Loss: 0.4550\n",
      "Epoch [285/1000], Loss: 0.0076, Val Loss: 0.4550\n",
      "Epoch [286/1000], Loss: 0.8761, Val Loss: 0.4549\n",
      "Epoch [287/1000], Loss: 0.4096, Val Loss: 0.4549\n",
      "Epoch [288/1000], Loss: 0.0005, Val Loss: 0.4548\n",
      "Epoch [289/1000], Loss: 0.9742, Val Loss: 0.4548\n",
      "Epoch [290/1000], Loss: 0.6188, Val Loss: 0.4547\n",
      "Epoch [291/1000], Loss: 0.0130, Val Loss: 0.4546\n",
      "Epoch [292/1000], Loss: 0.0474, Val Loss: 0.4546\n",
      "Epoch [293/1000], Loss: 1.8469, Val Loss: 0.4546\n",
      "Epoch [294/1000], Loss: 1.3685, Val Loss: 0.4545\n",
      "Epoch [295/1000], Loss: 0.0268, Val Loss: 0.4545\n",
      "Epoch [296/1000], Loss: 0.0650, Val Loss: 0.4545\n",
      "Epoch [297/1000], Loss: 0.6180, Val Loss: 0.4544\n",
      "Epoch [298/1000], Loss: 0.1597, Val Loss: 0.4543\n",
      "Epoch [299/1000], Loss: 0.2175, Val Loss: 0.4543\n",
      "Epoch [300/1000], Loss: 0.0473, Val Loss: 0.4542\n",
      "Epoch [301/1000], Loss: 0.0407, Val Loss: 0.4542\n",
      "Epoch [302/1000], Loss: 0.0203, Val Loss: 0.4541\n",
      "Epoch [303/1000], Loss: 0.8577, Val Loss: 0.4541\n",
      "Epoch [304/1000], Loss: 0.7670, Val Loss: 0.4541\n",
      "Epoch [305/1000], Loss: 0.8138, Val Loss: 0.4540\n",
      "Epoch [306/1000], Loss: 0.0744, Val Loss: 0.4540\n",
      "Epoch [307/1000], Loss: 0.4626, Val Loss: 0.4540\n",
      "Epoch [308/1000], Loss: 0.0172, Val Loss: 0.4540\n",
      "Epoch [309/1000], Loss: 0.0007, Val Loss: 0.4539\n",
      "Epoch [310/1000], Loss: 0.1845, Val Loss: 0.4538\n",
      "Epoch [311/1000], Loss: 0.0231, Val Loss: 0.4538\n",
      "Epoch [312/1000], Loss: 0.6850, Val Loss: 0.4538\n",
      "Epoch [313/1000], Loss: 0.0843, Val Loss: 0.4537\n",
      "Epoch [314/1000], Loss: 0.0351, Val Loss: 0.4537\n",
      "Epoch [315/1000], Loss: 0.0498, Val Loss: 0.4536\n",
      "Epoch [316/1000], Loss: 0.6996, Val Loss: 0.4536\n",
      "Epoch [317/1000], Loss: 0.1033, Val Loss: 0.4536\n",
      "Epoch [318/1000], Loss: 0.2049, Val Loss: 0.4536\n",
      "Epoch [319/1000], Loss: 0.6470, Val Loss: 0.4535\n",
      "Epoch [320/1000], Loss: 0.2511, Val Loss: 0.4535\n",
      "Epoch [321/1000], Loss: 0.8511, Val Loss: 0.4534\n",
      "Epoch [322/1000], Loss: 0.3978, Val Loss: 0.4534\n",
      "Epoch [323/1000], Loss: 0.0095, Val Loss: 0.4533\n",
      "Epoch [324/1000], Loss: 0.0012, Val Loss: 0.4533\n",
      "Epoch [325/1000], Loss: 0.5489, Val Loss: 0.4533\n",
      "Epoch [326/1000], Loss: 0.7832, Val Loss: 0.4533\n",
      "Epoch [327/1000], Loss: 0.0452, Val Loss: 0.4532\n",
      "Epoch [328/1000], Loss: 0.1243, Val Loss: 0.4532\n",
      "Epoch [329/1000], Loss: 0.3183, Val Loss: 0.4532\n",
      "Epoch [330/1000], Loss: 1.0504, Val Loss: 0.4531\n",
      "Epoch [331/1000], Loss: 0.3917, Val Loss: 0.4531\n",
      "Epoch [332/1000], Loss: 1.1594, Val Loss: 0.4531\n",
      "Epoch [333/1000], Loss: 0.3168, Val Loss: 0.4530\n",
      "Epoch [334/1000], Loss: 0.6987, Val Loss: 0.4530\n",
      "Epoch [335/1000], Loss: 0.9028, Val Loss: 0.4530\n",
      "Epoch [336/1000], Loss: 0.0009, Val Loss: 0.4529\n",
      "Epoch [337/1000], Loss: 0.0820, Val Loss: 0.4529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [338/1000], Loss: 0.5344, Val Loss: 0.4528\n",
      "Epoch [339/1000], Loss: 0.0150, Val Loss: 0.4528\n",
      "Epoch [340/1000], Loss: 0.3776, Val Loss: 0.4528\n",
      "Epoch [341/1000], Loss: 0.5779, Val Loss: 0.4528\n",
      "Epoch [342/1000], Loss: 0.8411, Val Loss: 0.4527\n",
      "Epoch [343/1000], Loss: 0.2811, Val Loss: 0.4527\n",
      "Epoch [344/1000], Loss: 0.0033, Val Loss: 0.4527\n",
      "Epoch [345/1000], Loss: 0.8447, Val Loss: 0.4527\n",
      "Epoch [346/1000], Loss: 0.0079, Val Loss: 0.4526\n",
      "Epoch [347/1000], Loss: 0.5701, Val Loss: 0.4526\n",
      "Epoch [348/1000], Loss: 0.4954, Val Loss: 0.4525\n",
      "Epoch [349/1000], Loss: 0.7195, Val Loss: 0.4525\n",
      "Epoch [350/1000], Loss: 0.3101, Val Loss: 0.4524\n",
      "Epoch [351/1000], Loss: 0.0431, Val Loss: 0.4524\n",
      "Epoch [352/1000], Loss: 0.0520, Val Loss: 0.4524\n",
      "Epoch [353/1000], Loss: 0.6561, Val Loss: 0.4524\n",
      "Epoch [354/1000], Loss: 0.0640, Val Loss: 0.4524\n",
      "Epoch [355/1000], Loss: 0.4558, Val Loss: 0.4523\n",
      "Epoch [356/1000], Loss: 0.7128, Val Loss: 0.4523\n",
      "Epoch [357/1000], Loss: 0.5595, Val Loss: 0.4522\n",
      "Epoch [358/1000], Loss: 0.3467, Val Loss: 0.4522\n",
      "Epoch [359/1000], Loss: 0.0540, Val Loss: 0.4522\n",
      "Epoch [360/1000], Loss: 0.3370, Val Loss: 0.4522\n",
      "Epoch [361/1000], Loss: 0.0613, Val Loss: 0.4521\n",
      "Epoch [362/1000], Loss: 0.1405, Val Loss: 0.4521\n",
      "Epoch [363/1000], Loss: 0.6175, Val Loss: 0.4521\n",
      "Epoch [364/1000], Loss: 0.3012, Val Loss: 0.4521\n",
      "Epoch [365/1000], Loss: 0.6007, Val Loss: 0.4521\n",
      "Epoch [366/1000], Loss: 0.4282, Val Loss: 0.4521\n",
      "Epoch [367/1000], Loss: 0.2903, Val Loss: 0.4520\n",
      "Epoch [368/1000], Loss: 0.8694, Val Loss: 0.4520\n",
      "Epoch [369/1000], Loss: 0.0568, Val Loss: 0.4519\n",
      "Epoch [370/1000], Loss: 0.1032, Val Loss: 0.4520\n",
      "Epoch [371/1000], Loss: 0.9199, Val Loss: 0.4519\n",
      "Epoch [372/1000], Loss: 0.5474, Val Loss: 0.4519\n",
      "Epoch [373/1000], Loss: 0.0284, Val Loss: 0.4519\n",
      "Epoch [374/1000], Loss: 0.8917, Val Loss: 0.4519\n",
      "Epoch [375/1000], Loss: 0.9631, Val Loss: 0.4519\n",
      "Epoch [376/1000], Loss: 0.6901, Val Loss: 0.4518\n",
      "Epoch [377/1000], Loss: 0.1067, Val Loss: 0.4517\n",
      "Epoch [378/1000], Loss: 1.6049, Val Loss: 0.4517\n",
      "Epoch [379/1000], Loss: 0.0169, Val Loss: 0.4517\n",
      "Epoch [380/1000], Loss: 0.0091, Val Loss: 0.4517\n",
      "Epoch [381/1000], Loss: 0.0245, Val Loss: 0.4517\n",
      "Epoch [382/1000], Loss: 0.4872, Val Loss: 0.4517\n",
      "Epoch [383/1000], Loss: 0.0729, Val Loss: 0.4516\n",
      "Epoch [384/1000], Loss: 0.0003, Val Loss: 0.4516\n",
      "Epoch [385/1000], Loss: 0.2944, Val Loss: 0.4516\n",
      "Epoch [386/1000], Loss: 0.5038, Val Loss: 0.4515\n",
      "Epoch [387/1000], Loss: 0.5768, Val Loss: 0.4515\n",
      "Epoch [388/1000], Loss: 0.4923, Val Loss: 0.4514\n",
      "Epoch [389/1000], Loss: 0.2152, Val Loss: 0.4514\n",
      "Epoch [390/1000], Loss: 1.5039, Val Loss: 0.4514\n",
      "Epoch [391/1000], Loss: 0.3908, Val Loss: 0.4513\n",
      "Epoch [392/1000], Loss: 0.4368, Val Loss: 0.4514\n",
      "Epoch [393/1000], Loss: 0.1052, Val Loss: 0.4513\n",
      "Epoch [394/1000], Loss: 0.1106, Val Loss: 0.4513\n",
      "Epoch [395/1000], Loss: 0.1033, Val Loss: 0.4513\n",
      "Epoch [396/1000], Loss: 0.1341, Val Loss: 0.4512\n",
      "Epoch [397/1000], Loss: 0.5696, Val Loss: 0.4512\n",
      "Epoch [398/1000], Loss: 0.1002, Val Loss: 0.4512\n",
      "Epoch [399/1000], Loss: 0.1003, Val Loss: 0.4512\n",
      "Epoch [400/1000], Loss: 0.3615, Val Loss: 0.4512\n",
      "Epoch [401/1000], Loss: 0.1414, Val Loss: 0.4512\n",
      "Epoch [402/1000], Loss: 1.9409, Val Loss: 0.4512\n",
      "Epoch [403/1000], Loss: 0.0163, Val Loss: 0.4512\n",
      "Epoch [404/1000], Loss: 0.0999, Val Loss: 0.4512\n",
      "Epoch [405/1000], Loss: 0.0308, Val Loss: 0.4511\n",
      "Epoch [406/1000], Loss: 1.0740, Val Loss: 0.4512\n",
      "Epoch [407/1000], Loss: 0.2078, Val Loss: 0.4512\n",
      "Epoch [408/1000], Loss: 2.4311, Val Loss: 0.4512\n",
      "Epoch [409/1000], Loss: 0.3526, Val Loss: 0.4511\n",
      "Epoch [410/1000], Loss: 0.6606, Val Loss: 0.4511\n",
      "Epoch [411/1000], Loss: 0.0195, Val Loss: 0.4511\n",
      "Epoch [412/1000], Loss: 0.9241, Val Loss: 0.4511\n",
      "Epoch [413/1000], Loss: 0.0607, Val Loss: 0.4511\n",
      "Epoch [414/1000], Loss: 0.2590, Val Loss: 0.4511\n",
      "Epoch [415/1000], Loss: 0.1618, Val Loss: 0.4510\n",
      "Epoch [416/1000], Loss: 0.4491, Val Loss: 0.4510\n",
      "Epoch [417/1000], Loss: 0.4012, Val Loss: 0.4510\n",
      "Epoch [418/1000], Loss: 0.0798, Val Loss: 0.4510\n",
      "Epoch [419/1000], Loss: 0.4063, Val Loss: 0.4510\n",
      "Epoch [420/1000], Loss: 0.9841, Val Loss: 0.4510\n",
      "Epoch [421/1000], Loss: 0.8695, Val Loss: 0.4510\n",
      "Epoch [422/1000], Loss: 0.1278, Val Loss: 0.4510\n",
      "Epoch [423/1000], Loss: 0.1637, Val Loss: 0.4510\n",
      "Epoch [424/1000], Loss: 0.5538, Val Loss: 0.4510\n",
      "Epoch [425/1000], Loss: 0.7141, Val Loss: 0.4509\n",
      "Epoch [426/1000], Loss: 0.0351, Val Loss: 0.4509\n",
      "Epoch [427/1000], Loss: 0.0191, Val Loss: 0.4509\n",
      "Epoch [428/1000], Loss: 0.8789, Val Loss: 0.4508\n",
      "Epoch [429/1000], Loss: 0.8057, Val Loss: 0.4508\n",
      "Epoch [430/1000], Loss: 0.2862, Val Loss: 0.4508\n",
      "Epoch [431/1000], Loss: 1.1652, Val Loss: 0.4508\n",
      "Epoch [432/1000], Loss: 0.4602, Val Loss: 0.4508\n",
      "Epoch [433/1000], Loss: 0.3317, Val Loss: 0.4508\n",
      "Epoch [434/1000], Loss: 0.2053, Val Loss: 0.4508\n",
      "Epoch [435/1000], Loss: 0.1173, Val Loss: 0.4507\n",
      "Epoch [436/1000], Loss: 0.2145, Val Loss: 0.4507\n",
      "Epoch [437/1000], Loss: 0.0886, Val Loss: 0.4507\n",
      "Epoch [438/1000], Loss: 0.3462, Val Loss: 0.4506\n",
      "Epoch [439/1000], Loss: 0.0738, Val Loss: 0.4507\n",
      "Epoch [440/1000], Loss: 0.0632, Val Loss: 0.4507\n",
      "Epoch [441/1000], Loss: 0.6060, Val Loss: 0.4506\n",
      "Epoch [442/1000], Loss: 0.2267, Val Loss: 0.4506\n",
      "Epoch [443/1000], Loss: 0.0189, Val Loss: 0.4506\n",
      "Epoch [444/1000], Loss: 0.3426, Val Loss: 0.4506\n",
      "Epoch [445/1000], Loss: 0.0064, Val Loss: 0.4506\n",
      "Epoch [446/1000], Loss: 0.7442, Val Loss: 0.4506\n",
      "Epoch [447/1000], Loss: 0.6777, Val Loss: 0.4506\n",
      "Epoch [448/1000], Loss: 3.3985, Val Loss: 0.4505\n",
      "Epoch [449/1000], Loss: 0.1756, Val Loss: 0.4505\n",
      "Epoch [450/1000], Loss: 0.6086, Val Loss: 0.4504\n",
      "Epoch [451/1000], Loss: 4.7917, Val Loss: 0.4504\n",
      "Epoch [452/1000], Loss: 0.0423, Val Loss: 0.4504\n",
      "Epoch [453/1000], Loss: 0.5884, Val Loss: 0.4504\n",
      "Epoch [454/1000], Loss: 2.4605, Val Loss: 0.4504\n",
      "Epoch [455/1000], Loss: 0.9435, Val Loss: 0.4504\n",
      "Epoch [456/1000], Loss: 0.0385, Val Loss: 0.4503\n",
      "Epoch [457/1000], Loss: 0.3064, Val Loss: 0.4503\n",
      "Epoch [458/1000], Loss: 0.3262, Val Loss: 0.4503\n",
      "Epoch [459/1000], Loss: 0.1985, Val Loss: 0.4503\n",
      "Epoch [460/1000], Loss: 0.3562, Val Loss: 0.4502\n",
      "Epoch [461/1000], Loss: 0.0294, Val Loss: 0.4502\n",
      "Epoch [462/1000], Loss: 0.3447, Val Loss: 0.4502\n",
      "Epoch [463/1000], Loss: 0.3119, Val Loss: 0.4502\n",
      "Epoch [464/1000], Loss: 0.0157, Val Loss: 0.4502\n",
      "Epoch [465/1000], Loss: 0.0776, Val Loss: 0.4502\n",
      "Epoch [466/1000], Loss: 0.4061, Val Loss: 0.4502\n",
      "Epoch [467/1000], Loss: 0.7110, Val Loss: 0.4502\n",
      "Epoch [468/1000], Loss: 0.3626, Val Loss: 0.4502\n",
      "Epoch [469/1000], Loss: 0.4437, Val Loss: 0.4502\n",
      "Epoch [470/1000], Loss: 0.4374, Val Loss: 0.4502\n",
      "Epoch [471/1000], Loss: 0.3488, Val Loss: 0.4501\n",
      "Epoch [472/1000], Loss: 0.1967, Val Loss: 0.4501\n",
      "Epoch [473/1000], Loss: 0.8345, Val Loss: 0.4501\n",
      "Epoch [474/1000], Loss: 0.5301, Val Loss: 0.4501\n",
      "Epoch [475/1000], Loss: 0.5039, Val Loss: 0.4501\n",
      "Epoch [476/1000], Loss: 0.5514, Val Loss: 0.4501\n",
      "Epoch [477/1000], Loss: 0.7259, Val Loss: 0.4502\n",
      "Epoch [478/1000], Loss: 0.9525, Val Loss: 0.4502\n",
      "Epoch [479/1000], Loss: 0.0633, Val Loss: 0.4502\n",
      "Epoch [480/1000], Loss: 0.0165, Val Loss: 0.4501\n",
      "Epoch [481/1000], Loss: 1.1804, Val Loss: 0.4501\n",
      "Epoch [482/1000], Loss: 0.7627, Val Loss: 0.4502\n",
      "Epoch [483/1000], Loss: 0.0231, Val Loss: 0.4502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:10:43,278] Trial 17 finished with value: 0.8212911086527628 and parameters: {'hidden_size': 43, 'dropout_prob': 0.4980128570521487, 'learning_rate': 1.8041679326541958e-05, 'weight_decay': 9.58102124900193e-05}. Best is trial 12 with value: 0.8213242453748782.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 484\n",
      "Epoch [1/1000], Loss: 0.4423, Val Loss: 0.6187\n",
      "Epoch [2/1000], Loss: 0.9417, Val Loss: 0.5623\n",
      "Epoch [3/1000], Loss: 0.1311, Val Loss: 0.5295\n",
      "Epoch [4/1000], Loss: 0.1740, Val Loss: 0.5136\n",
      "Epoch [5/1000], Loss: 0.3253, Val Loss: 0.5042\n",
      "Epoch [6/1000], Loss: 0.0895, Val Loss: 0.4949\n",
      "Epoch [7/1000], Loss: 0.1226, Val Loss: 0.4871\n",
      "Epoch [8/1000], Loss: 0.9711, Val Loss: 0.4812\n",
      "Epoch [9/1000], Loss: 0.5505, Val Loss: 0.4765\n",
      "Epoch [10/1000], Loss: 0.4049, Val Loss: 0.4736\n",
      "Epoch [11/1000], Loss: 0.0608, Val Loss: 0.4711\n",
      "Epoch [12/1000], Loss: 0.3466, Val Loss: 0.4685\n",
      "Epoch [13/1000], Loss: 0.7763, Val Loss: 0.4666\n",
      "Epoch [14/1000], Loss: 1.8089, Val Loss: 0.4652\n",
      "Epoch [15/1000], Loss: 0.0075, Val Loss: 0.4642\n",
      "Epoch [16/1000], Loss: 0.0776, Val Loss: 0.4625\n",
      "Epoch [17/1000], Loss: 0.4876, Val Loss: 0.4619\n",
      "Epoch [18/1000], Loss: 0.0264, Val Loss: 0.4608\n",
      "Epoch [19/1000], Loss: 0.8424, Val Loss: 0.4594\n",
      "Epoch [20/1000], Loss: 0.7155, Val Loss: 0.4586\n",
      "Epoch [21/1000], Loss: 0.0068, Val Loss: 0.4577\n",
      "Epoch [22/1000], Loss: 0.2880, Val Loss: 0.4570\n",
      "Epoch [23/1000], Loss: 1.2214, Val Loss: 0.4562\n",
      "Epoch [24/1000], Loss: 0.5558, Val Loss: 0.4554\n",
      "Epoch [25/1000], Loss: 0.4419, Val Loss: 0.4546\n",
      "Epoch [26/1000], Loss: 0.7115, Val Loss: 0.4539\n",
      "Epoch [27/1000], Loss: 0.6421, Val Loss: 0.4537\n",
      "Epoch [28/1000], Loss: 0.6382, Val Loss: 0.4534\n",
      "Epoch [29/1000], Loss: 0.0595, Val Loss: 0.4529\n",
      "Epoch [30/1000], Loss: 0.3486, Val Loss: 0.4526\n",
      "Epoch [31/1000], Loss: 0.4130, Val Loss: 0.4521\n",
      "Epoch [32/1000], Loss: 0.3213, Val Loss: 0.4518\n",
      "Epoch [33/1000], Loss: 0.8539, Val Loss: 0.4515\n",
      "Epoch [34/1000], Loss: 0.4634, Val Loss: 0.4514\n",
      "Epoch [35/1000], Loss: 0.5709, Val Loss: 0.4508\n",
      "Epoch [36/1000], Loss: 0.0340, Val Loss: 0.4507\n",
      "Epoch [37/1000], Loss: 0.6984, Val Loss: 0.4503\n",
      "Epoch [38/1000], Loss: 0.4509, Val Loss: 0.4503\n",
      "Epoch [39/1000], Loss: 0.1013, Val Loss: 0.4498\n",
      "Epoch [40/1000], Loss: 0.5019, Val Loss: 0.4498\n",
      "Epoch [41/1000], Loss: 1.1566, Val Loss: 0.4495\n",
      "Epoch [42/1000], Loss: 0.9901, Val Loss: 0.4495\n",
      "Epoch [43/1000], Loss: 0.0054, Val Loss: 0.4495\n",
      "Epoch [44/1000], Loss: 0.0101, Val Loss: 0.4493\n",
      "Epoch [45/1000], Loss: 0.0102, Val Loss: 0.4493\n",
      "Epoch [46/1000], Loss: 0.0419, Val Loss: 0.4493\n",
      "Epoch [47/1000], Loss: 3.2625, Val Loss: 0.4491\n",
      "Epoch [48/1000], Loss: 0.7785, Val Loss: 0.4492\n",
      "Epoch [49/1000], Loss: 0.3950, Val Loss: 0.4489\n",
      "Epoch [50/1000], Loss: 0.2836, Val Loss: 0.4487\n",
      "Epoch [51/1000], Loss: 0.0868, Val Loss: 0.4486\n",
      "Epoch [52/1000], Loss: 0.7578, Val Loss: 0.4485\n",
      "Epoch [53/1000], Loss: 0.1323, Val Loss: 0.4489\n",
      "Epoch [54/1000], Loss: 0.3023, Val Loss: 0.4488\n",
      "Epoch [55/1000], Loss: 0.1495, Val Loss: 0.4487\n",
      "Epoch [56/1000], Loss: 0.0955, Val Loss: 0.4490\n",
      "Epoch [57/1000], Loss: 0.0925, Val Loss: 0.4486\n",
      "Epoch [58/1000], Loss: 0.0191, Val Loss: 0.4488\n",
      "Epoch [59/1000], Loss: 0.8386, Val Loss: 0.4487\n",
      "Epoch [60/1000], Loss: 0.3623, Val Loss: 0.4483\n",
      "Epoch [61/1000], Loss: 0.0010, Val Loss: 0.4483\n",
      "Epoch [62/1000], Loss: 0.4773, Val Loss: 0.4482\n",
      "Epoch [63/1000], Loss: 0.7668, Val Loss: 0.4487\n",
      "Epoch [64/1000], Loss: 0.9802, Val Loss: 0.4485\n",
      "Epoch [65/1000], Loss: 0.0695, Val Loss: 0.4483\n",
      "Epoch [66/1000], Loss: 0.5384, Val Loss: 0.4482\n",
      "Epoch [67/1000], Loss: 0.0809, Val Loss: 0.4484\n",
      "Epoch [68/1000], Loss: 0.1860, Val Loss: 0.4483\n",
      "Epoch [69/1000], Loss: 0.1784, Val Loss: 0.4485\n",
      "Epoch [70/1000], Loss: 0.2893, Val Loss: 0.4483\n",
      "Epoch [71/1000], Loss: 0.2761, Val Loss: 0.4481\n",
      "Epoch [72/1000], Loss: 0.9297, Val Loss: 0.4481\n",
      "Epoch [73/1000], Loss: 0.0439, Val Loss: 0.4481\n",
      "Epoch [74/1000], Loss: 1.5348, Val Loss: 0.4483\n",
      "Epoch [75/1000], Loss: 0.5222, Val Loss: 0.4484\n",
      "Epoch [76/1000], Loss: 0.9733, Val Loss: 0.4484\n",
      "Epoch [77/1000], Loss: 0.1300, Val Loss: 0.4485\n",
      "Epoch [78/1000], Loss: 0.8820, Val Loss: 0.4483\n",
      "Epoch [79/1000], Loss: 1.4848, Val Loss: 0.4484\n",
      "Epoch [80/1000], Loss: 0.1228, Val Loss: 0.4484\n",
      "Epoch [81/1000], Loss: 0.0223, Val Loss: 0.4484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:11:04,750] Trial 18 finished with value: 0.8217499248051305 and parameters: {'hidden_size': 41, 'dropout_prob': 0.6297932242395012, 'learning_rate': 0.00015976743969715784, 'weight_decay': 9.963549838526717e-05}. Best is trial 18 with value: 0.8217499248051305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 82\n",
      "Epoch [1/1000], Loss: 0.7507, Val Loss: 0.7041\n",
      "Epoch [2/1000], Loss: 0.6699, Val Loss: 0.7009\n",
      "Epoch [3/1000], Loss: 0.6278, Val Loss: 0.6984\n",
      "Epoch [4/1000], Loss: 0.7231, Val Loss: 0.6963\n",
      "Epoch [5/1000], Loss: 0.6886, Val Loss: 0.6943\n",
      "Epoch [6/1000], Loss: 0.7735, Val Loss: 0.6922\n",
      "Epoch [7/1000], Loss: 0.6811, Val Loss: 0.6900\n",
      "Epoch [8/1000], Loss: 0.6896, Val Loss: 0.6875\n",
      "Epoch [9/1000], Loss: 0.6992, Val Loss: 0.6847\n",
      "Epoch [10/1000], Loss: 0.6890, Val Loss: 0.6811\n",
      "Epoch [11/1000], Loss: 0.7853, Val Loss: 0.6768\n",
      "Epoch [12/1000], Loss: 0.6342, Val Loss: 0.6719\n",
      "Epoch [13/1000], Loss: 0.7218, Val Loss: 0.6656\n",
      "Epoch [14/1000], Loss: 0.5435, Val Loss: 0.6582\n",
      "Epoch [15/1000], Loss: 0.7759, Val Loss: 0.6497\n",
      "Epoch [16/1000], Loss: 0.6294, Val Loss: 0.6401\n",
      "Epoch [17/1000], Loss: 0.8456, Val Loss: 0.6293\n",
      "Epoch [18/1000], Loss: 0.5140, Val Loss: 0.6178\n",
      "Epoch [19/1000], Loss: 0.8167, Val Loss: 0.6059\n",
      "Epoch [20/1000], Loss: 0.4892, Val Loss: 0.5943\n",
      "Epoch [21/1000], Loss: 0.2323, Val Loss: 0.5831\n",
      "Epoch [22/1000], Loss: 0.3054, Val Loss: 0.5732\n",
      "Epoch [23/1000], Loss: 0.4222, Val Loss: 0.5639\n",
      "Epoch [24/1000], Loss: 0.4648, Val Loss: 0.5560\n",
      "Epoch [25/1000], Loss: 0.3650, Val Loss: 0.5495\n",
      "Epoch [26/1000], Loss: 0.9130, Val Loss: 0.5438\n",
      "Epoch [27/1000], Loss: 0.5091, Val Loss: 0.5390\n",
      "Epoch [28/1000], Loss: 0.4773, Val Loss: 0.5352\n",
      "Epoch [29/1000], Loss: 0.4398, Val Loss: 0.5323\n",
      "Epoch [30/1000], Loss: 0.8211, Val Loss: 0.5294\n",
      "Epoch [31/1000], Loss: 0.0931, Val Loss: 0.5270\n",
      "Epoch [32/1000], Loss: 0.3676, Val Loss: 0.5251\n",
      "Epoch [33/1000], Loss: 0.4892, Val Loss: 0.5233\n",
      "Epoch [34/1000], Loss: 0.2915, Val Loss: 0.5214\n",
      "Epoch [35/1000], Loss: 0.4134, Val Loss: 0.5198\n",
      "Epoch [36/1000], Loss: 0.4687, Val Loss: 0.5184\n",
      "Epoch [37/1000], Loss: 0.3402, Val Loss: 0.5171\n",
      "Epoch [38/1000], Loss: 1.5433, Val Loss: 0.5158\n",
      "Epoch [39/1000], Loss: 0.8060, Val Loss: 0.5146\n",
      "Epoch [40/1000], Loss: 1.2896, Val Loss: 0.5134\n",
      "Epoch [41/1000], Loss: 0.5085, Val Loss: 0.5124\n",
      "Epoch [42/1000], Loss: 0.6106, Val Loss: 0.5114\n",
      "Epoch [43/1000], Loss: 1.1819, Val Loss: 0.5104\n",
      "Epoch [44/1000], Loss: 0.9653, Val Loss: 0.5095\n",
      "Epoch [45/1000], Loss: 0.4789, Val Loss: 0.5086\n",
      "Epoch [46/1000], Loss: 0.0343, Val Loss: 0.5076\n",
      "Epoch [47/1000], Loss: 0.7932, Val Loss: 0.5067\n",
      "Epoch [48/1000], Loss: 0.2201, Val Loss: 0.5059\n",
      "Epoch [49/1000], Loss: 0.1321, Val Loss: 0.5050\n",
      "Epoch [50/1000], Loss: 0.8940, Val Loss: 0.5041\n",
      "Epoch [51/1000], Loss: 0.2602, Val Loss: 0.5032\n",
      "Epoch [52/1000], Loss: 0.8858, Val Loss: 0.5023\n",
      "Epoch [53/1000], Loss: 0.0200, Val Loss: 0.5016\n",
      "Epoch [54/1000], Loss: 0.2541, Val Loss: 0.5008\n",
      "Epoch [55/1000], Loss: 0.4095, Val Loss: 0.5000\n",
      "Epoch [56/1000], Loss: 0.2883, Val Loss: 0.4993\n",
      "Epoch [57/1000], Loss: 0.9901, Val Loss: 0.4985\n",
      "Epoch [58/1000], Loss: 0.5513, Val Loss: 0.4978\n",
      "Epoch [59/1000], Loss: 0.9208, Val Loss: 0.4971\n",
      "Epoch [60/1000], Loss: 1.1943, Val Loss: 0.4964\n",
      "Epoch [61/1000], Loss: 0.9819, Val Loss: 0.4957\n",
      "Epoch [62/1000], Loss: 0.1959, Val Loss: 0.4951\n",
      "Epoch [63/1000], Loss: 0.8707, Val Loss: 0.4944\n",
      "Epoch [64/1000], Loss: 0.7475, Val Loss: 0.4937\n",
      "Epoch [65/1000], Loss: 0.0357, Val Loss: 0.4930\n",
      "Epoch [66/1000], Loss: 0.1458, Val Loss: 0.4924\n",
      "Epoch [67/1000], Loss: 0.0445, Val Loss: 0.4918\n",
      "Epoch [68/1000], Loss: 0.3874, Val Loss: 0.4911\n",
      "Epoch [69/1000], Loss: 0.6144, Val Loss: 0.4905\n",
      "Epoch [70/1000], Loss: 1.0427, Val Loss: 0.4900\n",
      "Epoch [71/1000], Loss: 0.5131, Val Loss: 0.4894\n",
      "Epoch [72/1000], Loss: 0.4756, Val Loss: 0.4889\n",
      "Epoch [73/1000], Loss: 0.8717, Val Loss: 0.4885\n",
      "Epoch [74/1000], Loss: 0.2710, Val Loss: 0.4879\n",
      "Epoch [75/1000], Loss: 0.1220, Val Loss: 0.4874\n",
      "Epoch [76/1000], Loss: 0.3255, Val Loss: 0.4868\n",
      "Epoch [77/1000], Loss: 0.0256, Val Loss: 0.4863\n",
      "Epoch [78/1000], Loss: 0.7558, Val Loss: 0.4858\n",
      "Epoch [79/1000], Loss: 0.4124, Val Loss: 0.4852\n",
      "Epoch [80/1000], Loss: 0.3446, Val Loss: 0.4847\n",
      "Epoch [81/1000], Loss: 0.0875, Val Loss: 0.4842\n",
      "Epoch [82/1000], Loss: 0.8400, Val Loss: 0.4837\n",
      "Epoch [83/1000], Loss: 0.3590, Val Loss: 0.4833\n",
      "Epoch [84/1000], Loss: 0.0268, Val Loss: 0.4828\n",
      "Epoch [85/1000], Loss: 0.2537, Val Loss: 0.4824\n",
      "Epoch [86/1000], Loss: 0.4500, Val Loss: 0.4819\n",
      "Epoch [87/1000], Loss: 0.2170, Val Loss: 0.4816\n",
      "Epoch [88/1000], Loss: 0.0241, Val Loss: 0.4811\n",
      "Epoch [89/1000], Loss: 0.3435, Val Loss: 0.4808\n",
      "Epoch [90/1000], Loss: 0.0454, Val Loss: 0.4803\n",
      "Epoch [91/1000], Loss: 0.1177, Val Loss: 0.4799\n",
      "Epoch [92/1000], Loss: 0.0566, Val Loss: 0.4795\n",
      "Epoch [93/1000], Loss: 0.8544, Val Loss: 0.4792\n",
      "Epoch [94/1000], Loss: 0.1155, Val Loss: 0.4788\n",
      "Epoch [95/1000], Loss: 0.5171, Val Loss: 0.4784\n",
      "Epoch [96/1000], Loss: 0.1682, Val Loss: 0.4781\n",
      "Epoch [97/1000], Loss: 0.4749, Val Loss: 0.4778\n",
      "Epoch [98/1000], Loss: 0.2996, Val Loss: 0.4774\n",
      "Epoch [99/1000], Loss: 0.5997, Val Loss: 0.4771\n",
      "Epoch [100/1000], Loss: 0.6422, Val Loss: 0.4768\n",
      "Epoch [101/1000], Loss: 0.3726, Val Loss: 0.4765\n",
      "Epoch [102/1000], Loss: 0.4105, Val Loss: 0.4762\n",
      "Epoch [103/1000], Loss: 0.0287, Val Loss: 0.4760\n",
      "Epoch [104/1000], Loss: 0.2338, Val Loss: 0.4757\n",
      "Epoch [105/1000], Loss: 0.6264, Val Loss: 0.4754\n",
      "Epoch [106/1000], Loss: 0.0630, Val Loss: 0.4751\n",
      "Epoch [107/1000], Loss: 0.3321, Val Loss: 0.4749\n",
      "Epoch [108/1000], Loss: 0.1570, Val Loss: 0.4746\n",
      "Epoch [109/1000], Loss: 0.5784, Val Loss: 0.4744\n",
      "Epoch [110/1000], Loss: 0.4509, Val Loss: 0.4742\n",
      "Epoch [111/1000], Loss: 0.7989, Val Loss: 0.4739\n",
      "Epoch [112/1000], Loss: 0.1140, Val Loss: 0.4737\n",
      "Epoch [113/1000], Loss: 0.0369, Val Loss: 0.4734\n",
      "Epoch [114/1000], Loss: 0.2464, Val Loss: 0.4732\n",
      "Epoch [115/1000], Loss: 0.7744, Val Loss: 0.4730\n",
      "Epoch [116/1000], Loss: 0.0063, Val Loss: 0.4728\n",
      "Epoch [117/1000], Loss: 2.5181, Val Loss: 0.4725\n",
      "Epoch [118/1000], Loss: 0.2900, Val Loss: 0.4724\n",
      "Epoch [119/1000], Loss: 0.2060, Val Loss: 0.4722\n",
      "Epoch [120/1000], Loss: 0.0071, Val Loss: 0.4720\n",
      "Epoch [121/1000], Loss: 0.1366, Val Loss: 0.4718\n",
      "Epoch [122/1000], Loss: 0.0248, Val Loss: 0.4716\n",
      "Epoch [123/1000], Loss: 0.0400, Val Loss: 0.4714\n",
      "Epoch [124/1000], Loss: 0.1492, Val Loss: 0.4712\n",
      "Epoch [125/1000], Loss: 0.4913, Val Loss: 0.4710\n",
      "Epoch [126/1000], Loss: 0.3348, Val Loss: 0.4708\n",
      "Epoch [127/1000], Loss: 1.0871, Val Loss: 0.4706\n",
      "Epoch [128/1000], Loss: 0.5109, Val Loss: 0.4704\n",
      "Epoch [129/1000], Loss: 0.7591, Val Loss: 0.4702\n",
      "Epoch [130/1000], Loss: 0.7266, Val Loss: 0.4700\n",
      "Epoch [131/1000], Loss: 0.0378, Val Loss: 0.4699\n",
      "Epoch [132/1000], Loss: 0.0941, Val Loss: 0.4696\n",
      "Epoch [133/1000], Loss: 0.7330, Val Loss: 0.4695\n",
      "Epoch [134/1000], Loss: 0.6980, Val Loss: 0.4694\n",
      "Epoch [135/1000], Loss: 0.7208, Val Loss: 0.4692\n",
      "Epoch [136/1000], Loss: 0.0898, Val Loss: 0.4690\n",
      "Epoch [137/1000], Loss: 0.8972, Val Loss: 0.4688\n",
      "Epoch [138/1000], Loss: 0.0363, Val Loss: 0.4687\n",
      "Epoch [139/1000], Loss: 0.2114, Val Loss: 0.4686\n",
      "Epoch [140/1000], Loss: 0.1756, Val Loss: 0.4684\n",
      "Epoch [141/1000], Loss: 0.0576, Val Loss: 0.4682\n",
      "Epoch [142/1000], Loss: 0.1698, Val Loss: 0.4681\n",
      "Epoch [143/1000], Loss: 0.5436, Val Loss: 0.4679\n",
      "Epoch [144/1000], Loss: 0.3387, Val Loss: 0.4677\n",
      "Epoch [145/1000], Loss: 0.6710, Val Loss: 0.4676\n",
      "Epoch [146/1000], Loss: 0.0511, Val Loss: 0.4674\n",
      "Epoch [147/1000], Loss: 0.2959, Val Loss: 0.4673\n",
      "Epoch [148/1000], Loss: 0.2426, Val Loss: 0.4671\n",
      "Epoch [149/1000], Loss: 2.3753, Val Loss: 0.4670\n",
      "Epoch [150/1000], Loss: 2.1426, Val Loss: 0.4668\n",
      "Epoch [151/1000], Loss: 0.1732, Val Loss: 0.4667\n",
      "Epoch [152/1000], Loss: 0.1518, Val Loss: 0.4666\n",
      "Epoch [153/1000], Loss: 0.6827, Val Loss: 0.4664\n",
      "Epoch [154/1000], Loss: 0.8455, Val Loss: 0.4663\n",
      "Epoch [155/1000], Loss: 0.5464, Val Loss: 0.4662\n",
      "Epoch [156/1000], Loss: 0.6662, Val Loss: 0.4661\n",
      "Epoch [157/1000], Loss: 0.4817, Val Loss: 0.4660\n",
      "Epoch [158/1000], Loss: 0.1677, Val Loss: 0.4659\n",
      "Epoch [159/1000], Loss: 0.1538, Val Loss: 0.4657\n",
      "Epoch [160/1000], Loss: 0.3306, Val Loss: 0.4656\n",
      "Epoch [161/1000], Loss: 0.7007, Val Loss: 0.4655\n",
      "Epoch [162/1000], Loss: 0.7032, Val Loss: 0.4653\n",
      "Epoch [163/1000], Loss: 0.5025, Val Loss: 0.4652\n",
      "Epoch [164/1000], Loss: 0.0676, Val Loss: 0.4650\n",
      "Epoch [165/1000], Loss: 0.2537, Val Loss: 0.4649\n",
      "Epoch [166/1000], Loss: 0.6726, Val Loss: 0.4648\n",
      "Epoch [167/1000], Loss: 0.0131, Val Loss: 0.4647\n",
      "Epoch [168/1000], Loss: 0.6830, Val Loss: 0.4646\n",
      "Epoch [169/1000], Loss: 0.7201, Val Loss: 0.4645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 0.0414, Val Loss: 0.4643\n",
      "Epoch [171/1000], Loss: 1.5315, Val Loss: 0.4642\n",
      "Epoch [172/1000], Loss: 0.0570, Val Loss: 0.4641\n",
      "Epoch [173/1000], Loss: 1.1978, Val Loss: 0.4640\n",
      "Epoch [174/1000], Loss: 0.0398, Val Loss: 0.4639\n",
      "Epoch [175/1000], Loss: 0.5399, Val Loss: 0.4638\n",
      "Epoch [176/1000], Loss: 0.0721, Val Loss: 0.4637\n",
      "Epoch [177/1000], Loss: 0.0977, Val Loss: 0.4635\n",
      "Epoch [178/1000], Loss: 1.5154, Val Loss: 0.4635\n",
      "Epoch [179/1000], Loss: 0.6913, Val Loss: 0.4633\n",
      "Epoch [180/1000], Loss: 3.2650, Val Loss: 0.4633\n",
      "Epoch [181/1000], Loss: 0.3817, Val Loss: 0.4632\n",
      "Epoch [182/1000], Loss: 0.0901, Val Loss: 0.4631\n",
      "Epoch [183/1000], Loss: 0.4076, Val Loss: 0.4630\n",
      "Epoch [184/1000], Loss: 0.6700, Val Loss: 0.4628\n",
      "Epoch [185/1000], Loss: 0.5602, Val Loss: 0.4628\n",
      "Epoch [186/1000], Loss: 0.6271, Val Loss: 0.4627\n",
      "Epoch [187/1000], Loss: 0.0040, Val Loss: 0.4626\n",
      "Epoch [188/1000], Loss: 0.6249, Val Loss: 0.4625\n",
      "Epoch [189/1000], Loss: 0.6695, Val Loss: 0.4624\n",
      "Epoch [190/1000], Loss: 0.1678, Val Loss: 0.4623\n",
      "Epoch [191/1000], Loss: 0.6339, Val Loss: 0.4622\n",
      "Epoch [192/1000], Loss: 0.3027, Val Loss: 0.4621\n",
      "Epoch [193/1000], Loss: 0.5522, Val Loss: 0.4620\n",
      "Epoch [194/1000], Loss: 0.5560, Val Loss: 0.4619\n",
      "Epoch [195/1000], Loss: 0.6477, Val Loss: 0.4618\n",
      "Epoch [196/1000], Loss: 0.0025, Val Loss: 0.4617\n",
      "Epoch [197/1000], Loss: 0.2071, Val Loss: 0.4616\n",
      "Epoch [198/1000], Loss: 1.0382, Val Loss: 0.4615\n",
      "Epoch [199/1000], Loss: 0.0934, Val Loss: 0.4614\n",
      "Epoch [200/1000], Loss: 0.0407, Val Loss: 0.4614\n",
      "Epoch [201/1000], Loss: 0.3751, Val Loss: 0.4613\n",
      "Epoch [202/1000], Loss: 0.4241, Val Loss: 0.4612\n",
      "Epoch [203/1000], Loss: 0.0635, Val Loss: 0.4611\n",
      "Epoch [204/1000], Loss: 0.4575, Val Loss: 0.4610\n",
      "Epoch [205/1000], Loss: 0.8546, Val Loss: 0.4609\n",
      "Epoch [206/1000], Loss: 0.7335, Val Loss: 0.4608\n",
      "Epoch [207/1000], Loss: 0.1969, Val Loss: 0.4607\n",
      "Epoch [208/1000], Loss: 0.0172, Val Loss: 0.4606\n",
      "Epoch [209/1000], Loss: 0.0627, Val Loss: 0.4605\n",
      "Epoch [210/1000], Loss: 0.3251, Val Loss: 0.4604\n",
      "Epoch [211/1000], Loss: 0.7043, Val Loss: 0.4603\n",
      "Epoch [212/1000], Loss: 0.6468, Val Loss: 0.4602\n",
      "Epoch [213/1000], Loss: 0.0339, Val Loss: 0.4602\n",
      "Epoch [214/1000], Loss: 0.4005, Val Loss: 0.4601\n",
      "Epoch [215/1000], Loss: 0.7480, Val Loss: 0.4600\n",
      "Epoch [216/1000], Loss: 0.8671, Val Loss: 0.4599\n",
      "Epoch [217/1000], Loss: 0.5889, Val Loss: 0.4599\n",
      "Epoch [218/1000], Loss: 0.4516, Val Loss: 0.4598\n",
      "Epoch [219/1000], Loss: 0.8247, Val Loss: 0.4597\n",
      "Epoch [220/1000], Loss: 0.3839, Val Loss: 0.4596\n",
      "Epoch [221/1000], Loss: 0.1041, Val Loss: 0.4596\n",
      "Epoch [222/1000], Loss: 1.3038, Val Loss: 0.4595\n",
      "Epoch [223/1000], Loss: 0.1613, Val Loss: 0.4594\n",
      "Epoch [224/1000], Loss: 0.6459, Val Loss: 0.4594\n",
      "Epoch [225/1000], Loss: 0.3572, Val Loss: 0.4593\n",
      "Epoch [226/1000], Loss: 0.6045, Val Loss: 0.4592\n",
      "Epoch [227/1000], Loss: 0.1396, Val Loss: 0.4591\n",
      "Epoch [228/1000], Loss: 1.3431, Val Loss: 0.4590\n",
      "Epoch [229/1000], Loss: 0.0116, Val Loss: 0.4589\n",
      "Epoch [230/1000], Loss: 0.5946, Val Loss: 0.4588\n",
      "Epoch [231/1000], Loss: 0.0291, Val Loss: 0.4587\n",
      "Epoch [232/1000], Loss: 0.5702, Val Loss: 0.4587\n",
      "Epoch [233/1000], Loss: 0.3374, Val Loss: 0.4586\n",
      "Epoch [234/1000], Loss: 0.7999, Val Loss: 0.4585\n",
      "Epoch [235/1000], Loss: 0.0628, Val Loss: 0.4584\n",
      "Epoch [236/1000], Loss: 0.0001, Val Loss: 0.4584\n",
      "Epoch [237/1000], Loss: 0.3386, Val Loss: 0.4583\n",
      "Epoch [238/1000], Loss: 0.0076, Val Loss: 0.4582\n",
      "Epoch [239/1000], Loss: 0.0056, Val Loss: 0.4581\n",
      "Epoch [240/1000], Loss: 0.4922, Val Loss: 0.4581\n",
      "Epoch [241/1000], Loss: 0.0162, Val Loss: 0.4580\n",
      "Epoch [242/1000], Loss: 0.1349, Val Loss: 0.4579\n",
      "Epoch [243/1000], Loss: 0.7688, Val Loss: 0.4579\n",
      "Epoch [244/1000], Loss: 0.5324, Val Loss: 0.4578\n",
      "Epoch [245/1000], Loss: 0.0605, Val Loss: 0.4577\n",
      "Epoch [246/1000], Loss: 0.0039, Val Loss: 0.4576\n",
      "Epoch [247/1000], Loss: 0.0064, Val Loss: 0.4576\n",
      "Epoch [248/1000], Loss: 0.0477, Val Loss: 0.4575\n",
      "Epoch [249/1000], Loss: 0.0471, Val Loss: 0.4574\n",
      "Epoch [250/1000], Loss: 0.2156, Val Loss: 0.4573\n",
      "Epoch [251/1000], Loss: 0.7052, Val Loss: 0.4572\n",
      "Epoch [252/1000], Loss: 0.1793, Val Loss: 0.4572\n",
      "Epoch [253/1000], Loss: 0.6815, Val Loss: 0.4571\n",
      "Epoch [254/1000], Loss: 0.0393, Val Loss: 0.4570\n",
      "Epoch [255/1000], Loss: 0.5467, Val Loss: 0.4570\n",
      "Epoch [256/1000], Loss: 0.0293, Val Loss: 0.4569\n",
      "Epoch [257/1000], Loss: 0.3777, Val Loss: 0.4569\n",
      "Epoch [258/1000], Loss: 0.0194, Val Loss: 0.4568\n",
      "Epoch [259/1000], Loss: 0.1680, Val Loss: 0.4568\n",
      "Epoch [260/1000], Loss: 1.3012, Val Loss: 0.4568\n",
      "Epoch [261/1000], Loss: 0.2903, Val Loss: 0.4567\n",
      "Epoch [262/1000], Loss: 0.5206, Val Loss: 0.4566\n",
      "Epoch [263/1000], Loss: 2.2818, Val Loss: 0.4565\n",
      "Epoch [264/1000], Loss: 0.3634, Val Loss: 0.4564\n",
      "Epoch [265/1000], Loss: 1.2797, Val Loss: 0.4564\n",
      "Epoch [266/1000], Loss: 0.1008, Val Loss: 0.4563\n",
      "Epoch [267/1000], Loss: 0.4506, Val Loss: 0.4563\n",
      "Epoch [268/1000], Loss: 0.1591, Val Loss: 0.4563\n",
      "Epoch [269/1000], Loss: 0.1335, Val Loss: 0.4562\n",
      "Epoch [270/1000], Loss: 0.7588, Val Loss: 0.4562\n",
      "Epoch [271/1000], Loss: 0.0763, Val Loss: 0.4561\n",
      "Epoch [272/1000], Loss: 0.1828, Val Loss: 0.4561\n",
      "Epoch [273/1000], Loss: 0.8053, Val Loss: 0.4561\n",
      "Epoch [274/1000], Loss: 0.3950, Val Loss: 0.4560\n",
      "Epoch [275/1000], Loss: 0.0074, Val Loss: 0.4559\n",
      "Epoch [276/1000], Loss: 0.1368, Val Loss: 0.4559\n",
      "Epoch [277/1000], Loss: 0.0130, Val Loss: 0.4559\n",
      "Epoch [278/1000], Loss: 0.6233, Val Loss: 0.4558\n",
      "Epoch [279/1000], Loss: 0.0266, Val Loss: 0.4558\n",
      "Epoch [280/1000], Loss: 0.1510, Val Loss: 0.4557\n",
      "Epoch [281/1000], Loss: 0.3030, Val Loss: 0.4557\n",
      "Epoch [282/1000], Loss: 0.7137, Val Loss: 0.4556\n",
      "Epoch [283/1000], Loss: 0.0184, Val Loss: 0.4556\n",
      "Epoch [284/1000], Loss: 1.4965, Val Loss: 0.4555\n",
      "Epoch [285/1000], Loss: 0.5399, Val Loss: 0.4555\n",
      "Epoch [286/1000], Loss: 0.4641, Val Loss: 0.4555\n",
      "Epoch [287/1000], Loss: 0.7594, Val Loss: 0.4554\n",
      "Epoch [288/1000], Loss: 0.0324, Val Loss: 0.4554\n",
      "Epoch [289/1000], Loss: 0.0710, Val Loss: 0.4553\n",
      "Epoch [290/1000], Loss: 0.5491, Val Loss: 0.4552\n",
      "Epoch [291/1000], Loss: 0.7409, Val Loss: 0.4552\n",
      "Epoch [292/1000], Loss: 0.2430, Val Loss: 0.4551\n",
      "Epoch [293/1000], Loss: 0.8284, Val Loss: 0.4551\n",
      "Epoch [294/1000], Loss: 0.2532, Val Loss: 0.4550\n",
      "Epoch [295/1000], Loss: 0.1453, Val Loss: 0.4549\n",
      "Epoch [296/1000], Loss: 0.3954, Val Loss: 0.4549\n",
      "Epoch [297/1000], Loss: 0.1040, Val Loss: 0.4548\n",
      "Epoch [298/1000], Loss: 0.3838, Val Loss: 0.4548\n",
      "Epoch [299/1000], Loss: 0.0565, Val Loss: 0.4547\n",
      "Epoch [300/1000], Loss: 0.6012, Val Loss: 0.4547\n",
      "Epoch [301/1000], Loss: 0.4839, Val Loss: 0.4547\n",
      "Epoch [302/1000], Loss: 1.0489, Val Loss: 0.4546\n",
      "Epoch [303/1000], Loss: 0.6187, Val Loss: 0.4545\n",
      "Epoch [304/1000], Loss: 0.6111, Val Loss: 0.4545\n",
      "Epoch [305/1000], Loss: 0.0857, Val Loss: 0.4544\n",
      "Epoch [306/1000], Loss: 0.7497, Val Loss: 0.4544\n",
      "Epoch [307/1000], Loss: 0.6680, Val Loss: 0.4543\n",
      "Epoch [308/1000], Loss: 0.5589, Val Loss: 0.4543\n",
      "Epoch [309/1000], Loss: 0.1153, Val Loss: 0.4542\n",
      "Epoch [310/1000], Loss: 0.1931, Val Loss: 0.4542\n",
      "Epoch [311/1000], Loss: 0.6480, Val Loss: 0.4541\n",
      "Epoch [312/1000], Loss: 0.0238, Val Loss: 0.4541\n",
      "Epoch [313/1000], Loss: 0.7241, Val Loss: 0.4541\n",
      "Epoch [314/1000], Loss: 0.0661, Val Loss: 0.4540\n",
      "Epoch [315/1000], Loss: 0.1018, Val Loss: 0.4540\n",
      "Epoch [316/1000], Loss: 0.0564, Val Loss: 0.4540\n",
      "Epoch [317/1000], Loss: 0.0240, Val Loss: 0.4539\n",
      "Epoch [318/1000], Loss: 0.2100, Val Loss: 0.4539\n",
      "Epoch [319/1000], Loss: 0.0043, Val Loss: 0.4538\n",
      "Epoch [320/1000], Loss: 0.3532, Val Loss: 0.4538\n",
      "Epoch [321/1000], Loss: 0.1339, Val Loss: 0.4537\n",
      "Epoch [322/1000], Loss: 0.0284, Val Loss: 0.4536\n",
      "Epoch [323/1000], Loss: 0.0024, Val Loss: 0.4536\n",
      "Epoch [324/1000], Loss: 0.5920, Val Loss: 0.4536\n",
      "Epoch [325/1000], Loss: 1.3661, Val Loss: 0.4535\n",
      "Epoch [326/1000], Loss: 0.0215, Val Loss: 0.4535\n",
      "Epoch [327/1000], Loss: 0.2091, Val Loss: 0.4535\n",
      "Epoch [328/1000], Loss: 0.3162, Val Loss: 0.4535\n",
      "Epoch [329/1000], Loss: 3.4394, Val Loss: 0.4534\n",
      "Epoch [330/1000], Loss: 0.1221, Val Loss: 0.4534\n",
      "Epoch [331/1000], Loss: 0.6121, Val Loss: 0.4533\n",
      "Epoch [332/1000], Loss: 0.0189, Val Loss: 0.4533\n",
      "Epoch [333/1000], Loss: 0.3768, Val Loss: 0.4533\n",
      "Epoch [334/1000], Loss: 0.6283, Val Loss: 0.4532\n",
      "Epoch [335/1000], Loss: 0.6839, Val Loss: 0.4532\n",
      "Epoch [336/1000], Loss: 0.0382, Val Loss: 0.4531\n",
      "Epoch [337/1000], Loss: 0.7066, Val Loss: 0.4531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [338/1000], Loss: 0.2454, Val Loss: 0.4531\n",
      "Epoch [339/1000], Loss: 0.0637, Val Loss: 0.4531\n",
      "Epoch [340/1000], Loss: 0.1856, Val Loss: 0.4530\n",
      "Epoch [341/1000], Loss: 0.5995, Val Loss: 0.4530\n",
      "Epoch [342/1000], Loss: 0.2756, Val Loss: 0.4530\n",
      "Epoch [343/1000], Loss: 0.9818, Val Loss: 0.4529\n",
      "Epoch [344/1000], Loss: 0.7480, Val Loss: 0.4529\n",
      "Epoch [345/1000], Loss: 0.0517, Val Loss: 0.4529\n",
      "Epoch [346/1000], Loss: 0.4122, Val Loss: 0.4529\n",
      "Epoch [347/1000], Loss: 0.4155, Val Loss: 0.4528\n",
      "Epoch [348/1000], Loss: 0.6891, Val Loss: 0.4528\n",
      "Epoch [349/1000], Loss: 0.6881, Val Loss: 0.4527\n",
      "Epoch [350/1000], Loss: 0.7100, Val Loss: 0.4527\n",
      "Epoch [351/1000], Loss: 0.3088, Val Loss: 0.4527\n",
      "Epoch [352/1000], Loss: 0.4062, Val Loss: 0.4526\n",
      "Epoch [353/1000], Loss: 0.6112, Val Loss: 0.4526\n",
      "Epoch [354/1000], Loss: 1.0210, Val Loss: 0.4525\n",
      "Epoch [355/1000], Loss: 1.0205, Val Loss: 0.4525\n",
      "Epoch [356/1000], Loss: 0.0612, Val Loss: 0.4525\n",
      "Epoch [357/1000], Loss: 0.0104, Val Loss: 0.4524\n",
      "Epoch [358/1000], Loss: 0.3307, Val Loss: 0.4524\n",
      "Epoch [359/1000], Loss: 0.0112, Val Loss: 0.4524\n",
      "Epoch [360/1000], Loss: 0.0042, Val Loss: 0.4523\n",
      "Epoch [361/1000], Loss: 0.3689, Val Loss: 0.4523\n",
      "Epoch [362/1000], Loss: 0.3998, Val Loss: 0.4522\n",
      "Epoch [363/1000], Loss: 0.0709, Val Loss: 0.4522\n",
      "Epoch [364/1000], Loss: 0.0013, Val Loss: 0.4522\n",
      "Epoch [365/1000], Loss: 1.9778, Val Loss: 0.4522\n",
      "Epoch [366/1000], Loss: 0.2216, Val Loss: 0.4521\n",
      "Epoch [367/1000], Loss: 0.3718, Val Loss: 0.4521\n",
      "Epoch [368/1000], Loss: 0.6931, Val Loss: 0.4520\n",
      "Epoch [369/1000], Loss: 0.0852, Val Loss: 0.4520\n",
      "Epoch [370/1000], Loss: 0.1027, Val Loss: 0.4520\n",
      "Epoch [371/1000], Loss: 0.0025, Val Loss: 0.4519\n",
      "Epoch [372/1000], Loss: 0.2240, Val Loss: 0.4519\n",
      "Epoch [373/1000], Loss: 0.7768, Val Loss: 0.4519\n",
      "Epoch [374/1000], Loss: 0.2746, Val Loss: 0.4519\n",
      "Epoch [375/1000], Loss: 0.5839, Val Loss: 0.4519\n",
      "Epoch [376/1000], Loss: 0.1254, Val Loss: 0.4519\n",
      "Epoch [377/1000], Loss: 0.0154, Val Loss: 0.4518\n",
      "Epoch [378/1000], Loss: 0.5661, Val Loss: 0.4518\n",
      "Epoch [379/1000], Loss: 0.0196, Val Loss: 0.4518\n",
      "Epoch [380/1000], Loss: 0.8535, Val Loss: 0.4518\n",
      "Epoch [381/1000], Loss: 0.2347, Val Loss: 0.4517\n",
      "Epoch [382/1000], Loss: 0.4513, Val Loss: 0.4517\n",
      "Epoch [383/1000], Loss: 0.0170, Val Loss: 0.4517\n",
      "Epoch [384/1000], Loss: 0.0433, Val Loss: 0.4516\n",
      "Epoch [385/1000], Loss: 0.0064, Val Loss: 0.4516\n",
      "Epoch [386/1000], Loss: 0.6661, Val Loss: 0.4516\n",
      "Epoch [387/1000], Loss: 0.3438, Val Loss: 0.4516\n",
      "Epoch [388/1000], Loss: 0.0190, Val Loss: 0.4516\n",
      "Epoch [389/1000], Loss: 0.0094, Val Loss: 0.4516\n",
      "Epoch [390/1000], Loss: 0.6059, Val Loss: 0.4516\n",
      "Epoch [391/1000], Loss: 1.0595, Val Loss: 0.4515\n",
      "Epoch [392/1000], Loss: 0.0607, Val Loss: 0.4515\n",
      "Epoch [393/1000], Loss: 0.0528, Val Loss: 0.4514\n",
      "Epoch [394/1000], Loss: 0.1466, Val Loss: 0.4514\n",
      "Epoch [395/1000], Loss: 0.1705, Val Loss: 0.4514\n",
      "Epoch [396/1000], Loss: 0.1649, Val Loss: 0.4514\n",
      "Epoch [397/1000], Loss: 0.4782, Val Loss: 0.4514\n",
      "Epoch [398/1000], Loss: 0.0510, Val Loss: 0.4514\n",
      "Epoch [399/1000], Loss: 1.3879, Val Loss: 0.4513\n",
      "Epoch [400/1000], Loss: 0.0013, Val Loss: 0.4513\n",
      "Epoch [401/1000], Loss: 0.1533, Val Loss: 0.4513\n",
      "Epoch [402/1000], Loss: 0.0882, Val Loss: 0.4513\n",
      "Epoch [403/1000], Loss: 0.3751, Val Loss: 0.4512\n",
      "Epoch [404/1000], Loss: 0.1044, Val Loss: 0.4512\n",
      "Epoch [405/1000], Loss: 0.0099, Val Loss: 0.4512\n",
      "Epoch [406/1000], Loss: 0.0229, Val Loss: 0.4512\n",
      "Epoch [407/1000], Loss: 0.1595, Val Loss: 0.4512\n",
      "Epoch [408/1000], Loss: 0.1450, Val Loss: 0.4511\n",
      "Epoch [409/1000], Loss: 0.5151, Val Loss: 0.4511\n",
      "Epoch [410/1000], Loss: 0.0642, Val Loss: 0.4511\n",
      "Epoch [411/1000], Loss: 0.3643, Val Loss: 0.4511\n",
      "Epoch [412/1000], Loss: 0.0016, Val Loss: 0.4511\n",
      "Epoch [413/1000], Loss: 0.6401, Val Loss: 0.4511\n",
      "Epoch [414/1000], Loss: 1.2754, Val Loss: 0.4511\n",
      "Epoch [415/1000], Loss: 0.0307, Val Loss: 0.4511\n",
      "Epoch [416/1000], Loss: 0.0509, Val Loss: 0.4511\n",
      "Epoch [417/1000], Loss: 0.5383, Val Loss: 0.4510\n",
      "Epoch [418/1000], Loss: 3.8071, Val Loss: 0.4510\n",
      "Epoch [419/1000], Loss: 0.9404, Val Loss: 0.4509\n",
      "Epoch [420/1000], Loss: 0.0028, Val Loss: 0.4509\n",
      "Epoch [421/1000], Loss: 0.1922, Val Loss: 0.4509\n",
      "Epoch [422/1000], Loss: 0.6768, Val Loss: 0.4508\n",
      "Epoch [423/1000], Loss: 1.1442, Val Loss: 0.4508\n",
      "Epoch [424/1000], Loss: 0.6792, Val Loss: 0.4508\n",
      "Epoch [425/1000], Loss: 0.0721, Val Loss: 0.4507\n",
      "Epoch [426/1000], Loss: 0.4655, Val Loss: 0.4507\n",
      "Epoch [427/1000], Loss: 0.2986, Val Loss: 0.4507\n",
      "Epoch [428/1000], Loss: 0.1007, Val Loss: 0.4507\n",
      "Epoch [429/1000], Loss: 0.0043, Val Loss: 0.4507\n",
      "Epoch [430/1000], Loss: 0.0761, Val Loss: 0.4507\n",
      "Epoch [431/1000], Loss: 0.0162, Val Loss: 0.4506\n",
      "Epoch [432/1000], Loss: 0.1589, Val Loss: 0.4506\n",
      "Epoch [433/1000], Loss: 0.2975, Val Loss: 0.4506\n",
      "Epoch [434/1000], Loss: 0.4606, Val Loss: 0.4506\n",
      "Epoch [435/1000], Loss: 0.1316, Val Loss: 0.4505\n",
      "Epoch [436/1000], Loss: 0.2380, Val Loss: 0.4506\n",
      "Epoch [437/1000], Loss: 0.1404, Val Loss: 0.4506\n",
      "Epoch [438/1000], Loss: 0.0038, Val Loss: 0.4505\n",
      "Epoch [439/1000], Loss: 0.0084, Val Loss: 0.4505\n",
      "Epoch [440/1000], Loss: 0.5434, Val Loss: 0.4504\n",
      "Epoch [441/1000], Loss: 0.0974, Val Loss: 0.4504\n",
      "Epoch [442/1000], Loss: 0.0613, Val Loss: 0.4504\n",
      "Epoch [443/1000], Loss: 0.6357, Val Loss: 0.4504\n",
      "Epoch [444/1000], Loss: 3.4290, Val Loss: 0.4504\n",
      "Epoch [445/1000], Loss: 0.1064, Val Loss: 0.4503\n",
      "Epoch [446/1000], Loss: 1.9173, Val Loss: 0.4503\n",
      "Epoch [447/1000], Loss: 0.0245, Val Loss: 0.4502\n",
      "Epoch [448/1000], Loss: 0.4168, Val Loss: 0.4502\n",
      "Epoch [449/1000], Loss: 0.5973, Val Loss: 0.4502\n",
      "Epoch [450/1000], Loss: 0.2292, Val Loss: 0.4501\n",
      "Epoch [451/1000], Loss: 0.8511, Val Loss: 0.4501\n",
      "Epoch [452/1000], Loss: 0.3555, Val Loss: 0.4501\n",
      "Epoch [453/1000], Loss: 0.7035, Val Loss: 0.4501\n",
      "Epoch [454/1000], Loss: 0.0132, Val Loss: 0.4501\n",
      "Epoch [455/1000], Loss: 0.0250, Val Loss: 0.4501\n",
      "Epoch [456/1000], Loss: 0.0284, Val Loss: 0.4501\n",
      "Epoch [457/1000], Loss: 0.0445, Val Loss: 0.4501\n",
      "Epoch [458/1000], Loss: 1.4183, Val Loss: 0.4500\n",
      "Epoch [459/1000], Loss: 0.0503, Val Loss: 0.4500\n",
      "Epoch [460/1000], Loss: 0.0017, Val Loss: 0.4500\n",
      "Epoch [461/1000], Loss: 0.0022, Val Loss: 0.4500\n",
      "Epoch [462/1000], Loss: 1.4890, Val Loss: 0.4500\n",
      "Epoch [463/1000], Loss: 1.0312, Val Loss: 0.4500\n",
      "Epoch [464/1000], Loss: 1.6653, Val Loss: 0.4500\n",
      "Epoch [465/1000], Loss: 0.3597, Val Loss: 0.4499\n",
      "Epoch [466/1000], Loss: 0.6445, Val Loss: 0.4499\n",
      "Epoch [467/1000], Loss: 0.0564, Val Loss: 0.4499\n",
      "Epoch [468/1000], Loss: 0.2240, Val Loss: 0.4498\n",
      "Epoch [469/1000], Loss: 0.1104, Val Loss: 0.4498\n",
      "Epoch [470/1000], Loss: 0.0015, Val Loss: 0.4498\n",
      "Epoch [471/1000], Loss: 0.0136, Val Loss: 0.4498\n",
      "Epoch [472/1000], Loss: 0.5306, Val Loss: 0.4498\n",
      "Epoch [473/1000], Loss: 1.1906, Val Loss: 0.4498\n",
      "Epoch [474/1000], Loss: 0.4805, Val Loss: 0.4498\n",
      "Epoch [475/1000], Loss: 0.7593, Val Loss: 0.4497\n",
      "Epoch [476/1000], Loss: 0.0007, Val Loss: 0.4497\n",
      "Epoch [477/1000], Loss: 0.7922, Val Loss: 0.4497\n",
      "Epoch [478/1000], Loss: 0.1199, Val Loss: 0.4497\n",
      "Epoch [479/1000], Loss: 1.2605, Val Loss: 0.4497\n",
      "Epoch [480/1000], Loss: 0.5262, Val Loss: 0.4497\n",
      "Epoch [481/1000], Loss: 0.2772, Val Loss: 0.4497\n",
      "Epoch [482/1000], Loss: 0.6210, Val Loss: 0.4496\n",
      "Epoch [483/1000], Loss: 1.3539, Val Loss: 0.4496\n",
      "Epoch [484/1000], Loss: 0.6986, Val Loss: 0.4497\n",
      "Epoch [485/1000], Loss: 0.0400, Val Loss: 0.4497\n",
      "Epoch [486/1000], Loss: 0.1640, Val Loss: 0.4496\n",
      "Epoch [487/1000], Loss: 0.2061, Val Loss: 0.4496\n",
      "Epoch [488/1000], Loss: 0.0514, Val Loss: 0.4496\n",
      "Epoch [489/1000], Loss: 0.8174, Val Loss: 0.4496\n",
      "Epoch [490/1000], Loss: 0.1022, Val Loss: 0.4495\n",
      "Epoch [491/1000], Loss: 0.1208, Val Loss: 0.4495\n",
      "Epoch [492/1000], Loss: 0.3636, Val Loss: 0.4495\n",
      "Epoch [493/1000], Loss: 0.8035, Val Loss: 0.4495\n",
      "Epoch [494/1000], Loss: 0.0673, Val Loss: 0.4495\n",
      "Epoch [495/1000], Loss: 0.0078, Val Loss: 0.4495\n",
      "Epoch [496/1000], Loss: 0.2312, Val Loss: 0.4494\n",
      "Epoch [497/1000], Loss: 0.1698, Val Loss: 0.4494\n",
      "Epoch [498/1000], Loss: 0.3916, Val Loss: 0.4495\n",
      "Epoch [499/1000], Loss: 0.0061, Val Loss: 0.4494\n",
      "Epoch [500/1000], Loss: 0.2910, Val Loss: 0.4494\n",
      "Epoch [501/1000], Loss: 0.3058, Val Loss: 0.4494\n",
      "Epoch [502/1000], Loss: 0.6461, Val Loss: 0.4493\n",
      "Epoch [503/1000], Loss: 0.0020, Val Loss: 0.4493\n",
      "Epoch [504/1000], Loss: 0.3248, Val Loss: 0.4493\n",
      "Epoch [505/1000], Loss: 0.3010, Val Loss: 0.4493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [506/1000], Loss: 0.3755, Val Loss: 0.4493\n",
      "Epoch [507/1000], Loss: 0.2778, Val Loss: 0.4493\n",
      "Epoch [508/1000], Loss: 0.0040, Val Loss: 0.4493\n",
      "Epoch [509/1000], Loss: 0.8037, Val Loss: 0.4492\n",
      "Epoch [510/1000], Loss: 0.2571, Val Loss: 0.4492\n",
      "Epoch [511/1000], Loss: 0.2813, Val Loss: 0.4492\n",
      "Epoch [512/1000], Loss: 1.0433, Val Loss: 0.4492\n",
      "Epoch [513/1000], Loss: 0.0060, Val Loss: 0.4491\n",
      "Epoch [514/1000], Loss: 0.6572, Val Loss: 0.4491\n",
      "Epoch [515/1000], Loss: 0.1227, Val Loss: 0.4491\n",
      "Epoch [516/1000], Loss: 0.4187, Val Loss: 0.4491\n",
      "Epoch [517/1000], Loss: 0.0045, Val Loss: 0.4491\n",
      "Epoch [518/1000], Loss: 0.7553, Val Loss: 0.4491\n",
      "Epoch [519/1000], Loss: 0.1331, Val Loss: 0.4491\n",
      "Epoch [520/1000], Loss: 0.4927, Val Loss: 0.4491\n",
      "Epoch [521/1000], Loss: 0.4907, Val Loss: 0.4490\n",
      "Epoch [522/1000], Loss: 0.0199, Val Loss: 0.4490\n",
      "Epoch [523/1000], Loss: 0.1178, Val Loss: 0.4489\n",
      "Epoch [524/1000], Loss: 0.2642, Val Loss: 0.4489\n",
      "Epoch [525/1000], Loss: 0.0987, Val Loss: 0.4488\n",
      "Epoch [526/1000], Loss: 0.7178, Val Loss: 0.4488\n",
      "Epoch [527/1000], Loss: 1.0659, Val Loss: 0.4488\n",
      "Epoch [528/1000], Loss: 0.4549, Val Loss: 0.4488\n",
      "Epoch [529/1000], Loss: 0.0363, Val Loss: 0.4487\n",
      "Epoch [530/1000], Loss: 0.6261, Val Loss: 0.4487\n",
      "Epoch [531/1000], Loss: 0.8368, Val Loss: 0.4487\n",
      "Epoch [532/1000], Loss: 0.0506, Val Loss: 0.4486\n",
      "Epoch [533/1000], Loss: 0.7463, Val Loss: 0.4487\n",
      "Epoch [534/1000], Loss: 0.1065, Val Loss: 0.4487\n",
      "Epoch [535/1000], Loss: 0.6472, Val Loss: 0.4487\n",
      "Epoch [536/1000], Loss: 1.9511, Val Loss: 0.4487\n",
      "Epoch [537/1000], Loss: 0.1449, Val Loss: 0.4486\n",
      "Epoch [538/1000], Loss: 0.3112, Val Loss: 0.4486\n",
      "Epoch [539/1000], Loss: 0.0080, Val Loss: 0.4485\n",
      "Epoch [540/1000], Loss: 0.2175, Val Loss: 0.4485\n",
      "Epoch [541/1000], Loss: 0.9459, Val Loss: 0.4485\n",
      "Epoch [542/1000], Loss: 0.0009, Val Loss: 0.4484\n",
      "Epoch [543/1000], Loss: 0.3862, Val Loss: 0.4484\n",
      "Epoch [544/1000], Loss: 0.1043, Val Loss: 0.4484\n",
      "Epoch [545/1000], Loss: 0.1858, Val Loss: 0.4484\n",
      "Epoch [546/1000], Loss: 0.0386, Val Loss: 0.4484\n",
      "Epoch [547/1000], Loss: 0.3017, Val Loss: 0.4484\n",
      "Epoch [548/1000], Loss: 0.4811, Val Loss: 0.4484\n",
      "Epoch [549/1000], Loss: 0.2723, Val Loss: 0.4484\n",
      "Epoch [550/1000], Loss: 0.0013, Val Loss: 0.4484\n",
      "Epoch [551/1000], Loss: 0.0125, Val Loss: 0.4484\n",
      "Epoch [552/1000], Loss: 0.1116, Val Loss: 0.4484\n",
      "Epoch [553/1000], Loss: 0.7087, Val Loss: 0.4484\n",
      "Epoch [554/1000], Loss: 0.2074, Val Loss: 0.4483\n",
      "Epoch [555/1000], Loss: 0.4080, Val Loss: 0.4483\n",
      "Epoch [556/1000], Loss: 0.4865, Val Loss: 0.4483\n",
      "Epoch [557/1000], Loss: 0.0201, Val Loss: 0.4483\n",
      "Epoch [558/1000], Loss: 0.8895, Val Loss: 0.4483\n",
      "Epoch [559/1000], Loss: 0.5900, Val Loss: 0.4482\n",
      "Epoch [560/1000], Loss: 0.8024, Val Loss: 0.4482\n",
      "Epoch [561/1000], Loss: 0.0056, Val Loss: 0.4482\n",
      "Epoch [562/1000], Loss: 0.2895, Val Loss: 0.4482\n",
      "Epoch [563/1000], Loss: 0.0776, Val Loss: 0.4482\n",
      "Epoch [564/1000], Loss: 2.2002, Val Loss: 0.4482\n",
      "Epoch [565/1000], Loss: 0.0025, Val Loss: 0.4481\n",
      "Epoch [566/1000], Loss: 3.6175, Val Loss: 0.4481\n",
      "Epoch [567/1000], Loss: 0.0020, Val Loss: 0.4481\n",
      "Epoch [568/1000], Loss: 0.0141, Val Loss: 0.4481\n",
      "Epoch [569/1000], Loss: 0.3483, Val Loss: 0.4481\n",
      "Epoch [570/1000], Loss: 0.5413, Val Loss: 0.4480\n",
      "Epoch [571/1000], Loss: 0.4451, Val Loss: 0.4480\n",
      "Epoch [572/1000], Loss: 0.0399, Val Loss: 0.4480\n",
      "Epoch [573/1000], Loss: 0.0721, Val Loss: 0.4480\n",
      "Epoch [574/1000], Loss: 0.0473, Val Loss: 0.4479\n",
      "Epoch [575/1000], Loss: 0.0010, Val Loss: 0.4479\n",
      "Epoch [576/1000], Loss: 0.6103, Val Loss: 0.4479\n",
      "Epoch [577/1000], Loss: 0.0058, Val Loss: 0.4479\n",
      "Epoch [578/1000], Loss: 0.0153, Val Loss: 0.4479\n",
      "Epoch [579/1000], Loss: 1.2479, Val Loss: 0.4479\n",
      "Epoch [580/1000], Loss: 1.0111, Val Loss: 0.4479\n",
      "Epoch [581/1000], Loss: 0.0007, Val Loss: 0.4479\n",
      "Epoch [582/1000], Loss: 1.1772, Val Loss: 0.4479\n",
      "Epoch [583/1000], Loss: 0.4293, Val Loss: 0.4479\n",
      "Epoch [584/1000], Loss: 0.2849, Val Loss: 0.4479\n",
      "Epoch [585/1000], Loss: 0.0662, Val Loss: 0.4479\n",
      "Epoch [586/1000], Loss: 0.3891, Val Loss: 0.4478\n",
      "Epoch [587/1000], Loss: 0.2489, Val Loss: 0.4478\n",
      "Epoch [588/1000], Loss: 0.9248, Val Loss: 0.4478\n",
      "Epoch [589/1000], Loss: 0.2797, Val Loss: 0.4478\n",
      "Epoch [590/1000], Loss: 0.5046, Val Loss: 0.4478\n",
      "Epoch [591/1000], Loss: 0.6915, Val Loss: 0.4478\n",
      "Epoch [592/1000], Loss: 0.0802, Val Loss: 0.4477\n",
      "Epoch [593/1000], Loss: 0.5988, Val Loss: 0.4477\n",
      "Epoch [594/1000], Loss: 0.0156, Val Loss: 0.4477\n",
      "Epoch [595/1000], Loss: 0.1895, Val Loss: 0.4477\n",
      "Epoch [596/1000], Loss: 0.2617, Val Loss: 0.4477\n",
      "Epoch [597/1000], Loss: 0.2179, Val Loss: 0.4477\n",
      "Epoch [598/1000], Loss: 0.2340, Val Loss: 0.4477\n",
      "Epoch [599/1000], Loss: 0.0529, Val Loss: 0.4477\n",
      "Epoch [600/1000], Loss: 0.5484, Val Loss: 0.4477\n",
      "Epoch [601/1000], Loss: 0.6014, Val Loss: 0.4477\n",
      "Epoch [602/1000], Loss: 1.4355, Val Loss: 0.4476\n",
      "Epoch [603/1000], Loss: 0.0035, Val Loss: 0.4476\n",
      "Epoch [604/1000], Loss: 0.2022, Val Loss: 0.4476\n",
      "Epoch [605/1000], Loss: 0.6469, Val Loss: 0.4476\n",
      "Epoch [606/1000], Loss: 0.3371, Val Loss: 0.4476\n",
      "Epoch [607/1000], Loss: 1.1431, Val Loss: 0.4475\n",
      "Epoch [608/1000], Loss: 0.3933, Val Loss: 0.4475\n",
      "Epoch [609/1000], Loss: 0.5590, Val Loss: 0.4475\n",
      "Epoch [610/1000], Loss: 0.4495, Val Loss: 0.4475\n",
      "Epoch [611/1000], Loss: 0.0459, Val Loss: 0.4475\n",
      "Epoch [612/1000], Loss: 0.0054, Val Loss: 0.4475\n",
      "Epoch [613/1000], Loss: 0.7274, Val Loss: 0.4475\n",
      "Epoch [614/1000], Loss: 1.1085, Val Loss: 0.4475\n",
      "Epoch [615/1000], Loss: 0.5828, Val Loss: 0.4475\n",
      "Epoch [616/1000], Loss: 0.2647, Val Loss: 0.4475\n",
      "Epoch [617/1000], Loss: 0.5320, Val Loss: 0.4475\n",
      "Epoch [618/1000], Loss: 1.8397, Val Loss: 0.4474\n",
      "Epoch [619/1000], Loss: 0.4913, Val Loss: 0.4474\n",
      "Epoch [620/1000], Loss: 0.0181, Val Loss: 0.4474\n",
      "Epoch [621/1000], Loss: 1.0250, Val Loss: 0.4474\n",
      "Epoch [622/1000], Loss: 0.7053, Val Loss: 0.4474\n",
      "Epoch [623/1000], Loss: 0.7063, Val Loss: 0.4474\n",
      "Epoch [624/1000], Loss: 0.4229, Val Loss: 0.4474\n",
      "Epoch [625/1000], Loss: 0.2712, Val Loss: 0.4474\n",
      "Epoch [626/1000], Loss: 0.0065, Val Loss: 0.4473\n",
      "Epoch [627/1000], Loss: 0.0168, Val Loss: 0.4473\n",
      "Epoch [628/1000], Loss: 0.0862, Val Loss: 0.4473\n",
      "Epoch [629/1000], Loss: 0.6822, Val Loss: 0.4473\n",
      "Epoch [630/1000], Loss: 0.0056, Val Loss: 0.4473\n",
      "Epoch [631/1000], Loss: 0.1526, Val Loss: 0.4473\n",
      "Epoch [632/1000], Loss: 0.9822, Val Loss: 0.4472\n",
      "Epoch [633/1000], Loss: 0.0015, Val Loss: 0.4472\n",
      "Epoch [634/1000], Loss: 0.2551, Val Loss: 0.4472\n",
      "Epoch [635/1000], Loss: 0.7329, Val Loss: 0.4473\n",
      "Epoch [636/1000], Loss: 0.1359, Val Loss: 0.4473\n",
      "Epoch [637/1000], Loss: 0.5741, Val Loss: 0.4472\n",
      "Epoch [638/1000], Loss: 0.8613, Val Loss: 0.4472\n",
      "Epoch [639/1000], Loss: 0.5141, Val Loss: 0.4472\n",
      "Epoch [640/1000], Loss: 0.0056, Val Loss: 0.4471\n",
      "Epoch [641/1000], Loss: 0.8772, Val Loss: 0.4471\n",
      "Epoch [642/1000], Loss: 0.1574, Val Loss: 0.4471\n",
      "Epoch [643/1000], Loss: 0.1130, Val Loss: 0.4471\n",
      "Epoch [644/1000], Loss: 0.1838, Val Loss: 0.4471\n",
      "Epoch [645/1000], Loss: 0.0357, Val Loss: 0.4471\n",
      "Epoch [646/1000], Loss: 0.5357, Val Loss: 0.4471\n",
      "Epoch [647/1000], Loss: 0.9244, Val Loss: 0.4471\n",
      "Epoch [648/1000], Loss: 0.0263, Val Loss: 0.4471\n",
      "Epoch [649/1000], Loss: 0.4883, Val Loss: 0.4471\n",
      "Epoch [650/1000], Loss: 0.3298, Val Loss: 0.4471\n",
      "Epoch [651/1000], Loss: 0.7373, Val Loss: 0.4471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:14:06,697] Trial 19 finished with value: 0.8240159668021024 and parameters: {'hidden_size': 38, 'dropout_prob': 0.6332309344490077, 'learning_rate': 2.2408720229007666e-05, 'weight_decay': 9.95509226926929e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 652\n",
      "Epoch [1/1000], Loss: 0.7658, Val Loss: 0.7060\n",
      "Epoch [2/1000], Loss: 0.6881, Val Loss: 0.6689\n",
      "Epoch [3/1000], Loss: 0.3664, Val Loss: 0.6200\n",
      "Epoch [4/1000], Loss: 0.2743, Val Loss: 0.5744\n",
      "Epoch [5/1000], Loss: 0.4841, Val Loss: 0.5480\n",
      "Epoch [6/1000], Loss: 0.4790, Val Loss: 0.5343\n",
      "Epoch [7/1000], Loss: 1.3698, Val Loss: 0.5251\n",
      "Epoch [8/1000], Loss: 0.4778, Val Loss: 0.5194\n",
      "Epoch [9/1000], Loss: 0.0503, Val Loss: 0.5140\n",
      "Epoch [10/1000], Loss: 0.2082, Val Loss: 0.5086\n",
      "Epoch [11/1000], Loss: 0.0574, Val Loss: 0.5043\n",
      "Epoch [12/1000], Loss: 0.6147, Val Loss: 0.5009\n",
      "Epoch [13/1000], Loss: 0.1882, Val Loss: 0.4975\n",
      "Epoch [14/1000], Loss: 0.2510, Val Loss: 0.4945\n",
      "Epoch [15/1000], Loss: 0.8824, Val Loss: 0.4913\n",
      "Epoch [16/1000], Loss: 0.1993, Val Loss: 0.4890\n",
      "Epoch [17/1000], Loss: 0.7764, Val Loss: 0.4863\n",
      "Epoch [18/1000], Loss: 0.0946, Val Loss: 0.4840\n",
      "Epoch [19/1000], Loss: 0.1395, Val Loss: 0.4824\n",
      "Epoch [20/1000], Loss: 0.8118, Val Loss: 0.4804\n",
      "Epoch [21/1000], Loss: 0.4046, Val Loss: 0.4788\n",
      "Epoch [22/1000], Loss: 0.8493, Val Loss: 0.4776\n",
      "Epoch [23/1000], Loss: 0.7441, Val Loss: 0.4760\n",
      "Epoch [24/1000], Loss: 0.0143, Val Loss: 0.4748\n",
      "Epoch [25/1000], Loss: 0.0844, Val Loss: 0.4737\n",
      "Epoch [26/1000], Loss: 0.0528, Val Loss: 0.4723\n",
      "Epoch [27/1000], Loss: 0.0382, Val Loss: 0.4715\n",
      "Epoch [28/1000], Loss: 0.4579, Val Loss: 0.4706\n",
      "Epoch [29/1000], Loss: 0.6931, Val Loss: 0.4698\n",
      "Epoch [30/1000], Loss: 0.7355, Val Loss: 0.4693\n",
      "Epoch [31/1000], Loss: 0.6020, Val Loss: 0.4688\n",
      "Epoch [32/1000], Loss: 0.0128, Val Loss: 0.4682\n",
      "Epoch [33/1000], Loss: 0.2420, Val Loss: 0.4674\n",
      "Epoch [34/1000], Loss: 0.0060, Val Loss: 0.4672\n",
      "Epoch [35/1000], Loss: 0.7789, Val Loss: 0.4669\n",
      "Epoch [36/1000], Loss: 0.5084, Val Loss: 0.4664\n",
      "Epoch [37/1000], Loss: 0.0271, Val Loss: 0.4664\n",
      "Epoch [38/1000], Loss: 0.0909, Val Loss: 0.4656\n",
      "Epoch [39/1000], Loss: 0.5563, Val Loss: 0.4651\n",
      "Epoch [40/1000], Loss: 0.0277, Val Loss: 0.4648\n",
      "Epoch [41/1000], Loss: 0.4894, Val Loss: 0.4647\n",
      "Epoch [42/1000], Loss: 0.1477, Val Loss: 0.4644\n",
      "Epoch [43/1000], Loss: 0.0014, Val Loss: 0.4641\n",
      "Epoch [44/1000], Loss: 0.0913, Val Loss: 0.4639\n",
      "Epoch [45/1000], Loss: 0.3760, Val Loss: 0.4637\n",
      "Epoch [46/1000], Loss: 0.9100, Val Loss: 0.4633\n",
      "Epoch [47/1000], Loss: 0.3858, Val Loss: 0.4633\n",
      "Epoch [48/1000], Loss: 0.1923, Val Loss: 0.4631\n",
      "Epoch [49/1000], Loss: 0.0562, Val Loss: 0.4629\n",
      "Epoch [50/1000], Loss: 0.0147, Val Loss: 0.4630\n",
      "Epoch [51/1000], Loss: 0.8542, Val Loss: 0.4627\n",
      "Epoch [52/1000], Loss: 0.6486, Val Loss: 0.4627\n",
      "Epoch [53/1000], Loss: 0.4917, Val Loss: 0.4620\n",
      "Epoch [54/1000], Loss: 2.5473, Val Loss: 0.4618\n",
      "Epoch [55/1000], Loss: 0.0423, Val Loss: 0.4620\n",
      "Epoch [56/1000], Loss: 0.5687, Val Loss: 0.4618\n",
      "Epoch [57/1000], Loss: 0.2670, Val Loss: 0.4615\n",
      "Epoch [58/1000], Loss: 0.0201, Val Loss: 0.4614\n",
      "Epoch [59/1000], Loss: 0.7775, Val Loss: 0.4612\n",
      "Epoch [60/1000], Loss: 0.3425, Val Loss: 0.4610\n",
      "Epoch [61/1000], Loss: 1.4225, Val Loss: 0.4607\n",
      "Epoch [62/1000], Loss: 0.0037, Val Loss: 0.4610\n",
      "Epoch [63/1000], Loss: 0.2931, Val Loss: 0.4610\n",
      "Epoch [64/1000], Loss: 0.6450, Val Loss: 0.4607\n",
      "Epoch [65/1000], Loss: 0.6749, Val Loss: 0.4606\n",
      "Epoch [66/1000], Loss: 0.1322, Val Loss: 0.4606\n",
      "Epoch [67/1000], Loss: 0.4712, Val Loss: 0.4601\n",
      "Epoch [68/1000], Loss: 0.6179, Val Loss: 0.4599\n",
      "Epoch [69/1000], Loss: 0.4847, Val Loss: 0.4600\n",
      "Epoch [70/1000], Loss: 1.2794, Val Loss: 0.4598\n",
      "Epoch [71/1000], Loss: 0.1599, Val Loss: 0.4602\n",
      "Epoch [72/1000], Loss: 0.6684, Val Loss: 0.4600\n",
      "Epoch [73/1000], Loss: 0.3305, Val Loss: 0.4597\n",
      "Epoch [74/1000], Loss: 0.0446, Val Loss: 0.4598\n",
      "Epoch [75/1000], Loss: 0.0589, Val Loss: 0.4599\n",
      "Epoch [76/1000], Loss: 0.6203, Val Loss: 0.4597\n",
      "Epoch [77/1000], Loss: 1.0839, Val Loss: 0.4594\n",
      "Epoch [78/1000], Loss: 0.0117, Val Loss: 0.4597\n",
      "Epoch [79/1000], Loss: 0.7498, Val Loss: 0.4596\n",
      "Epoch [80/1000], Loss: 0.8093, Val Loss: 0.4597\n",
      "Epoch [81/1000], Loss: 0.5332, Val Loss: 0.4596\n",
      "Epoch [82/1000], Loss: 2.3018, Val Loss: 0.4594\n",
      "Epoch [83/1000], Loss: 0.0363, Val Loss: 0.4596\n",
      "Epoch [84/1000], Loss: 0.7778, Val Loss: 0.4595\n",
      "Epoch [85/1000], Loss: 0.2338, Val Loss: 0.4592\n",
      "Epoch [86/1000], Loss: 0.6039, Val Loss: 0.4589\n",
      "Epoch [87/1000], Loss: 0.3161, Val Loss: 0.4586\n",
      "Epoch [88/1000], Loss: 0.7961, Val Loss: 0.4584\n",
      "Epoch [89/1000], Loss: 0.7379, Val Loss: 0.4584\n",
      "Epoch [90/1000], Loss: 2.6317, Val Loss: 0.4588\n",
      "Epoch [91/1000], Loss: 0.4128, Val Loss: 0.4586\n",
      "Epoch [92/1000], Loss: 0.0775, Val Loss: 0.4586\n",
      "Epoch [93/1000], Loss: 0.6325, Val Loss: 0.4583\n",
      "Epoch [94/1000], Loss: 0.2043, Val Loss: 0.4581\n",
      "Epoch [95/1000], Loss: 0.1704, Val Loss: 0.4577\n",
      "Epoch [96/1000], Loss: 0.8028, Val Loss: 0.4576\n",
      "Epoch [97/1000], Loss: 0.2795, Val Loss: 0.4576\n",
      "Epoch [98/1000], Loss: 1.4057, Val Loss: 0.4577\n",
      "Epoch [99/1000], Loss: 0.1665, Val Loss: 0.4579\n",
      "Epoch [100/1000], Loss: 0.0033, Val Loss: 0.4577\n",
      "Epoch [101/1000], Loss: 0.0059, Val Loss: 0.4576\n",
      "Epoch [102/1000], Loss: 0.6202, Val Loss: 0.4573\n",
      "Epoch [103/1000], Loss: 0.0762, Val Loss: 0.4573\n",
      "Epoch [104/1000], Loss: 0.0456, Val Loss: 0.4571\n",
      "Epoch [105/1000], Loss: 0.1807, Val Loss: 0.4570\n",
      "Epoch [106/1000], Loss: 0.0041, Val Loss: 0.4570\n",
      "Epoch [107/1000], Loss: 0.4124, Val Loss: 0.4571\n",
      "Epoch [108/1000], Loss: 0.4875, Val Loss: 0.4571\n",
      "Epoch [109/1000], Loss: 0.0569, Val Loss: 0.4568\n",
      "Epoch [110/1000], Loss: 0.0092, Val Loss: 0.4566\n",
      "Epoch [111/1000], Loss: 0.3514, Val Loss: 0.4567\n",
      "Epoch [112/1000], Loss: 0.0037, Val Loss: 0.4567\n",
      "Epoch [113/1000], Loss: 0.0104, Val Loss: 0.4566\n",
      "Epoch [114/1000], Loss: 0.1314, Val Loss: 0.4565\n",
      "Epoch [115/1000], Loss: 0.3743, Val Loss: 0.4564\n",
      "Epoch [116/1000], Loss: 0.2525, Val Loss: 0.4564\n",
      "Epoch [117/1000], Loss: 0.4506, Val Loss: 0.4562\n",
      "Epoch [118/1000], Loss: 0.4479, Val Loss: 0.4562\n",
      "Epoch [119/1000], Loss: 0.1818, Val Loss: 0.4563\n",
      "Epoch [120/1000], Loss: 0.2239, Val Loss: 0.4559\n",
      "Epoch [121/1000], Loss: 0.0027, Val Loss: 0.4557\n",
      "Epoch [122/1000], Loss: 0.2728, Val Loss: 0.4556\n",
      "Epoch [123/1000], Loss: 0.1997, Val Loss: 0.4555\n",
      "Epoch [124/1000], Loss: 0.6823, Val Loss: 0.4555\n",
      "Epoch [125/1000], Loss: 0.1915, Val Loss: 0.4553\n",
      "Epoch [126/1000], Loss: 0.4359, Val Loss: 0.4552\n",
      "Epoch [127/1000], Loss: 0.8804, Val Loss: 0.4553\n",
      "Epoch [128/1000], Loss: 1.1645, Val Loss: 0.4554\n",
      "Epoch [129/1000], Loss: 0.5306, Val Loss: 0.4554\n",
      "Epoch [130/1000], Loss: 0.2063, Val Loss: 0.4550\n",
      "Epoch [131/1000], Loss: 0.6281, Val Loss: 0.4549\n",
      "Epoch [132/1000], Loss: 0.7998, Val Loss: 0.4549\n",
      "Epoch [133/1000], Loss: 0.0688, Val Loss: 0.4548\n",
      "Epoch [134/1000], Loss: 1.2796, Val Loss: 0.4549\n",
      "Epoch [135/1000], Loss: 0.1103, Val Loss: 0.4551\n",
      "Epoch [136/1000], Loss: 0.0093, Val Loss: 0.4550\n",
      "Epoch [137/1000], Loss: 0.0072, Val Loss: 0.4551\n",
      "Epoch [138/1000], Loss: 1.3373, Val Loss: 0.4550\n",
      "Epoch [139/1000], Loss: 0.7201, Val Loss: 0.4551\n",
      "Epoch [140/1000], Loss: 0.0033, Val Loss: 0.4551\n",
      "Epoch [141/1000], Loss: 0.8097, Val Loss: 0.4548\n",
      "Epoch [142/1000], Loss: 0.0034, Val Loss: 0.4546\n",
      "Epoch [143/1000], Loss: 0.3609, Val Loss: 0.4545\n",
      "Epoch [144/1000], Loss: 0.7966, Val Loss: 0.4543\n",
      "Epoch [145/1000], Loss: 0.8141, Val Loss: 0.4543\n",
      "Epoch [146/1000], Loss: 0.0004, Val Loss: 0.4542\n",
      "Epoch [147/1000], Loss: 0.3840, Val Loss: 0.4541\n",
      "Epoch [148/1000], Loss: 0.6565, Val Loss: 0.4542\n",
      "Epoch [149/1000], Loss: 0.2131, Val Loss: 0.4541\n",
      "Epoch [150/1000], Loss: 1.8447, Val Loss: 0.4541\n",
      "Epoch [151/1000], Loss: 0.4045, Val Loss: 0.4539\n",
      "Epoch [152/1000], Loss: 0.4670, Val Loss: 0.4541\n",
      "Epoch [153/1000], Loss: 1.4786, Val Loss: 0.4543\n",
      "Epoch [154/1000], Loss: 1.3733, Val Loss: 0.4546\n",
      "Epoch [155/1000], Loss: 0.4433, Val Loss: 0.4543\n",
      "Epoch [156/1000], Loss: 0.1198, Val Loss: 0.4540\n",
      "Epoch [157/1000], Loss: 0.1815, Val Loss: 0.4538\n",
      "Epoch [158/1000], Loss: 0.3906, Val Loss: 0.4538\n",
      "Epoch [159/1000], Loss: 0.0006, Val Loss: 0.4538\n",
      "Epoch [160/1000], Loss: 0.2458, Val Loss: 0.4538\n",
      "Epoch [161/1000], Loss: 0.1795, Val Loss: 0.4540\n",
      "Epoch [162/1000], Loss: 0.3300, Val Loss: 0.4541\n",
      "Epoch [163/1000], Loss: 0.0006, Val Loss: 0.4538\n",
      "Epoch [164/1000], Loss: 0.7288, Val Loss: 0.4539\n",
      "Epoch [165/1000], Loss: 0.5060, Val Loss: 0.4540\n",
      "Epoch [166/1000], Loss: 0.1578, Val Loss: 0.4539\n",
      "Epoch [167/1000], Loss: 0.8480, Val Loss: 0.4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:14:55,257] Trial 20 finished with value: 0.8195361368699562 and parameters: {'hidden_size': 41, 'dropout_prob': 0.6381782923919525, 'learning_rate': 7.367382472709529e-05, 'weight_decay': 9.836691922533448e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 168\n",
      "Epoch [1/1000], Loss: 0.1283, Val Loss: 0.4752\n",
      "Epoch [2/1000], Loss: 0.1894, Val Loss: 0.4672\n",
      "Epoch [3/1000], Loss: 0.0113, Val Loss: 0.4590\n",
      "Epoch [4/1000], Loss: 0.4776, Val Loss: 0.4546\n",
      "Epoch [5/1000], Loss: 0.0875, Val Loss: 0.4543\n",
      "Epoch [6/1000], Loss: 0.3289, Val Loss: 0.4539\n",
      "Epoch [7/1000], Loss: 2.2001, Val Loss: 0.4501\n",
      "Epoch [8/1000], Loss: 0.2297, Val Loss: 0.4519\n",
      "Epoch [9/1000], Loss: 0.4224, Val Loss: 0.4480\n",
      "Epoch [10/1000], Loss: 0.9881, Val Loss: 0.4487\n",
      "Epoch [11/1000], Loss: 0.5388, Val Loss: 0.4487\n",
      "Epoch [12/1000], Loss: 0.4559, Val Loss: 0.4531\n",
      "Epoch [13/1000], Loss: 0.5145, Val Loss: 0.4550\n",
      "Epoch [14/1000], Loss: 0.0932, Val Loss: 0.4541\n",
      "Epoch [15/1000], Loss: 0.3210, Val Loss: 0.4527\n",
      "Epoch [16/1000], Loss: 0.0586, Val Loss: 0.4534\n",
      "Epoch [17/1000], Loss: 0.3109, Val Loss: 0.4493\n",
      "Epoch [18/1000], Loss: 0.0901, Val Loss: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:15:01,077] Trial 21 finished with value: 0.8196572133546088 and parameters: {'hidden_size': 33, 'dropout_prob': 0.6122567620624586, 'learning_rate': 0.0021687376509674566, 'weight_decay': 8.620967826857434e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 19\n",
      "Epoch [1/1000], Loss: 0.0795, Val Loss: 0.4851\n",
      "Epoch [2/1000], Loss: 0.4551, Val Loss: 0.4684\n",
      "Epoch [3/1000], Loss: 0.9342, Val Loss: 0.4585\n",
      "Epoch [4/1000], Loss: 0.3182, Val Loss: 0.4529\n",
      "Epoch [5/1000], Loss: 0.9179, Val Loss: 0.4507\n",
      "Epoch [6/1000], Loss: 0.0876, Val Loss: 0.4508\n",
      "Epoch [7/1000], Loss: 0.6328, Val Loss: 0.4490\n",
      "Epoch [8/1000], Loss: 0.4356, Val Loss: 0.4489\n",
      "Epoch [9/1000], Loss: 0.0182, Val Loss: 0.4491\n",
      "Epoch [10/1000], Loss: 0.1157, Val Loss: 0.4474\n",
      "Epoch [11/1000], Loss: 0.9690, Val Loss: 0.4474\n",
      "Epoch [12/1000], Loss: 1.1433, Val Loss: 0.4488\n",
      "Epoch [13/1000], Loss: 0.0033, Val Loss: 0.4483\n",
      "Epoch [14/1000], Loss: 0.0437, Val Loss: 0.4477\n",
      "Epoch [15/1000], Loss: 0.3363, Val Loss: 0.4478\n",
      "Epoch [16/1000], Loss: 0.3287, Val Loss: 0.4482\n",
      "Epoch [17/1000], Loss: 0.3536, Val Loss: 0.4467\n",
      "Epoch [18/1000], Loss: 2.5096, Val Loss: 0.4464\n",
      "Epoch [19/1000], Loss: 0.1106, Val Loss: 0.4465\n",
      "Epoch [20/1000], Loss: 0.0375, Val Loss: 0.4469\n",
      "Epoch [21/1000], Loss: 0.0482, Val Loss: 0.4473\n",
      "Epoch [22/1000], Loss: 0.9903, Val Loss: 0.4465\n",
      "Epoch [23/1000], Loss: 0.0230, Val Loss: 0.4471\n",
      "Epoch [24/1000], Loss: 0.5822, Val Loss: 0.4468\n",
      "Epoch [25/1000], Loss: 0.1701, Val Loss: 0.4459\n",
      "Epoch [26/1000], Loss: 0.1226, Val Loss: 0.4466\n",
      "Epoch [27/1000], Loss: 0.0111, Val Loss: 0.4464\n",
      "Epoch [28/1000], Loss: 0.2149, Val Loss: 0.4486\n",
      "Epoch [29/1000], Loss: 0.5018, Val Loss: 0.4462\n",
      "Epoch [30/1000], Loss: 0.1954, Val Loss: 0.4445\n",
      "Epoch [31/1000], Loss: 0.0173, Val Loss: 0.4467\n",
      "Epoch [32/1000], Loss: 1.8493, Val Loss: 0.4485\n",
      "Epoch [33/1000], Loss: 0.0018, Val Loss: 0.4464\n",
      "Epoch [34/1000], Loss: 0.3896, Val Loss: 0.4477\n",
      "Epoch [35/1000], Loss: 0.1931, Val Loss: 0.4470\n",
      "Epoch [36/1000], Loss: 0.0338, Val Loss: 0.4470\n",
      "Epoch [37/1000], Loss: 0.0777, Val Loss: 0.4501\n",
      "Epoch [38/1000], Loss: 0.1025, Val Loss: 0.4497\n",
      "Epoch [39/1000], Loss: 0.0113, Val Loss: 0.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:15:12,704] Trial 22 finished with value: 0.8228944162074256 and parameters: {'hidden_size': 80, 'dropout_prob': 0.6667854035333941, 'learning_rate': 0.0009214723327594473, 'weight_decay': 8.858810488145207e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 40\n",
      "Epoch [1/1000], Loss: 1.8952, Val Loss: 0.5165\n",
      "Epoch [2/1000], Loss: 0.2639, Val Loss: 0.4677\n",
      "Epoch [3/1000], Loss: 0.6531, Val Loss: 0.4624\n",
      "Epoch [4/1000], Loss: 0.2290, Val Loss: 0.4566\n",
      "Epoch [5/1000], Loss: 1.0221, Val Loss: 0.4526\n",
      "Epoch [6/1000], Loss: 0.6456, Val Loss: 0.4514\n",
      "Epoch [7/1000], Loss: 0.8339, Val Loss: 0.4516\n",
      "Epoch [8/1000], Loss: 1.5937, Val Loss: 0.4520\n",
      "Epoch [9/1000], Loss: 0.0372, Val Loss: 0.4488\n",
      "Epoch [10/1000], Loss: 1.2668, Val Loss: 0.4486\n",
      "Epoch [11/1000], Loss: 0.7436, Val Loss: 0.4487\n",
      "Epoch [12/1000], Loss: 0.4451, Val Loss: 0.4490\n",
      "Epoch [13/1000], Loss: 0.1686, Val Loss: 0.4489\n",
      "Epoch [14/1000], Loss: 0.2652, Val Loss: 0.4491\n",
      "Epoch [15/1000], Loss: 0.5059, Val Loss: 0.4497\n",
      "Epoch [16/1000], Loss: 0.5184, Val Loss: 0.4479\n",
      "Epoch [17/1000], Loss: 3.8249, Val Loss: 0.4487\n",
      "Epoch [18/1000], Loss: 0.0414, Val Loss: 0.4497\n",
      "Epoch [19/1000], Loss: 0.0094, Val Loss: 0.4483\n",
      "Epoch [20/1000], Loss: 0.6993, Val Loss: 0.4469\n",
      "Epoch [21/1000], Loss: 0.0014, Val Loss: 0.4490\n",
      "Epoch [22/1000], Loss: 0.0453, Val Loss: 0.4482\n",
      "Epoch [23/1000], Loss: 0.5736, Val Loss: 0.4496\n",
      "Epoch [24/1000], Loss: 0.2003, Val Loss: 0.4489\n",
      "Epoch [25/1000], Loss: 0.7936, Val Loss: 0.4491\n",
      "Epoch [26/1000], Loss: 0.1329, Val Loss: 0.4505\n",
      "Epoch [27/1000], Loss: 0.1646, Val Loss: 0.4501\n",
      "Epoch [28/1000], Loss: 1.0492, Val Loss: 0.4503\n",
      "Epoch [29/1000], Loss: 0.1750, Val Loss: 0.4481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:15:20,893] Trial 23 finished with value: 0.8219691369668175 and parameters: {'hidden_size': 37, 'dropout_prob': 0.7010080179689446, 'learning_rate': 0.00106170559841212, 'weight_decay': 8.784571569560743e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 30\n",
      "Epoch [1/1000], Loss: 1.0256, Val Loss: 0.4850\n",
      "Epoch [2/1000], Loss: 0.5648, Val Loss: 0.4699\n",
      "Epoch [3/1000], Loss: 0.2001, Val Loss: 0.4629\n",
      "Epoch [4/1000], Loss: 3.3416, Val Loss: 0.4594\n",
      "Epoch [5/1000], Loss: 1.2205, Val Loss: 0.4567\n",
      "Epoch [6/1000], Loss: 0.8136, Val Loss: 0.4574\n",
      "Epoch [7/1000], Loss: 0.3896, Val Loss: 0.4547\n",
      "Epoch [8/1000], Loss: 0.0089, Val Loss: 0.4539\n",
      "Epoch [9/1000], Loss: 0.0135, Val Loss: 0.4509\n",
      "Epoch [10/1000], Loss: 0.6803, Val Loss: 0.4519\n",
      "Epoch [11/1000], Loss: 0.7987, Val Loss: 0.4497\n",
      "Epoch [12/1000], Loss: 0.0108, Val Loss: 0.4504\n",
      "Epoch [13/1000], Loss: 0.3583, Val Loss: 0.4497\n",
      "Epoch [14/1000], Loss: 0.1049, Val Loss: 0.4509\n",
      "Epoch [15/1000], Loss: 0.1784, Val Loss: 0.4496\n",
      "Epoch [16/1000], Loss: 0.5949, Val Loss: 0.4501\n",
      "Epoch [17/1000], Loss: 0.1425, Val Loss: 0.4498\n",
      "Epoch [18/1000], Loss: 0.4346, Val Loss: 0.4506\n",
      "Epoch [19/1000], Loss: 1.1218, Val Loss: 0.4511\n",
      "Epoch [20/1000], Loss: 0.4603, Val Loss: 0.4503\n",
      "Epoch [21/1000], Loss: 0.5878, Val Loss: 0.4496\n",
      "Epoch [22/1000], Loss: 0.5710, Val Loss: 0.4516\n",
      "Epoch [23/1000], Loss: 0.0062, Val Loss: 0.4481\n",
      "Epoch [24/1000], Loss: 0.3304, Val Loss: 0.4480\n",
      "Epoch [25/1000], Loss: 0.0337, Val Loss: 0.4476\n",
      "Epoch [26/1000], Loss: 0.1145, Val Loss: 0.4488\n",
      "Epoch [27/1000], Loss: 0.9134, Val Loss: 0.4474\n",
      "Epoch [28/1000], Loss: 0.3588, Val Loss: 0.4492\n",
      "Epoch [29/1000], Loss: 0.8305, Val Loss: 0.4501\n",
      "Epoch [30/1000], Loss: 0.4616, Val Loss: 0.4475\n",
      "Epoch [31/1000], Loss: 0.1435, Val Loss: 0.4477\n",
      "Epoch [32/1000], Loss: 1.3071, Val Loss: 0.4477\n",
      "Epoch [33/1000], Loss: 0.6172, Val Loss: 0.4486\n",
      "Epoch [34/1000], Loss: 0.0024, Val Loss: 0.4511\n",
      "Epoch [35/1000], Loss: 1.1914, Val Loss: 0.4496\n",
      "Epoch [36/1000], Loss: 0.0448, Val Loss: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:15:30,767] Trial 24 finished with value: 0.8212605209092716 and parameters: {'hidden_size': 89, 'dropout_prob': 0.7757321705115664, 'learning_rate': 0.0010654858581604853, 'weight_decay': 8.781571776959472e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 37\n",
      "Epoch [1/1000], Loss: 0.8992, Val Loss: 0.4960\n",
      "Epoch [2/1000], Loss: 0.1891, Val Loss: 0.4726\n",
      "Epoch [3/1000], Loss: 0.0494, Val Loss: 0.4811\n",
      "Epoch [4/1000], Loss: 0.9408, Val Loss: 0.4611\n",
      "Epoch [5/1000], Loss: 0.1000, Val Loss: 0.4613\n",
      "Epoch [6/1000], Loss: 0.1570, Val Loss: 0.4618\n",
      "Epoch [7/1000], Loss: 0.5464, Val Loss: 0.4605\n",
      "Epoch [8/1000], Loss: 0.6919, Val Loss: 0.4693\n",
      "Epoch [9/1000], Loss: 1.9257, Val Loss: 0.4577\n",
      "Epoch [10/1000], Loss: 0.1858, Val Loss: 0.4563\n",
      "Epoch [11/1000], Loss: 0.2625, Val Loss: 0.4541\n",
      "Epoch [12/1000], Loss: 0.0443, Val Loss: 0.4742\n",
      "Epoch [13/1000], Loss: 0.0534, Val Loss: 0.4639\n",
      "Epoch [14/1000], Loss: 0.0936, Val Loss: 0.4685\n",
      "Epoch [15/1000], Loss: 0.5826, Val Loss: 0.4608\n",
      "Epoch [16/1000], Loss: 0.2182, Val Loss: 0.4590\n",
      "Epoch [17/1000], Loss: 0.1696, Val Loss: 0.4577\n",
      "Epoch [18/1000], Loss: 0.8846, Val Loss: 0.4649\n",
      "Epoch [19/1000], Loss: 0.0744, Val Loss: 0.4580\n",
      "Epoch [20/1000], Loss: 0.0175, Val Loss: 0.4644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:15:36,464] Trial 25 finished with value: 0.8231722548774706 and parameters: {'hidden_size': 18, 'dropout_prob': 0.7148173085419363, 'learning_rate': 0.0017958358217532368, 'weight_decay': 8.826894197784329e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 21\n",
      "Epoch [1/1000], Loss: 0.5129, Val Loss: 0.5080\n",
      "Epoch [2/1000], Loss: 0.3900, Val Loss: 0.4746\n",
      "Epoch [3/1000], Loss: 0.2764, Val Loss: 0.4636\n",
      "Epoch [4/1000], Loss: 0.6591, Val Loss: 0.4622\n",
      "Epoch [5/1000], Loss: 0.8269, Val Loss: 0.4586\n",
      "Epoch [6/1000], Loss: 1.0472, Val Loss: 0.4597\n",
      "Epoch [7/1000], Loss: 0.1378, Val Loss: 0.4578\n",
      "Epoch [8/1000], Loss: 1.6047, Val Loss: 0.4568\n",
      "Epoch [9/1000], Loss: 0.0139, Val Loss: 0.4538\n",
      "Epoch [10/1000], Loss: 0.5034, Val Loss: 0.4509\n",
      "Epoch [11/1000], Loss: 1.1574, Val Loss: 0.4524\n",
      "Epoch [12/1000], Loss: 0.1567, Val Loss: 0.4543\n",
      "Epoch [13/1000], Loss: 0.3677, Val Loss: 0.4556\n",
      "Epoch [14/1000], Loss: 0.4519, Val Loss: 0.4564\n",
      "Epoch [15/1000], Loss: 0.0628, Val Loss: 0.4549\n",
      "Epoch [16/1000], Loss: 0.5577, Val Loss: 0.4550\n",
      "Epoch [17/1000], Loss: 0.2183, Val Loss: 0.4516\n",
      "Epoch [18/1000], Loss: 0.0237, Val Loss: 0.4527\n",
      "Epoch [19/1000], Loss: 0.5500, Val Loss: 0.4527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:15:42,083] Trial 26 finished with value: 0.818084493543437 and parameters: {'hidden_size': 17, 'dropout_prob': 0.6781824335348343, 'learning_rate': 0.002179385906216318, 'weight_decay': 9.092255079739619e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 20\n",
      "Epoch [1/1000], Loss: 0.4057, Val Loss: 0.4777\n",
      "Epoch [2/1000], Loss: 1.7359, Val Loss: 0.4563\n",
      "Epoch [3/1000], Loss: 0.8769, Val Loss: 0.4520\n",
      "Epoch [4/1000], Loss: 0.0030, Val Loss: 0.4585\n",
      "Epoch [5/1000], Loss: 0.0123, Val Loss: 0.4501\n",
      "Epoch [6/1000], Loss: 0.4569, Val Loss: 0.4514\n",
      "Epoch [7/1000], Loss: 0.1585, Val Loss: 0.4493\n",
      "Epoch [8/1000], Loss: 0.7188, Val Loss: 0.4487\n",
      "Epoch [9/1000], Loss: 0.6161, Val Loss: 0.4576\n",
      "Epoch [10/1000], Loss: 0.2659, Val Loss: 0.4495\n",
      "Epoch [11/1000], Loss: 0.2281, Val Loss: 0.4528\n",
      "Epoch [12/1000], Loss: 1.1123, Val Loss: 0.4521\n",
      "Epoch [13/1000], Loss: 0.0348, Val Loss: 0.4508\n",
      "Epoch [14/1000], Loss: 0.6473, Val Loss: 0.4503\n",
      "Epoch [15/1000], Loss: 0.6849, Val Loss: 0.4509\n",
      "Epoch [16/1000], Loss: 1.5797, Val Loss: 0.4494\n",
      "Epoch [17/1000], Loss: 0.1791, Val Loss: 0.4490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:15:47,309] Trial 27 finished with value: 0.8209317026667415 and parameters: {'hidden_size': 97, 'dropout_prob': 0.7602292323748097, 'learning_rate': 0.002882388836963799, 'weight_decay': 8.204101773179926e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 18\n",
      "Epoch [1/1000], Loss: 1.5249, Val Loss: 0.5296\n",
      "Epoch [2/1000], Loss: 0.1485, Val Loss: 0.4979\n",
      "Epoch [3/1000], Loss: 0.0438, Val Loss: 0.4821\n",
      "Epoch [4/1000], Loss: 0.4766, Val Loss: 0.4764\n",
      "Epoch [5/1000], Loss: 0.7199, Val Loss: 0.4665\n",
      "Epoch [6/1000], Loss: 0.0166, Val Loss: 0.4683\n",
      "Epoch [7/1000], Loss: 0.0126, Val Loss: 0.4640\n",
      "Epoch [8/1000], Loss: 0.1287, Val Loss: 0.4641\n",
      "Epoch [9/1000], Loss: 0.5055, Val Loss: 0.4609\n",
      "Epoch [10/1000], Loss: 0.0312, Val Loss: 0.4605\n",
      "Epoch [11/1000], Loss: 0.1785, Val Loss: 0.4594\n",
      "Epoch [12/1000], Loss: 0.6472, Val Loss: 0.4607\n",
      "Epoch [13/1000], Loss: 0.0049, Val Loss: 0.4572\n",
      "Epoch [14/1000], Loss: 0.1210, Val Loss: 0.4623\n",
      "Epoch [15/1000], Loss: 0.3314, Val Loss: 0.4591\n",
      "Epoch [16/1000], Loss: 0.7354, Val Loss: 0.4636\n",
      "Epoch [17/1000], Loss: 0.6122, Val Loss: 0.4556\n",
      "Epoch [18/1000], Loss: 0.4218, Val Loss: 0.4583\n",
      "Epoch [19/1000], Loss: 0.3010, Val Loss: 0.4555\n",
      "Epoch [20/1000], Loss: 0.0168, Val Loss: 0.4554\n",
      "Epoch [21/1000], Loss: 0.5127, Val Loss: 0.4612\n",
      "Epoch [22/1000], Loss: 0.0578, Val Loss: 0.4617\n",
      "Epoch [23/1000], Loss: 0.0577, Val Loss: 0.4582\n",
      "Epoch [24/1000], Loss: 0.3580, Val Loss: 0.4561\n",
      "Epoch [25/1000], Loss: 0.0727, Val Loss: 0.4584\n",
      "Epoch [26/1000], Loss: 1.7294, Val Loss: 0.4631\n",
      "Epoch [27/1000], Loss: 0.6199, Val Loss: 0.4521\n",
      "Epoch [28/1000], Loss: 1.7330, Val Loss: 0.4555\n",
      "Epoch [29/1000], Loss: 0.3784, Val Loss: 0.4527\n",
      "Epoch [30/1000], Loss: 0.1913, Val Loss: 0.4521\n",
      "Epoch [31/1000], Loss: 0.3387, Val Loss: 0.4509\n",
      "Epoch [32/1000], Loss: 0.1887, Val Loss: 0.4567\n",
      "Epoch [33/1000], Loss: 0.5954, Val Loss: 0.4593\n",
      "Epoch [34/1000], Loss: 0.6680, Val Loss: 0.4535\n",
      "Epoch [35/1000], Loss: 0.4482, Val Loss: 0.4558\n",
      "Epoch [36/1000], Loss: 0.0825, Val Loss: 0.4585\n",
      "Epoch [37/1000], Loss: 0.2577, Val Loss: 0.4564\n",
      "Epoch [38/1000], Loss: 0.0049, Val Loss: 0.4536\n",
      "Epoch [39/1000], Loss: 0.7082, Val Loss: 0.4601\n",
      "Epoch [40/1000], Loss: 0.0035, Val Loss: 0.4531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:15:59,603] Trial 28 finished with value: 0.8206946476546848 and parameters: {'hidden_size': 25, 'dropout_prob': 0.7399633415163763, 'learning_rate': 0.0006985751866951373, 'weight_decay': 6.100251395766454e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 41\n",
      "Epoch [1/1000], Loss: 0.8268, Val Loss: 0.4728\n",
      "Epoch [2/1000], Loss: 0.0142, Val Loss: 0.4630\n",
      "Epoch [3/1000], Loss: 0.6044, Val Loss: 0.4579\n",
      "Epoch [4/1000], Loss: 0.8327, Val Loss: 0.4528\n",
      "Epoch [5/1000], Loss: 0.0297, Val Loss: 0.4536\n",
      "Epoch [6/1000], Loss: 0.2704, Val Loss: 0.4498\n",
      "Epoch [7/1000], Loss: 1.3635, Val Loss: 0.4493\n",
      "Epoch [8/1000], Loss: 0.5825, Val Loss: 0.4515\n",
      "Epoch [9/1000], Loss: 0.1230, Val Loss: 0.4489\n",
      "Epoch [10/1000], Loss: 0.4645, Val Loss: 0.4497\n",
      "Epoch [11/1000], Loss: 0.1403, Val Loss: 0.4510\n",
      "Epoch [12/1000], Loss: 0.6694, Val Loss: 0.4484\n",
      "Epoch [13/1000], Loss: 1.0090, Val Loss: 0.4490\n",
      "Epoch [14/1000], Loss: 0.0494, Val Loss: 0.4492\n",
      "Epoch [15/1000], Loss: 0.5900, Val Loss: 0.4487\n",
      "Epoch [16/1000], Loss: 0.3851, Val Loss: 0.4482\n",
      "Epoch [17/1000], Loss: 0.0524, Val Loss: 0.4503\n",
      "Epoch [18/1000], Loss: 0.8192, Val Loss: 0.4481\n",
      "Epoch [19/1000], Loss: 0.1188, Val Loss: 0.4513\n",
      "Epoch [20/1000], Loss: 0.8696, Val Loss: 0.4503\n",
      "Epoch [21/1000], Loss: 0.0375, Val Loss: 0.4511\n",
      "Epoch [22/1000], Loss: 0.7123, Val Loss: 0.4489\n",
      "Epoch [23/1000], Loss: 0.0348, Val Loss: 0.4505\n",
      "Epoch [24/1000], Loss: 1.3325, Val Loss: 0.4479\n",
      "Epoch [25/1000], Loss: 0.1500, Val Loss: 0.4477\n",
      "Epoch [26/1000], Loss: 0.2966, Val Loss: 0.4516\n",
      "Epoch [27/1000], Loss: 0.0544, Val Loss: 0.4489\n",
      "Epoch [28/1000], Loss: 0.3819, Val Loss: 0.4454\n",
      "Epoch [29/1000], Loss: 0.1956, Val Loss: 0.4461\n",
      "Epoch [30/1000], Loss: 0.6421, Val Loss: 0.4488\n",
      "Epoch [31/1000], Loss: 0.1383, Val Loss: 0.4479\n",
      "Epoch [32/1000], Loss: 2.1821, Val Loss: 0.4493\n",
      "Epoch [33/1000], Loss: 0.3813, Val Loss: 0.4510\n",
      "Epoch [34/1000], Loss: 2.0579, Val Loss: 0.4520\n",
      "Epoch [35/1000], Loss: 0.1035, Val Loss: 0.4500\n",
      "Epoch [36/1000], Loss: 0.3282, Val Loss: 0.4500\n",
      "Epoch [37/1000], Loss: 0.0013, Val Loss: 0.4495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:16:10,462] Trial 29 finished with value: 0.8220583512186668 and parameters: {'hidden_size': 80, 'dropout_prob': 0.670434445229374, 'learning_rate': 0.0016345936280886863, 'weight_decay': 7.238057109319098e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 38\n",
      "Epoch [1/1000], Loss: 0.9889, Val Loss: 0.5125\n",
      "Epoch [2/1000], Loss: 0.2257, Val Loss: 0.4947\n",
      "Epoch [3/1000], Loss: 0.1772, Val Loss: 0.4922\n",
      "Epoch [4/1000], Loss: 0.7513, Val Loss: 0.4977\n",
      "Epoch [5/1000], Loss: 1.8092, Val Loss: 0.5248\n",
      "Epoch [6/1000], Loss: 0.3133, Val Loss: 0.4769\n",
      "Epoch [7/1000], Loss: 0.7038, Val Loss: 0.4939\n",
      "Epoch [8/1000], Loss: 0.6745, Val Loss: 0.4986\n",
      "Epoch [9/1000], Loss: 0.7025, Val Loss: 0.5065\n",
      "Epoch [10/1000], Loss: 0.1264, Val Loss: 0.5095\n",
      "Epoch [11/1000], Loss: 0.2119, Val Loss: 0.4993\n",
      "Epoch [12/1000], Loss: 0.8717, Val Loss: 0.5020\n",
      "Epoch [13/1000], Loss: 0.4038, Val Loss: 0.5215\n",
      "Epoch [14/1000], Loss: 0.4495, Val Loss: 0.4724\n",
      "Epoch [15/1000], Loss: 0.2412, Val Loss: 0.5342\n",
      "Epoch [16/1000], Loss: 0.0333, Val Loss: 0.5041\n",
      "Epoch [17/1000], Loss: 1.0143, Val Loss: 0.5035\n",
      "Epoch [18/1000], Loss: 0.6351, Val Loss: 0.5290\n",
      "Epoch [19/1000], Loss: 0.7659, Val Loss: 0.5232\n",
      "Epoch [20/1000], Loss: 0.0278, Val Loss: 0.5078\n",
      "Epoch [21/1000], Loss: 0.6714, Val Loss: 0.5202\n",
      "Epoch [22/1000], Loss: 0.0151, Val Loss: 0.5309\n",
      "Epoch [23/1000], Loss: 0.0053, Val Loss: 0.5354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:16:16,717] Trial 30 finished with value: 0.8171847040890715 and parameters: {'hidden_size': 26, 'dropout_prob': 0.7954612168544207, 'learning_rate': 0.0026693366930037924, 'weight_decay': 9.320410295341944e-05}. Best is trial 19 with value: 0.8240159668021024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 24\n",
      "Epoch [1/1000], Loss: 0.7005, Val Loss: 0.4682\n",
      "Epoch [2/1000], Loss: 0.0118, Val Loss: 0.4584\n",
      "Epoch [3/1000], Loss: 0.4596, Val Loss: 0.4545\n",
      "Epoch [4/1000], Loss: 0.8367, Val Loss: 0.4495\n",
      "Epoch [5/1000], Loss: 0.5088, Val Loss: 0.4488\n",
      "Epoch [6/1000], Loss: 0.0381, Val Loss: 0.4484\n",
      "Epoch [7/1000], Loss: 0.5080, Val Loss: 0.4476\n",
      "Epoch [8/1000], Loss: 0.1595, Val Loss: 0.4482\n",
      "Epoch [9/1000], Loss: 0.3552, Val Loss: 0.4477\n",
      "Epoch [10/1000], Loss: 0.4965, Val Loss: 0.4505\n",
      "Epoch [11/1000], Loss: 1.0966, Val Loss: 0.4503\n",
      "Epoch [12/1000], Loss: 0.0303, Val Loss: 0.4503\n",
      "Epoch [13/1000], Loss: 0.3944, Val Loss: 0.4463\n",
      "Epoch [14/1000], Loss: 3.3309, Val Loss: 0.4494\n",
      "Epoch [15/1000], Loss: 0.2112, Val Loss: 0.4466\n",
      "Epoch [16/1000], Loss: 0.8886, Val Loss: 0.4464\n",
      "Epoch [17/1000], Loss: 1.2103, Val Loss: 0.4481\n",
      "Epoch [18/1000], Loss: 0.1529, Val Loss: 0.4467\n",
      "Epoch [19/1000], Loss: 0.3215, Val Loss: 0.4452\n",
      "Epoch [20/1000], Loss: 0.0519, Val Loss: 0.4457\n",
      "Epoch [21/1000], Loss: 0.6352, Val Loss: 0.4469\n",
      "Epoch [22/1000], Loss: 0.5016, Val Loss: 0.4448\n",
      "Epoch [23/1000], Loss: 0.0386, Val Loss: 0.4462\n",
      "Epoch [24/1000], Loss: 0.1382, Val Loss: 0.4464\n",
      "Epoch [25/1000], Loss: 0.1122, Val Loss: 0.4524\n",
      "Epoch [26/1000], Loss: 0.0308, Val Loss: 0.4494\n",
      "Epoch [27/1000], Loss: 0.0035, Val Loss: 0.4471\n",
      "Epoch [28/1000], Loss: 0.5816, Val Loss: 0.4470\n",
      "Epoch [29/1000], Loss: 0.1367, Val Loss: 0.4468\n",
      "Epoch [30/1000], Loss: 0.0062, Val Loss: 0.4480\n",
      "Epoch [31/1000], Loss: 0.0369, Val Loss: 0.4486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:16:25,543] Trial 31 finished with value: 0.8248367379191159 and parameters: {'hidden_size': 80, 'dropout_prob': 0.6637967701094895, 'learning_rate': 0.0015734040867304835, 'weight_decay': 5.296599401924851e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 32\n",
      "Epoch [1/1000], Loss: 0.7097, Val Loss: 0.4698\n",
      "Epoch [2/1000], Loss: 0.3500, Val Loss: 0.4615\n",
      "Epoch [3/1000], Loss: 0.4891, Val Loss: 0.4660\n",
      "Epoch [4/1000], Loss: 0.3685, Val Loss: 0.4563\n",
      "Epoch [5/1000], Loss: 0.4130, Val Loss: 0.4539\n",
      "Epoch [6/1000], Loss: 1.2103, Val Loss: 0.4518\n",
      "Epoch [7/1000], Loss: 1.4834, Val Loss: 0.4495\n",
      "Epoch [8/1000], Loss: 0.0191, Val Loss: 0.4516\n",
      "Epoch [9/1000], Loss: 0.8076, Val Loss: 0.4502\n",
      "Epoch [10/1000], Loss: 0.1134, Val Loss: 0.4479\n",
      "Epoch [11/1000], Loss: 0.3804, Val Loss: 0.4478\n",
      "Epoch [12/1000], Loss: 0.2982, Val Loss: 0.4474\n",
      "Epoch [13/1000], Loss: 0.7113, Val Loss: 0.4491\n",
      "Epoch [14/1000], Loss: 0.3798, Val Loss: 0.4493\n",
      "Epoch [15/1000], Loss: 0.0622, Val Loss: 0.4501\n",
      "Epoch [16/1000], Loss: 0.4486, Val Loss: 0.4507\n",
      "Epoch [17/1000], Loss: 1.1704, Val Loss: 0.4510\n",
      "Epoch [18/1000], Loss: 1.4897, Val Loss: 0.4491\n",
      "Epoch [19/1000], Loss: 0.2188, Val Loss: 0.4569\n",
      "Epoch [20/1000], Loss: 0.2535, Val Loss: 0.4511\n",
      "Epoch [21/1000], Loss: 0.0507, Val Loss: 0.4503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:16:31,515] Trial 32 finished with value: 0.8212095413367864 and parameters: {'hidden_size': 64, 'dropout_prob': 0.6028134597480206, 'learning_rate': 0.0017889991317031851, 'weight_decay': 5.648993974937405e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 22\n",
      "Epoch [1/1000], Loss: 0.5102, Val Loss: 0.5031\n",
      "Epoch [2/1000], Loss: 0.1536, Val Loss: 0.4805\n",
      "Epoch [3/1000], Loss: 0.0622, Val Loss: 0.4707\n",
      "Epoch [4/1000], Loss: 0.8789, Val Loss: 0.4659\n",
      "Epoch [5/1000], Loss: 0.5695, Val Loss: 0.4620\n",
      "Epoch [6/1000], Loss: 0.0194, Val Loss: 0.4591\n",
      "Epoch [7/1000], Loss: 0.0900, Val Loss: 0.4580\n",
      "Epoch [8/1000], Loss: 0.0174, Val Loss: 0.4537\n",
      "Epoch [9/1000], Loss: 0.7648, Val Loss: 0.4532\n",
      "Epoch [10/1000], Loss: 0.5792, Val Loss: 0.4528\n",
      "Epoch [11/1000], Loss: 0.2147, Val Loss: 0.4523\n",
      "Epoch [12/1000], Loss: 0.2427, Val Loss: 0.4524\n",
      "Epoch [13/1000], Loss: 2.0246, Val Loss: 0.4517\n",
      "Epoch [14/1000], Loss: 1.4488, Val Loss: 0.4502\n",
      "Epoch [15/1000], Loss: 0.0069, Val Loss: 0.4504\n",
      "Epoch [16/1000], Loss: 0.8105, Val Loss: 0.4513\n",
      "Epoch [17/1000], Loss: 1.1570, Val Loss: 0.4508\n",
      "Epoch [18/1000], Loss: 0.5501, Val Loss: 0.4512\n",
      "Epoch [19/1000], Loss: 0.1714, Val Loss: 0.4506\n",
      "Epoch [20/1000], Loss: 0.0708, Val Loss: 0.4501\n",
      "Epoch [21/1000], Loss: 0.2475, Val Loss: 0.4502\n",
      "Epoch [22/1000], Loss: 0.2279, Val Loss: 0.4504\n",
      "Epoch [23/1000], Loss: 0.3219, Val Loss: 0.4493\n",
      "Epoch [24/1000], Loss: 0.0589, Val Loss: 0.4491\n",
      "Epoch [25/1000], Loss: 0.6966, Val Loss: 0.4493\n",
      "Epoch [26/1000], Loss: 0.0051, Val Loss: 0.4486\n",
      "Epoch [27/1000], Loss: 0.3010, Val Loss: 0.4485\n",
      "Epoch [28/1000], Loss: 0.3167, Val Loss: 0.4483\n",
      "Epoch [29/1000], Loss: 0.0232, Val Loss: 0.4485\n",
      "Epoch [30/1000], Loss: 0.0148, Val Loss: 0.4488\n",
      "Epoch [31/1000], Loss: 0.0446, Val Loss: 0.4486\n",
      "Epoch [32/1000], Loss: 0.0694, Val Loss: 0.4494\n",
      "Epoch [33/1000], Loss: 0.0822, Val Loss: 0.4480\n",
      "Epoch [34/1000], Loss: 0.0363, Val Loss: 0.4482\n",
      "Epoch [35/1000], Loss: 1.2503, Val Loss: 0.4482\n",
      "Epoch [36/1000], Loss: 1.0633, Val Loss: 0.4486\n",
      "Epoch [37/1000], Loss: 0.0010, Val Loss: 0.4491\n",
      "Epoch [38/1000], Loss: 0.3662, Val Loss: 0.4482\n",
      "Epoch [39/1000], Loss: 0.0078, Val Loss: 0.4482\n",
      "Epoch [40/1000], Loss: 0.6761, Val Loss: 0.4478\n",
      "Epoch [41/1000], Loss: 0.1496, Val Loss: 0.4475\n",
      "Epoch [42/1000], Loss: 0.0124, Val Loss: 0.4477\n",
      "Epoch [43/1000], Loss: 0.4669, Val Loss: 0.4482\n",
      "Epoch [44/1000], Loss: 0.3703, Val Loss: 0.4475\n",
      "Epoch [45/1000], Loss: 0.5875, Val Loss: 0.4478\n",
      "Epoch [46/1000], Loss: 0.7024, Val Loss: 0.4490\n",
      "Epoch [47/1000], Loss: 0.4421, Val Loss: 0.4497\n",
      "Epoch [48/1000], Loss: 0.0369, Val Loss: 0.4492\n",
      "Epoch [49/1000], Loss: 0.3752, Val Loss: 0.4488\n",
      "Epoch [50/1000], Loss: 0.4680, Val Loss: 0.4490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:16:45,822] Trial 33 finished with value: 0.8223234449955903 and parameters: {'hidden_size': 80, 'dropout_prob': 0.6478463065089365, 'learning_rate': 0.0005045933370083078, 'weight_decay': 4.109110522289084e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 51\n",
      "Epoch [1/1000], Loss: 0.6138, Val Loss: 0.4611\n",
      "Epoch [2/1000], Loss: 0.7676, Val Loss: 0.4547\n",
      "Epoch [3/1000], Loss: 0.0509, Val Loss: 0.4532\n",
      "Epoch [4/1000], Loss: 0.4331, Val Loss: 0.4555\n",
      "Epoch [5/1000], Loss: 0.8443, Val Loss: 0.4647\n",
      "Epoch [6/1000], Loss: 0.0100, Val Loss: 0.4557\n",
      "Epoch [7/1000], Loss: 0.0174, Val Loss: 0.4522\n",
      "Epoch [8/1000], Loss: 1.0215, Val Loss: 0.4593\n",
      "Epoch [9/1000], Loss: 0.3772, Val Loss: 0.4595\n",
      "Epoch [10/1000], Loss: 1.0142, Val Loss: 0.4543\n",
      "Epoch [11/1000], Loss: 0.4536, Val Loss: 0.4555\n",
      "Epoch [12/1000], Loss: 0.3472, Val Loss: 0.4577\n",
      "Epoch [13/1000], Loss: 0.2337, Val Loss: 0.4584\n",
      "Epoch [14/1000], Loss: 0.0461, Val Loss: 0.4555\n",
      "Epoch [15/1000], Loss: 0.0955, Val Loss: 0.4516\n",
      "Epoch [16/1000], Loss: 0.5353, Val Loss: 0.4559\n",
      "Epoch [17/1000], Loss: 0.2603, Val Loss: 0.4535\n",
      "Epoch [18/1000], Loss: 0.1895, Val Loss: 0.4599\n",
      "Epoch [19/1000], Loss: 0.3061, Val Loss: 0.4621\n",
      "Epoch [20/1000], Loss: 0.5737, Val Loss: 0.4558\n",
      "Epoch [21/1000], Loss: 0.7057, Val Loss: 0.4558\n",
      "Epoch [22/1000], Loss: 0.2693, Val Loss: 0.4570\n",
      "Epoch [23/1000], Loss: 0.1345, Val Loss: 0.4591\n",
      "Epoch [24/1000], Loss: 0.1007, Val Loss: 0.4544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:16:53,306] Trial 34 finished with value: 0.8158566862258294 and parameters: {'hidden_size': 67, 'dropout_prob': 0.6840275657234286, 'learning_rate': 0.006755076236098224, 'weight_decay': 5.0928495295770195e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 25\n",
      "Epoch [1/1000], Loss: 0.0268, Val Loss: 0.5116\n",
      "Epoch [2/1000], Loss: 1.7657, Val Loss: 0.4837\n",
      "Epoch [3/1000], Loss: 0.6736, Val Loss: 0.4726\n",
      "Epoch [4/1000], Loss: 0.2061, Val Loss: 0.4700\n",
      "Epoch [5/1000], Loss: 0.5049, Val Loss: 0.4651\n",
      "Epoch [6/1000], Loss: 0.1780, Val Loss: 0.4608\n",
      "Epoch [7/1000], Loss: 0.0481, Val Loss: 0.4590\n",
      "Epoch [8/1000], Loss: 0.5659, Val Loss: 0.4585\n",
      "Epoch [9/1000], Loss: 0.5017, Val Loss: 0.4562\n",
      "Epoch [10/1000], Loss: 0.4054, Val Loss: 0.4550\n",
      "Epoch [11/1000], Loss: 0.0567, Val Loss: 0.4519\n",
      "Epoch [12/1000], Loss: 0.5144, Val Loss: 0.4521\n",
      "Epoch [13/1000], Loss: 0.8967, Val Loss: 0.4528\n",
      "Epoch [14/1000], Loss: 0.5699, Val Loss: 0.4523\n",
      "Epoch [15/1000], Loss: 0.0224, Val Loss: 0.4523\n",
      "Epoch [16/1000], Loss: 0.0007, Val Loss: 0.4526\n",
      "Epoch [17/1000], Loss: 0.4511, Val Loss: 0.4509\n",
      "Epoch [18/1000], Loss: 0.5341, Val Loss: 0.4517\n",
      "Epoch [19/1000], Loss: 0.0028, Val Loss: 0.4494\n",
      "Epoch [20/1000], Loss: 1.4104, Val Loss: 0.4500\n",
      "Epoch [21/1000], Loss: 0.6526, Val Loss: 0.4496\n",
      "Epoch [22/1000], Loss: 0.1431, Val Loss: 0.4505\n",
      "Epoch [23/1000], Loss: 0.7069, Val Loss: 0.4501\n",
      "Epoch [24/1000], Loss: 0.0197, Val Loss: 0.4488\n",
      "Epoch [25/1000], Loss: 0.0656, Val Loss: 0.4500\n",
      "Epoch [26/1000], Loss: 0.1709, Val Loss: 0.4506\n",
      "Epoch [27/1000], Loss: 0.4624, Val Loss: 0.4501\n",
      "Epoch [28/1000], Loss: 1.8649, Val Loss: 0.4515\n",
      "Epoch [29/1000], Loss: 1.0737, Val Loss: 0.4497\n",
      "Epoch [30/1000], Loss: 0.6279, Val Loss: 0.4500\n",
      "Epoch [31/1000], Loss: 0.3802, Val Loss: 0.4504\n",
      "Epoch [32/1000], Loss: 0.3810, Val Loss: 0.4498\n",
      "Epoch [33/1000], Loss: 0.0734, Val Loss: 0.4516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:17:03,583] Trial 35 finished with value: 0.8207609210989157 and parameters: {'hidden_size': 51, 'dropout_prob': 0.7360396075870922, 'learning_rate': 0.0008647604136489651, 'weight_decay': 3.597693295473835e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 34\n",
      "Epoch [1/1000], Loss: 1.3801, Val Loss: 0.4678\n",
      "Epoch [2/1000], Loss: 0.1428, Val Loss: 0.4606\n",
      "Epoch [3/1000], Loss: 0.9969, Val Loss: 0.4549\n",
      "Epoch [4/1000], Loss: 3.6591, Val Loss: 0.4498\n",
      "Epoch [5/1000], Loss: 0.1388, Val Loss: 0.4514\n",
      "Epoch [6/1000], Loss: 0.8079, Val Loss: 0.4522\n",
      "Epoch [7/1000], Loss: 0.5470, Val Loss: 0.4483\n",
      "Epoch [8/1000], Loss: 0.2660, Val Loss: 0.4544\n",
      "Epoch [9/1000], Loss: 0.0037, Val Loss: 0.4472\n",
      "Epoch [10/1000], Loss: 0.2404, Val Loss: 0.4493\n",
      "Epoch [11/1000], Loss: 0.2216, Val Loss: 0.4467\n",
      "Epoch [12/1000], Loss: 0.7879, Val Loss: 0.4483\n",
      "Epoch [13/1000], Loss: 0.0539, Val Loss: 0.4492\n",
      "Epoch [14/1000], Loss: 1.1183, Val Loss: 0.4494\n",
      "Epoch [15/1000], Loss: 0.3767, Val Loss: 0.4462\n",
      "Epoch [16/1000], Loss: 0.0871, Val Loss: 0.4501\n",
      "Epoch [17/1000], Loss: 0.1455, Val Loss: 0.4454\n",
      "Epoch [18/1000], Loss: 0.9219, Val Loss: 0.4469\n",
      "Epoch [19/1000], Loss: 0.0123, Val Loss: 0.4476\n",
      "Epoch [20/1000], Loss: 0.5130, Val Loss: 0.4463\n",
      "Epoch [21/1000], Loss: 0.0334, Val Loss: 0.4481\n",
      "Epoch [22/1000], Loss: 0.1910, Val Loss: 0.4515\n",
      "Epoch [23/1000], Loss: 0.2002, Val Loss: 0.4478\n",
      "Epoch [24/1000], Loss: 0.0177, Val Loss: 0.4497\n",
      "Epoch [25/1000], Loss: 0.3951, Val Loss: 0.4517\n",
      "Epoch [26/1000], Loss: 0.8951, Val Loss: 0.4471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:17:11,658] Trial 36 finished with value: 0.8227159877037271 and parameters: {'hidden_size': 94, 'dropout_prob': 0.5808211612740303, 'learning_rate': 0.0014842530610283402, 'weight_decay': 3.0264076677675264e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 27\n",
      "Epoch [1/1000], Loss: 0.5034, Val Loss: 0.4720\n",
      "Epoch [2/1000], Loss: 0.2177, Val Loss: 0.4614\n",
      "Epoch [3/1000], Loss: 0.5688, Val Loss: 0.4712\n",
      "Epoch [4/1000], Loss: 0.6937, Val Loss: 0.4533\n",
      "Epoch [5/1000], Loss: 0.1371, Val Loss: 0.4517\n",
      "Epoch [6/1000], Loss: 1.0326, Val Loss: 0.4499\n",
      "Epoch [7/1000], Loss: 0.1243, Val Loss: 0.4557\n",
      "Epoch [8/1000], Loss: 0.8116, Val Loss: 0.4531\n",
      "Epoch [9/1000], Loss: 0.0923, Val Loss: 0.4507\n",
      "Epoch [10/1000], Loss: 0.3928, Val Loss: 0.4566\n",
      "Epoch [11/1000], Loss: 0.0596, Val Loss: 0.4528\n",
      "Epoch [12/1000], Loss: 0.3314, Val Loss: 0.4529\n",
      "Epoch [13/1000], Loss: 1.1916, Val Loss: 0.4523\n",
      "Epoch [14/1000], Loss: 0.2452, Val Loss: 0.4516\n",
      "Epoch [15/1000], Loss: 1.3439, Val Loss: 0.4520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:17:15,891] Trial 37 finished with value: 0.8196164296966204 and parameters: {'hidden_size': 17, 'dropout_prob': 0.5329122869646185, 'learning_rate': 0.0024317526556693345, 'weight_decay': 3.958375511290931e-06}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 16\n",
      "Epoch [1/1000], Loss: 0.1063, Val Loss: 0.5037\n",
      "Epoch [2/1000], Loss: 0.1258, Val Loss: 0.4772\n",
      "Epoch [3/1000], Loss: 0.1399, Val Loss: 0.4666\n",
      "Epoch [4/1000], Loss: 0.0590, Val Loss: 0.4625\n",
      "Epoch [5/1000], Loss: 2.5899, Val Loss: 0.4578\n",
      "Epoch [6/1000], Loss: 0.5952, Val Loss: 0.4553\n",
      "Epoch [7/1000], Loss: 0.1408, Val Loss: 0.4527\n",
      "Epoch [8/1000], Loss: 0.0228, Val Loss: 0.4516\n",
      "Epoch [9/1000], Loss: 0.1513, Val Loss: 0.4503\n",
      "Epoch [10/1000], Loss: 0.0008, Val Loss: 0.4491\n",
      "Epoch [11/1000], Loss: 1.3289, Val Loss: 0.4490\n",
      "Epoch [12/1000], Loss: 0.3968, Val Loss: 0.4488\n",
      "Epoch [13/1000], Loss: 0.0530, Val Loss: 0.4474\n",
      "Epoch [14/1000], Loss: 0.7732, Val Loss: 0.4497\n",
      "Epoch [15/1000], Loss: 0.0253, Val Loss: 0.4471\n",
      "Epoch [16/1000], Loss: 0.4194, Val Loss: 0.4470\n",
      "Epoch [17/1000], Loss: 0.3134, Val Loss: 0.4460\n",
      "Epoch [18/1000], Loss: 1.0717, Val Loss: 0.4456\n",
      "Epoch [19/1000], Loss: 0.6703, Val Loss: 0.4471\n",
      "Epoch [20/1000], Loss: 0.5804, Val Loss: 0.4480\n",
      "Epoch [21/1000], Loss: 0.8247, Val Loss: 0.4469\n",
      "Epoch [22/1000], Loss: 0.9524, Val Loss: 0.4474\n",
      "Epoch [23/1000], Loss: 0.0283, Val Loss: 0.4475\n",
      "Epoch [24/1000], Loss: 0.2726, Val Loss: 0.4462\n",
      "Epoch [25/1000], Loss: 1.9481, Val Loss: 0.4469\n",
      "Epoch [26/1000], Loss: 0.5219, Val Loss: 0.4465\n",
      "Epoch [27/1000], Loss: 0.0028, Val Loss: 0.4468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:17:23,774] Trial 38 finished with value: 0.8232716650438169 and parameters: {'hidden_size': 102, 'dropout_prob': 0.7202303660027303, 'learning_rate': 0.0005539652258014079, 'weight_decay': 7.148606512861626e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 28\n",
      "Epoch [1/1000], Loss: 0.4091, Val Loss: 0.5167\n",
      "Epoch [2/1000], Loss: 0.9812, Val Loss: 0.4899\n",
      "Epoch [3/1000], Loss: 0.3211, Val Loss: 0.4763\n",
      "Epoch [4/1000], Loss: 0.5984, Val Loss: 0.4692\n",
      "Epoch [5/1000], Loss: 1.2700, Val Loss: 0.4657\n",
      "Epoch [6/1000], Loss: 0.7145, Val Loss: 0.4626\n",
      "Epoch [7/1000], Loss: 0.4805, Val Loss: 0.4621\n",
      "Epoch [8/1000], Loss: 0.5197, Val Loss: 0.4596\n",
      "Epoch [9/1000], Loss: 0.3794, Val Loss: 0.4580\n",
      "Epoch [10/1000], Loss: 0.6946, Val Loss: 0.4565\n",
      "Epoch [11/1000], Loss: 0.5512, Val Loss: 0.4555\n",
      "Epoch [12/1000], Loss: 0.1678, Val Loss: 0.4542\n",
      "Epoch [13/1000], Loss: 0.0789, Val Loss: 0.4533\n",
      "Epoch [14/1000], Loss: 0.6124, Val Loss: 0.4529\n",
      "Epoch [15/1000], Loss: 0.0453, Val Loss: 0.4522\n",
      "Epoch [16/1000], Loss: 0.0360, Val Loss: 0.4519\n",
      "Epoch [17/1000], Loss: 1.5650, Val Loss: 0.4517\n",
      "Epoch [18/1000], Loss: 0.4649, Val Loss: 0.4513\n",
      "Epoch [19/1000], Loss: 0.2107, Val Loss: 0.4500\n",
      "Epoch [20/1000], Loss: 0.1790, Val Loss: 0.4505\n",
      "Epoch [21/1000], Loss: 0.1323, Val Loss: 0.4498\n",
      "Epoch [22/1000], Loss: 1.3573, Val Loss: 0.4493\n",
      "Epoch [23/1000], Loss: 0.0869, Val Loss: 0.4501\n",
      "Epoch [24/1000], Loss: 4.6901, Val Loss: 0.4504\n",
      "Epoch [25/1000], Loss: 1.8521, Val Loss: 0.4490\n",
      "Epoch [26/1000], Loss: 0.0127, Val Loss: 0.4492\n",
      "Epoch [27/1000], Loss: 0.7214, Val Loss: 0.4491\n",
      "Epoch [28/1000], Loss: 0.5284, Val Loss: 0.4485\n",
      "Epoch [29/1000], Loss: 0.0028, Val Loss: 0.4489\n",
      "Epoch [30/1000], Loss: 2.3328, Val Loss: 0.4491\n",
      "Epoch [31/1000], Loss: 0.1536, Val Loss: 0.4491\n",
      "Epoch [32/1000], Loss: 0.8191, Val Loss: 0.4484\n",
      "Epoch [33/1000], Loss: 0.3518, Val Loss: 0.4480\n",
      "Epoch [34/1000], Loss: 0.0385, Val Loss: 0.4485\n",
      "Epoch [35/1000], Loss: 0.0181, Val Loss: 0.4476\n",
      "Epoch [36/1000], Loss: 0.6279, Val Loss: 0.4479\n",
      "Epoch [37/1000], Loss: 0.4127, Val Loss: 0.4475\n",
      "Epoch [38/1000], Loss: 0.3287, Val Loss: 0.4479\n",
      "Epoch [39/1000], Loss: 0.0536, Val Loss: 0.4473\n",
      "Epoch [40/1000], Loss: 0.1340, Val Loss: 0.4472\n",
      "Epoch [41/1000], Loss: 0.2203, Val Loss: 0.4472\n",
      "Epoch [42/1000], Loss: 0.0829, Val Loss: 0.4464\n",
      "Epoch [43/1000], Loss: 0.0271, Val Loss: 0.4467\n",
      "Epoch [44/1000], Loss: 0.4458, Val Loss: 0.4465\n",
      "Epoch [45/1000], Loss: 1.2356, Val Loss: 0.4467\n",
      "Epoch [46/1000], Loss: 0.2813, Val Loss: 0.4469\n",
      "Epoch [47/1000], Loss: 0.0125, Val Loss: 0.4468\n",
      "Epoch [48/1000], Loss: 0.4590, Val Loss: 0.4465\n",
      "Epoch [49/1000], Loss: 0.0173, Val Loss: 0.4460\n",
      "Epoch [50/1000], Loss: 0.1474, Val Loss: 0.4474\n",
      "Epoch [51/1000], Loss: 1.0310, Val Loss: 0.4466\n",
      "Epoch [52/1000], Loss: 0.0210, Val Loss: 0.4475\n",
      "Epoch [53/1000], Loss: 0.6441, Val Loss: 0.4471\n",
      "Epoch [54/1000], Loss: 0.0346, Val Loss: 0.4467\n",
      "Epoch [55/1000], Loss: 0.0015, Val Loss: 0.4469\n",
      "Epoch [56/1000], Loss: 0.3762, Val Loss: 0.4472\n",
      "Epoch [57/1000], Loss: 0.5546, Val Loss: 0.4468\n",
      "Epoch [58/1000], Loss: 0.7053, Val Loss: 0.4468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:17:40,275] Trial 39 finished with value: 0.8231314712194823 and parameters: {'hidden_size': 101, 'dropout_prob': 0.7137413385572866, 'learning_rate': 0.00040773819034212015, 'weight_decay': 5.288570168801338e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 59\n",
      "Epoch [1/1000], Loss: 0.0346, Val Loss: 0.4756\n",
      "Epoch [2/1000], Loss: 0.7835, Val Loss: 0.4663\n",
      "Epoch [3/1000], Loss: 0.2972, Val Loss: 0.4627\n",
      "Epoch [4/1000], Loss: 0.5244, Val Loss: 0.4545\n",
      "Epoch [5/1000], Loss: 0.1325, Val Loss: 0.4528\n",
      "Epoch [6/1000], Loss: 0.1186, Val Loss: 0.4527\n",
      "Epoch [7/1000], Loss: 0.0263, Val Loss: 0.4516\n",
      "Epoch [8/1000], Loss: 0.1073, Val Loss: 0.4537\n",
      "Epoch [9/1000], Loss: 0.9439, Val Loss: 0.4505\n",
      "Epoch [10/1000], Loss: 0.0283, Val Loss: 0.4532\n",
      "Epoch [11/1000], Loss: 0.4176, Val Loss: 0.4530\n",
      "Epoch [12/1000], Loss: 0.5070, Val Loss: 0.4483\n",
      "Epoch [13/1000], Loss: 0.6017, Val Loss: 0.4498\n",
      "Epoch [14/1000], Loss: 0.0674, Val Loss: 0.4491\n",
      "Epoch [15/1000], Loss: 0.1174, Val Loss: 0.4487\n",
      "Epoch [16/1000], Loss: 0.2681, Val Loss: 0.4479\n",
      "Epoch [17/1000], Loss: 0.0668, Val Loss: 0.4487\n",
      "Epoch [18/1000], Loss: 0.3014, Val Loss: 0.4483\n",
      "Epoch [19/1000], Loss: 0.4089, Val Loss: 0.4513\n",
      "Epoch [20/1000], Loss: 0.4629, Val Loss: 0.4531\n",
      "Epoch [21/1000], Loss: 0.0641, Val Loss: 0.4508\n",
      "Epoch [22/1000], Loss: 0.6192, Val Loss: 0.4501\n",
      "Epoch [23/1000], Loss: 0.4871, Val Loss: 0.4503\n",
      "Epoch [24/1000], Loss: 0.8761, Val Loss: 0.4482\n",
      "Epoch [25/1000], Loss: 0.1048, Val Loss: 0.4519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:17:47,527] Trial 40 finished with value: 0.8203938681770215 and parameters: {'hidden_size': 106, 'dropout_prob': 0.7549586680970531, 'learning_rate': 0.001319423398534273, 'weight_decay': 7.118666378031422e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 26\n",
      "Epoch [1/1000], Loss: 0.2324, Val Loss: 0.5123\n",
      "Epoch [2/1000], Loss: 0.3534, Val Loss: 0.4861\n",
      "Epoch [3/1000], Loss: 0.5208, Val Loss: 0.4743\n",
      "Epoch [4/1000], Loss: 0.7611, Val Loss: 0.4700\n",
      "Epoch [5/1000], Loss: 0.5783, Val Loss: 0.4656\n",
      "Epoch [6/1000], Loss: 0.7367, Val Loss: 0.4636\n",
      "Epoch [7/1000], Loss: 0.3446, Val Loss: 0.4641\n",
      "Epoch [8/1000], Loss: 0.6402, Val Loss: 0.4589\n",
      "Epoch [9/1000], Loss: 0.4834, Val Loss: 0.4583\n",
      "Epoch [10/1000], Loss: 0.4087, Val Loss: 0.4561\n",
      "Epoch [11/1000], Loss: 0.0176, Val Loss: 0.4552\n",
      "Epoch [12/1000], Loss: 0.4332, Val Loss: 0.4549\n",
      "Epoch [13/1000], Loss: 0.5484, Val Loss: 0.4529\n",
      "Epoch [14/1000], Loss: 0.1470, Val Loss: 0.4518\n",
      "Epoch [15/1000], Loss: 0.3196, Val Loss: 0.4522\n",
      "Epoch [16/1000], Loss: 0.8085, Val Loss: 0.4513\n",
      "Epoch [17/1000], Loss: 0.1223, Val Loss: 0.4509\n",
      "Epoch [18/1000], Loss: 0.0073, Val Loss: 0.4510\n",
      "Epoch [19/1000], Loss: 0.3632, Val Loss: 0.4496\n",
      "Epoch [20/1000], Loss: 1.0975, Val Loss: 0.4495\n",
      "Epoch [21/1000], Loss: 0.7505, Val Loss: 0.4504\n",
      "Epoch [22/1000], Loss: 0.5907, Val Loss: 0.4504\n",
      "Epoch [23/1000], Loss: 0.0060, Val Loss: 0.4501\n",
      "Epoch [24/1000], Loss: 0.0775, Val Loss: 0.4500\n",
      "Epoch [25/1000], Loss: 0.6895, Val Loss: 0.4494\n",
      "Epoch [26/1000], Loss: 0.9349, Val Loss: 0.4486\n",
      "Epoch [27/1000], Loss: 0.6867, Val Loss: 0.4486\n",
      "Epoch [28/1000], Loss: 0.0871, Val Loss: 0.4490\n",
      "Epoch [29/1000], Loss: 0.1841, Val Loss: 0.4488\n",
      "Epoch [30/1000], Loss: 0.4429, Val Loss: 0.4485\n",
      "Epoch [31/1000], Loss: 1.2527, Val Loss: 0.4482\n",
      "Epoch [32/1000], Loss: 0.6638, Val Loss: 0.4487\n",
      "Epoch [33/1000], Loss: 0.4001, Val Loss: 0.4482\n",
      "Epoch [34/1000], Loss: 0.0531, Val Loss: 0.4492\n",
      "Epoch [35/1000], Loss: 0.0704, Val Loss: 0.4482\n",
      "Epoch [36/1000], Loss: 0.3873, Val Loss: 0.4481\n",
      "Epoch [37/1000], Loss: 0.0489, Val Loss: 0.4483\n",
      "Epoch [38/1000], Loss: 0.3416, Val Loss: 0.4479\n",
      "Epoch [39/1000], Loss: 0.2490, Val Loss: 0.4486\n",
      "Epoch [40/1000], Loss: 0.3803, Val Loss: 0.4475\n",
      "Epoch [41/1000], Loss: 0.0340, Val Loss: 0.4481\n",
      "Epoch [42/1000], Loss: 0.0344, Val Loss: 0.4477\n",
      "Epoch [43/1000], Loss: 1.0688, Val Loss: 0.4476\n",
      "Epoch [44/1000], Loss: 0.2320, Val Loss: 0.4476\n",
      "Epoch [45/1000], Loss: 0.5749, Val Loss: 0.4477\n",
      "Epoch [46/1000], Loss: 0.4301, Val Loss: 0.4473\n",
      "Epoch [47/1000], Loss: 0.4266, Val Loss: 0.4485\n",
      "Epoch [48/1000], Loss: 0.1763, Val Loss: 0.4471\n",
      "Epoch [49/1000], Loss: 0.2010, Val Loss: 0.4475\n",
      "Epoch [50/1000], Loss: 0.6721, Val Loss: 0.4479\n",
      "Epoch [51/1000], Loss: 0.1693, Val Loss: 0.4477\n",
      "Epoch [52/1000], Loss: 0.7899, Val Loss: 0.4471\n",
      "Epoch [53/1000], Loss: 0.7175, Val Loss: 0.4471\n",
      "Epoch [54/1000], Loss: 0.3877, Val Loss: 0.4484\n",
      "Epoch [55/1000], Loss: 0.2595, Val Loss: 0.4477\n",
      "Epoch [56/1000], Loss: 0.3914, Val Loss: 0.4482\n",
      "Epoch [57/1000], Loss: 0.4551, Val Loss: 0.4478\n",
      "Epoch [58/1000], Loss: 0.6111, Val Loss: 0.4482\n",
      "Epoch [59/1000], Loss: 1.2849, Val Loss: 0.4483\n",
      "Epoch [60/1000], Loss: 1.5230, Val Loss: 0.4477\n",
      "Epoch [61/1000], Loss: 0.0675, Val Loss: 0.4479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:18:07,015] Trial 41 finished with value: 0.8227516734044669 and parameters: {'hidden_size': 114, 'dropout_prob': 0.7070138747823681, 'learning_rate': 0.00037137515166768423, 'weight_decay': 5.237707275127787e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 62\n",
      "Epoch [1/1000], Loss: 1.3513, Val Loss: 0.5170\n",
      "Epoch [2/1000], Loss: 1.0096, Val Loss: 0.4867\n",
      "Epoch [3/1000], Loss: 0.1602, Val Loss: 0.4771\n",
      "Epoch [4/1000], Loss: 0.7297, Val Loss: 0.4692\n",
      "Epoch [5/1000], Loss: 0.1494, Val Loss: 0.4652\n",
      "Epoch [6/1000], Loss: 0.0470, Val Loss: 0.4627\n",
      "Epoch [7/1000], Loss: 0.1701, Val Loss: 0.4615\n",
      "Epoch [8/1000], Loss: 0.9336, Val Loss: 0.4564\n",
      "Epoch [9/1000], Loss: 1.3212, Val Loss: 0.4554\n",
      "Epoch [10/1000], Loss: 0.5226, Val Loss: 0.4551\n",
      "Epoch [11/1000], Loss: 0.2842, Val Loss: 0.4531\n",
      "Epoch [12/1000], Loss: 0.5716, Val Loss: 0.4528\n",
      "Epoch [13/1000], Loss: 0.0310, Val Loss: 0.4527\n",
      "Epoch [14/1000], Loss: 0.1015, Val Loss: 0.4528\n",
      "Epoch [15/1000], Loss: 0.0400, Val Loss: 0.4509\n",
      "Epoch [16/1000], Loss: 0.5948, Val Loss: 0.4510\n",
      "Epoch [17/1000], Loss: 0.7845, Val Loss: 0.4504\n",
      "Epoch [18/1000], Loss: 0.0127, Val Loss: 0.4511\n",
      "Epoch [19/1000], Loss: 0.4830, Val Loss: 0.4504\n",
      "Epoch [20/1000], Loss: 0.2629, Val Loss: 0.4507\n",
      "Epoch [21/1000], Loss: 0.2889, Val Loss: 0.4510\n",
      "Epoch [22/1000], Loss: 0.4652, Val Loss: 0.4497\n",
      "Epoch [23/1000], Loss: 0.0516, Val Loss: 0.4500\n",
      "Epoch [24/1000], Loss: 0.0033, Val Loss: 0.4501\n",
      "Epoch [25/1000], Loss: 0.5957, Val Loss: 0.4501\n",
      "Epoch [26/1000], Loss: 0.5482, Val Loss: 0.4497\n",
      "Epoch [27/1000], Loss: 0.0176, Val Loss: 0.4512\n",
      "Epoch [28/1000], Loss: 0.9910, Val Loss: 0.4500\n",
      "Epoch [29/1000], Loss: 0.0245, Val Loss: 0.4499\n",
      "Epoch [30/1000], Loss: 0.9567, Val Loss: 0.4498\n",
      "Epoch [31/1000], Loss: 0.0071, Val Loss: 0.4499\n",
      "Epoch [32/1000], Loss: 0.0029, Val Loss: 0.4502\n",
      "Epoch [33/1000], Loss: 0.9738, Val Loss: 0.4501\n",
      "Epoch [34/1000], Loss: 0.5766, Val Loss: 0.4504\n",
      "Epoch [35/1000], Loss: 0.0804, Val Loss: 0.4501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:18:17,911] Trial 42 finished with value: 0.820523866086859 and parameters: {'hidden_size': 103, 'dropout_prob': 0.7120794700482834, 'learning_rate': 0.0004542421672043552, 'weight_decay': 4.615230767166354e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 36\n",
      "Epoch [1/1000], Loss: 0.0405, Val Loss: 0.4732\n",
      "Epoch [2/1000], Loss: 0.0915, Val Loss: 0.4603\n",
      "Epoch [3/1000], Loss: 0.1044, Val Loss: 0.4559\n",
      "Epoch [4/1000], Loss: 0.4817, Val Loss: 0.4527\n",
      "Epoch [5/1000], Loss: 0.4660, Val Loss: 0.4473\n",
      "Epoch [6/1000], Loss: 0.0005, Val Loss: 0.4508\n",
      "Epoch [7/1000], Loss: 0.5431, Val Loss: 0.4490\n",
      "Epoch [8/1000], Loss: 0.9568, Val Loss: 0.4516\n",
      "Epoch [9/1000], Loss: 0.8471, Val Loss: 0.4473\n",
      "Epoch [10/1000], Loss: 1.2242, Val Loss: 0.4511\n",
      "Epoch [11/1000], Loss: 2.7807, Val Loss: 0.4504\n",
      "Epoch [12/1000], Loss: 0.0577, Val Loss: 0.4511\n",
      "Epoch [13/1000], Loss: 0.0372, Val Loss: 0.4452\n",
      "Epoch [14/1000], Loss: 0.8970, Val Loss: 0.4483\n",
      "Epoch [15/1000], Loss: 0.2555, Val Loss: 0.4501\n",
      "Epoch [16/1000], Loss: 0.1109, Val Loss: 0.4468\n",
      "Epoch [17/1000], Loss: 0.2441, Val Loss: 0.4574\n",
      "Epoch [18/1000], Loss: 0.0677, Val Loss: 0.4472\n",
      "Epoch [19/1000], Loss: 0.2528, Val Loss: 0.4493\n",
      "Epoch [20/1000], Loss: 0.1440, Val Loss: 0.4485\n",
      "Epoch [21/1000], Loss: 0.0435, Val Loss: 0.4480\n",
      "Epoch [22/1000], Loss: 0.0038, Val Loss: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:18:25,591] Trial 43 finished with value: 0.8211534638070527 and parameters: {'hidden_size': 125, 'dropout_prob': 0.7235933718914755, 'learning_rate': 0.0016397766158227798, 'weight_decay': 5.871181664069733e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 23\n",
      "Epoch [1/1000], Loss: 0.5839, Val Loss: 0.4695\n",
      "Epoch [2/1000], Loss: 0.0036, Val Loss: 0.4539\n",
      "Epoch [3/1000], Loss: 0.5081, Val Loss: 0.4539\n",
      "Epoch [4/1000], Loss: 0.6407, Val Loss: 0.4504\n",
      "Epoch [5/1000], Loss: 0.4882, Val Loss: 0.4542\n",
      "Epoch [6/1000], Loss: 0.7432, Val Loss: 0.4502\n",
      "Epoch [7/1000], Loss: 0.6026, Val Loss: 0.4529\n",
      "Epoch [8/1000], Loss: 0.6771, Val Loss: 0.4515\n",
      "Epoch [9/1000], Loss: 0.1497, Val Loss: 0.4506\n",
      "Epoch [10/1000], Loss: 0.1818, Val Loss: 0.4496\n",
      "Epoch [11/1000], Loss: 0.3819, Val Loss: 0.4505\n",
      "Epoch [12/1000], Loss: 0.3686, Val Loss: 0.4580\n",
      "Epoch [13/1000], Loss: 0.2702, Val Loss: 0.4526\n",
      "Epoch [14/1000], Loss: 0.0248, Val Loss: 0.4524\n",
      "Epoch [15/1000], Loss: 2.2938, Val Loss: 0.4507\n",
      "Epoch [16/1000], Loss: 1.8510, Val Loss: 0.4544\n",
      "Epoch [17/1000], Loss: 0.6123, Val Loss: 0.4489\n",
      "Epoch [18/1000], Loss: 1.4376, Val Loss: 0.4522\n",
      "Epoch [19/1000], Loss: 0.4045, Val Loss: 0.4489\n",
      "Epoch [20/1000], Loss: 1.2236, Val Loss: 0.4514\n",
      "Epoch [21/1000], Loss: 1.1222, Val Loss: 0.4499\n",
      "Epoch [22/1000], Loss: 0.2551, Val Loss: 0.4489\n",
      "Epoch [23/1000], Loss: 1.4180, Val Loss: 0.4505\n",
      "Epoch [24/1000], Loss: 0.0103, Val Loss: 0.4474\n",
      "Epoch [25/1000], Loss: 0.3142, Val Loss: 0.4506\n",
      "Epoch [26/1000], Loss: 0.1817, Val Loss: 0.4484\n",
      "Epoch [27/1000], Loss: 0.0011, Val Loss: 0.4460\n",
      "Epoch [28/1000], Loss: 0.0039, Val Loss: 0.4487\n",
      "Epoch [29/1000], Loss: 0.3281, Val Loss: 0.4474\n",
      "Epoch [30/1000], Loss: 0.0751, Val Loss: 0.4490\n",
      "Epoch [31/1000], Loss: 1.4574, Val Loss: 0.4479\n",
      "Epoch [32/1000], Loss: 1.4009, Val Loss: 0.4476\n",
      "Epoch [33/1000], Loss: 1.7322, Val Loss: 0.4586\n",
      "Epoch [34/1000], Loss: 0.0292, Val Loss: 0.4496\n",
      "Epoch [35/1000], Loss: 0.2307, Val Loss: 0.4503\n",
      "Epoch [36/1000], Loss: 0.0129, Val Loss: 0.4606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:18:36,869] Trial 44 finished with value: 0.8198381908369318 and parameters: {'hidden_size': 114, 'dropout_prob': 0.6488367055484638, 'learning_rate': 0.0030462718496752166, 'weight_decay': 8.292632164910751e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 37\n",
      "Epoch [1/1000], Loss: 1.8517, Val Loss: 0.4887\n",
      "Epoch [2/1000], Loss: 0.7364, Val Loss: 0.4617\n",
      "Epoch [3/1000], Loss: 0.7462, Val Loss: 0.4667\n",
      "Epoch [4/1000], Loss: 0.0037, Val Loss: 0.4489\n",
      "Epoch [5/1000], Loss: 0.5211, Val Loss: 0.4493\n",
      "Epoch [6/1000], Loss: 0.0822, Val Loss: 0.4805\n",
      "Epoch [7/1000], Loss: 2.0413, Val Loss: 0.4543\n",
      "Epoch [8/1000], Loss: 0.9809, Val Loss: 0.4574\n",
      "Epoch [9/1000], Loss: 0.5620, Val Loss: 0.4543\n",
      "Epoch [10/1000], Loss: 0.1066, Val Loss: 0.4530\n",
      "Epoch [11/1000], Loss: 0.3065, Val Loss: 0.4637\n",
      "Epoch [12/1000], Loss: 0.0165, Val Loss: 0.4560\n",
      "Epoch [13/1000], Loss: 0.1925, Val Loss: 0.4567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:18:40,757] Trial 45 finished with value: 0.821436400434346 and parameters: {'hidden_size': 98, 'dropout_prob': 0.6061523329635748, 'learning_rate': 0.008945906762841235, 'weight_decay': 6.440344478479644e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 14\n",
      "Epoch [1/1000], Loss: 0.3811, Val Loss: 0.4604\n",
      "Epoch [2/1000], Loss: 0.7629, Val Loss: 0.4722\n",
      "Epoch [3/1000], Loss: 0.8765, Val Loss: 0.4559\n",
      "Epoch [4/1000], Loss: 0.3243, Val Loss: 0.4558\n",
      "Epoch [5/1000], Loss: 0.5804, Val Loss: 0.4527\n",
      "Epoch [6/1000], Loss: 0.0531, Val Loss: 0.4519\n",
      "Epoch [7/1000], Loss: 0.9070, Val Loss: 0.4598\n",
      "Epoch [8/1000], Loss: 0.1642, Val Loss: 0.4528\n",
      "Epoch [9/1000], Loss: 0.2665, Val Loss: 0.4541\n",
      "Epoch [10/1000], Loss: 0.6228, Val Loss: 0.4492\n",
      "Epoch [11/1000], Loss: 0.3135, Val Loss: 0.4522\n",
      "Epoch [12/1000], Loss: 0.0067, Val Loss: 0.4508\n",
      "Epoch [13/1000], Loss: 0.0159, Val Loss: 0.4549\n",
      "Epoch [14/1000], Loss: 2.1242, Val Loss: 0.4497\n",
      "Epoch [15/1000], Loss: 0.2391, Val Loss: 0.4514\n",
      "Epoch [16/1000], Loss: 0.0055, Val Loss: 0.4510\n",
      "Epoch [17/1000], Loss: 0.0002, Val Loss: 0.4489\n",
      "Epoch [18/1000], Loss: 0.2577, Val Loss: 0.4487\n",
      "Epoch [19/1000], Loss: 0.0148, Val Loss: 0.4566\n",
      "Epoch [20/1000], Loss: 1.7097, Val Loss: 0.4528\n",
      "Epoch [21/1000], Loss: 0.0924, Val Loss: 0.4538\n",
      "Epoch [22/1000], Loss: 1.1006, Val Loss: 0.4486\n",
      "Epoch [23/1000], Loss: 0.3096, Val Loss: 0.4507\n",
      "Epoch [24/1000], Loss: 0.3256, Val Loss: 0.4518\n",
      "Epoch [25/1000], Loss: 1.2739, Val Loss: 0.4491\n",
      "Epoch [26/1000], Loss: 1.2659, Val Loss: 0.4486\n",
      "Epoch [27/1000], Loss: 0.3884, Val Loss: 0.4535\n",
      "Epoch [28/1000], Loss: 0.5547, Val Loss: 0.4515\n",
      "Epoch [29/1000], Loss: 0.6678, Val Loss: 0.4513\n",
      "Epoch [30/1000], Loss: 0.0868, Val Loss: 0.4509\n",
      "Epoch [31/1000], Loss: 0.1769, Val Loss: 0.4505\n",
      "Epoch [32/1000], Loss: 0.0001, Val Loss: 0.4494\n",
      "Epoch [33/1000], Loss: 0.4518, Val Loss: 0.4522\n",
      "Epoch [34/1000], Loss: 0.9839, Val Loss: 0.4503\n",
      "Epoch [35/1000], Loss: 0.0057, Val Loss: 0.4512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:18:50,830] Trial 46 finished with value: 0.8204015151128943 and parameters: {'hidden_size': 108, 'dropout_prob': 0.7764855875483259, 'learning_rate': 0.0042226830304631265, 'weight_decay': 7.027761229883012e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 36\n",
      "Epoch [1/1000], Loss: 1.4235, Val Loss: 0.4994\n",
      "Epoch [2/1000], Loss: 0.6534, Val Loss: 0.4752\n",
      "Epoch [3/1000], Loss: 0.5098, Val Loss: 0.4660\n",
      "Epoch [4/1000], Loss: 0.1131, Val Loss: 0.4632\n",
      "Epoch [5/1000], Loss: 0.0025, Val Loss: 0.4617\n",
      "Epoch [6/1000], Loss: 0.0150, Val Loss: 0.4593\n",
      "Epoch [7/1000], Loss: 0.7954, Val Loss: 0.4584\n",
      "Epoch [8/1000], Loss: 0.0017, Val Loss: 0.4562\n",
      "Epoch [9/1000], Loss: 0.0275, Val Loss: 0.4554\n",
      "Epoch [10/1000], Loss: 0.9480, Val Loss: 0.4541\n",
      "Epoch [11/1000], Loss: 0.9294, Val Loss: 0.4533\n",
      "Epoch [12/1000], Loss: 0.6044, Val Loss: 0.4533\n",
      "Epoch [13/1000], Loss: 0.9802, Val Loss: 0.4511\n",
      "Epoch [14/1000], Loss: 0.2678, Val Loss: 0.4496\n",
      "Epoch [15/1000], Loss: 0.1450, Val Loss: 0.4501\n",
      "Epoch [16/1000], Loss: 0.0132, Val Loss: 0.4502\n",
      "Epoch [17/1000], Loss: 0.5501, Val Loss: 0.4498\n",
      "Epoch [18/1000], Loss: 0.0361, Val Loss: 0.4484\n",
      "Epoch [19/1000], Loss: 0.5388, Val Loss: 0.4497\n",
      "Epoch [20/1000], Loss: 0.5059, Val Loss: 0.4473\n",
      "Epoch [21/1000], Loss: 1.0135, Val Loss: 0.4473\n",
      "Epoch [22/1000], Loss: 0.2810, Val Loss: 0.4467\n",
      "Epoch [23/1000], Loss: 0.5104, Val Loss: 0.4470\n",
      "Epoch [24/1000], Loss: 0.0021, Val Loss: 0.4471\n",
      "Epoch [25/1000], Loss: 0.0075, Val Loss: 0.4473\n",
      "Epoch [26/1000], Loss: 0.5007, Val Loss: 0.4465\n",
      "Epoch [27/1000], Loss: 0.1211, Val Loss: 0.4459\n",
      "Epoch [28/1000], Loss: 0.0523, Val Loss: 0.4469\n",
      "Epoch [29/1000], Loss: 0.0300, Val Loss: 0.4463\n",
      "Epoch [30/1000], Loss: 0.0315, Val Loss: 0.4457\n",
      "Epoch [31/1000], Loss: 0.1624, Val Loss: 0.4470\n",
      "Epoch [32/1000], Loss: 0.7400, Val Loss: 0.4459\n",
      "Epoch [33/1000], Loss: 3.0176, Val Loss: 0.4461\n",
      "Epoch [34/1000], Loss: 0.1903, Val Loss: 0.4461\n",
      "Epoch [35/1000], Loss: 0.0159, Val Loss: 0.4460\n",
      "Epoch [36/1000], Loss: 0.2566, Val Loss: 0.4460\n",
      "Epoch [37/1000], Loss: 1.9121, Val Loss: 0.4465\n",
      "Epoch [38/1000], Loss: 0.0811, Val Loss: 0.4464\n",
      "Epoch [39/1000], Loss: 0.0569, Val Loss: 0.4461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:19:01,688] Trial 47 finished with value: 0.8225808918366411 and parameters: {'hidden_size': 86, 'dropout_prob': 0.6950563786707242, 'learning_rate': 0.000527710557205804, 'weight_decay': 3.3205834292905016e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 40\n",
      "Epoch [1/1000], Loss: 0.1201, Val Loss: 0.4755\n",
      "Epoch [2/1000], Loss: 0.5775, Val Loss: 0.4581\n",
      "Epoch [3/1000], Loss: 1.0972, Val Loss: 0.4537\n",
      "Epoch [4/1000], Loss: 0.9313, Val Loss: 0.4675\n",
      "Epoch [5/1000], Loss: 0.4909, Val Loss: 0.4550\n",
      "Epoch [6/1000], Loss: 0.9566, Val Loss: 0.4550\n",
      "Epoch [7/1000], Loss: 1.6587, Val Loss: 0.4555\n",
      "Epoch [8/1000], Loss: 0.0338, Val Loss: 0.4548\n",
      "Epoch [9/1000], Loss: 1.1279, Val Loss: 0.4506\n",
      "Epoch [10/1000], Loss: 0.1618, Val Loss: 0.4515\n",
      "Epoch [11/1000], Loss: 0.4905, Val Loss: 0.4491\n",
      "Epoch [12/1000], Loss: 0.6978, Val Loss: 0.4540\n",
      "Epoch [13/1000], Loss: 0.0732, Val Loss: 0.4512\n",
      "Epoch [14/1000], Loss: 0.0046, Val Loss: 0.4609\n",
      "Epoch [15/1000], Loss: 1.4221, Val Loss: 0.4491\n",
      "Epoch [16/1000], Loss: 0.3426, Val Loss: 0.4527\n",
      "Epoch [17/1000], Loss: 0.0215, Val Loss: 0.4547\n",
      "Epoch [18/1000], Loss: 0.5871, Val Loss: 0.4628\n",
      "Epoch [19/1000], Loss: 0.1722, Val Loss: 0.4504\n",
      "Epoch [20/1000], Loss: 0.1724, Val Loss: 0.4531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:19:07,452] Trial 48 finished with value: 0.8190199686985425 and parameters: {'hidden_size': 22, 'dropout_prob': 0.4262613954958661, 'learning_rate': 0.006075597240540905, 'weight_decay': 6.660714616186184e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 21\n",
      "Epoch [1/1000], Loss: 0.3630, Val Loss: 0.4891\n",
      "Epoch [2/1000], Loss: 0.7572, Val Loss: 0.4697\n",
      "Epoch [3/1000], Loss: 0.0610, Val Loss: 0.4602\n",
      "Epoch [4/1000], Loss: 0.7332, Val Loss: 0.4576\n",
      "Epoch [5/1000], Loss: 0.4056, Val Loss: 0.4557\n",
      "Epoch [6/1000], Loss: 0.0309, Val Loss: 0.4544\n",
      "Epoch [7/1000], Loss: 0.7199, Val Loss: 0.4525\n",
      "Epoch [8/1000], Loss: 0.4275, Val Loss: 0.4521\n",
      "Epoch [9/1000], Loss: 0.6668, Val Loss: 0.4514\n",
      "Epoch [10/1000], Loss: 0.0048, Val Loss: 0.4508\n",
      "Epoch [11/1000], Loss: 0.8374, Val Loss: 0.4522\n",
      "Epoch [12/1000], Loss: 0.1389, Val Loss: 0.4518\n",
      "Epoch [13/1000], Loss: 0.0145, Val Loss: 0.4507\n",
      "Epoch [14/1000], Loss: 0.3687, Val Loss: 0.4526\n",
      "Epoch [15/1000], Loss: 0.7527, Val Loss: 0.4523\n",
      "Epoch [16/1000], Loss: 0.6141, Val Loss: 0.4528\n",
      "Epoch [17/1000], Loss: 0.6897, Val Loss: 0.4510\n",
      "Epoch [18/1000], Loss: 0.0565, Val Loss: 0.4500\n",
      "Epoch [19/1000], Loss: 0.5062, Val Loss: 0.4488\n",
      "Epoch [20/1000], Loss: 0.0041, Val Loss: 0.4496\n",
      "Epoch [21/1000], Loss: 1.1428, Val Loss: 0.4502\n",
      "Epoch [22/1000], Loss: 0.0332, Val Loss: 0.4552\n",
      "Epoch [23/1000], Loss: 0.4805, Val Loss: 0.4512\n",
      "Epoch [24/1000], Loss: 0.5543, Val Loss: 0.4512\n",
      "Epoch [25/1000], Loss: 0.1981, Val Loss: 0.4501\n",
      "Epoch [26/1000], Loss: 0.2867, Val Loss: 0.4494\n",
      "Epoch [27/1000], Loss: 0.0405, Val Loss: 0.4504\n",
      "Epoch [28/1000], Loss: 0.7783, Val Loss: 0.4495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:19:15,692] Trial 49 finished with value: 0.8194889807654073 and parameters: {'hidden_size': 59, 'dropout_prob': 0.7449479086020069, 'learning_rate': 0.0012974369633620927, 'weight_decay': 5.6167242451780406e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 29\n",
      "Epoch [1/1000], Loss: 0.1772, Val Loss: 0.4788\n",
      "Epoch [2/1000], Loss: 0.4907, Val Loss: 0.4646\n",
      "Epoch [3/1000], Loss: 0.5511, Val Loss: 0.4578\n",
      "Epoch [4/1000], Loss: 0.8058, Val Loss: 0.4544\n",
      "Epoch [5/1000], Loss: 0.1811, Val Loss: 0.4509\n",
      "Epoch [6/1000], Loss: 0.2429, Val Loss: 0.4490\n",
      "Epoch [7/1000], Loss: 0.0480, Val Loss: 0.4504\n",
      "Epoch [8/1000], Loss: 0.8136, Val Loss: 0.4491\n",
      "Epoch [9/1000], Loss: 0.2716, Val Loss: 0.4526\n",
      "Epoch [10/1000], Loss: 0.7531, Val Loss: 0.4511\n",
      "Epoch [11/1000], Loss: 3.0629, Val Loss: 0.4504\n",
      "Epoch [12/1000], Loss: 0.4784, Val Loss: 0.4502\n",
      "Epoch [13/1000], Loss: 0.4179, Val Loss: 0.4489\n",
      "Epoch [14/1000], Loss: 0.5892, Val Loss: 0.4481\n",
      "Epoch [15/1000], Loss: 0.4198, Val Loss: 0.4544\n",
      "Epoch [16/1000], Loss: 0.6141, Val Loss: 0.4523\n",
      "Epoch [17/1000], Loss: 0.2581, Val Loss: 0.4517\n",
      "Epoch [18/1000], Loss: 0.0255, Val Loss: 0.4572\n",
      "Epoch [19/1000], Loss: 0.3312, Val Loss: 0.4597\n",
      "Epoch [20/1000], Loss: 0.0035, Val Loss: 0.4523\n",
      "Epoch [21/1000], Loss: 0.0056, Val Loss: 0.4531\n",
      "Epoch [22/1000], Loss: 0.0320, Val Loss: 0.4506\n",
      "Epoch [23/1000], Loss: 0.5484, Val Loss: 0.4560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:19:22,224] Trial 50 finished with value: 0.8208361159683316 and parameters: {'hidden_size': 33, 'dropout_prob': 0.6273697936688367, 'learning_rate': 0.0023670751599458315, 'weight_decay': 4.60474114336713e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 24\n",
      "Epoch [1/1000], Loss: 0.3350, Val Loss: 0.5020\n",
      "Epoch [2/1000], Loss: 0.0910, Val Loss: 0.4751\n",
      "Epoch [3/1000], Loss: 0.1147, Val Loss: 0.4626\n",
      "Epoch [4/1000], Loss: 0.8848, Val Loss: 0.4584\n",
      "Epoch [5/1000], Loss: 1.3307, Val Loss: 0.4534\n",
      "Epoch [6/1000], Loss: 0.0485, Val Loss: 0.4515\n",
      "Epoch [7/1000], Loss: 0.7981, Val Loss: 0.4508\n",
      "Epoch [8/1000], Loss: 0.3745, Val Loss: 0.4514\n",
      "Epoch [9/1000], Loss: 0.1544, Val Loss: 0.4499\n",
      "Epoch [10/1000], Loss: 0.9318, Val Loss: 0.4509\n",
      "Epoch [11/1000], Loss: 0.8911, Val Loss: 0.4480\n",
      "Epoch [12/1000], Loss: 0.0835, Val Loss: 0.4479\n",
      "Epoch [13/1000], Loss: 0.3757, Val Loss: 0.4471\n",
      "Epoch [14/1000], Loss: 0.3732, Val Loss: 0.4484\n",
      "Epoch [15/1000], Loss: 0.5504, Val Loss: 0.4466\n",
      "Epoch [16/1000], Loss: 1.1768, Val Loss: 0.4462\n",
      "Epoch [17/1000], Loss: 0.0727, Val Loss: 0.4464\n",
      "Epoch [18/1000], Loss: 0.2965, Val Loss: 0.4473\n",
      "Epoch [19/1000], Loss: 1.0171, Val Loss: 0.4475\n",
      "Epoch [20/1000], Loss: 1.3875, Val Loss: 0.4473\n",
      "Epoch [21/1000], Loss: 0.6492, Val Loss: 0.4482\n",
      "Epoch [22/1000], Loss: 0.0341, Val Loss: 0.4491\n",
      "Epoch [23/1000], Loss: 0.1107, Val Loss: 0.4468\n",
      "Epoch [24/1000], Loss: 2.1520, Val Loss: 0.4481\n",
      "Epoch [25/1000], Loss: 0.0021, Val Loss: 0.4472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:19:29,032] Trial 51 finished with value: 0.8222214858506196 and parameters: {'hidden_size': 74, 'dropout_prob': 0.681288351723266, 'learning_rate': 0.0008421796427187297, 'weight_decay': 9.338396059138951e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 26\n",
      "Epoch [1/1000], Loss: 0.4924, Val Loss: 0.6343\n",
      "Epoch [2/1000], Loss: 0.4648, Val Loss: 0.6303\n",
      "Epoch [3/1000], Loss: 0.8921, Val Loss: 0.6261\n",
      "Epoch [4/1000], Loss: 0.8236, Val Loss: 0.6219\n",
      "Epoch [5/1000], Loss: 0.9485, Val Loss: 0.6176\n",
      "Epoch [6/1000], Loss: 0.5298, Val Loss: 0.6134\n",
      "Epoch [7/1000], Loss: 0.5465, Val Loss: 0.6090\n",
      "Epoch [8/1000], Loss: 0.4368, Val Loss: 0.6044\n",
      "Epoch [9/1000], Loss: 0.4743, Val Loss: 0.5998\n",
      "Epoch [10/1000], Loss: 0.5071, Val Loss: 0.5952\n",
      "Epoch [11/1000], Loss: 0.3908, Val Loss: 0.5906\n",
      "Epoch [12/1000], Loss: 0.4039, Val Loss: 0.5859\n",
      "Epoch [13/1000], Loss: 0.3341, Val Loss: 0.5813\n",
      "Epoch [14/1000], Loss: 1.4418, Val Loss: 0.5768\n",
      "Epoch [15/1000], Loss: 0.5442, Val Loss: 0.5730\n",
      "Epoch [16/1000], Loss: 1.0605, Val Loss: 0.5691\n",
      "Epoch [17/1000], Loss: 0.3812, Val Loss: 0.5653\n",
      "Epoch [18/1000], Loss: 0.4366, Val Loss: 0.5616\n",
      "Epoch [19/1000], Loss: 0.3089, Val Loss: 0.5580\n",
      "Epoch [20/1000], Loss: 1.1639, Val Loss: 0.5546\n",
      "Epoch [21/1000], Loss: 0.4634, Val Loss: 0.5517\n",
      "Epoch [22/1000], Loss: 0.4520, Val Loss: 0.5488\n",
      "Epoch [23/1000], Loss: 0.4391, Val Loss: 0.5460\n",
      "Epoch [24/1000], Loss: 1.4082, Val Loss: 0.5434\n",
      "Epoch [25/1000], Loss: 0.4397, Val Loss: 0.5411\n",
      "Epoch [26/1000], Loss: 0.3971, Val Loss: 0.5388\n",
      "Epoch [27/1000], Loss: 0.4023, Val Loss: 0.5366\n",
      "Epoch [28/1000], Loss: 0.2997, Val Loss: 0.5347\n",
      "Epoch [29/1000], Loss: 0.2150, Val Loss: 0.5328\n",
      "Epoch [30/1000], Loss: 0.0598, Val Loss: 0.5309\n",
      "Epoch [31/1000], Loss: 0.1250, Val Loss: 0.5292\n",
      "Epoch [32/1000], Loss: 0.1673, Val Loss: 0.5276\n",
      "Epoch [33/1000], Loss: 1.0305, Val Loss: 0.5260\n",
      "Epoch [34/1000], Loss: 0.2930, Val Loss: 0.5246\n",
      "Epoch [35/1000], Loss: 0.2609, Val Loss: 0.5232\n",
      "Epoch [36/1000], Loss: 0.4847, Val Loss: 0.5220\n",
      "Epoch [37/1000], Loss: 0.1008, Val Loss: 0.5206\n",
      "Epoch [38/1000], Loss: 1.2446, Val Loss: 0.5194\n",
      "Epoch [39/1000], Loss: 0.4538, Val Loss: 0.5181\n",
      "Epoch [40/1000], Loss: 0.1722, Val Loss: 0.5170\n",
      "Epoch [41/1000], Loss: 0.3319, Val Loss: 0.5158\n",
      "Epoch [42/1000], Loss: 0.4411, Val Loss: 0.5147\n",
      "Epoch [43/1000], Loss: 0.1011, Val Loss: 0.5136\n",
      "Epoch [44/1000], Loss: 0.1766, Val Loss: 0.5127\n",
      "Epoch [45/1000], Loss: 0.3750, Val Loss: 0.5116\n",
      "Epoch [46/1000], Loss: 0.3637, Val Loss: 0.5105\n",
      "Epoch [47/1000], Loss: 0.3540, Val Loss: 0.5095\n",
      "Epoch [48/1000], Loss: 1.1078, Val Loss: 0.5086\n",
      "Epoch [49/1000], Loss: 0.2996, Val Loss: 0.5078\n",
      "Epoch [50/1000], Loss: 0.1654, Val Loss: 0.5068\n",
      "Epoch [51/1000], Loss: 0.1945, Val Loss: 0.5060\n",
      "Epoch [52/1000], Loss: 0.1636, Val Loss: 0.5051\n",
      "Epoch [53/1000], Loss: 0.3866, Val Loss: 0.5042\n",
      "Epoch [54/1000], Loss: 0.4706, Val Loss: 0.5033\n",
      "Epoch [55/1000], Loss: 0.3418, Val Loss: 0.5025\n",
      "Epoch [56/1000], Loss: 0.0928, Val Loss: 0.5017\n",
      "Epoch [57/1000], Loss: 0.5132, Val Loss: 0.5009\n",
      "Epoch [58/1000], Loss: 0.1584, Val Loss: 0.5001\n",
      "Epoch [59/1000], Loss: 0.4651, Val Loss: 0.4993\n",
      "Epoch [60/1000], Loss: 1.6828, Val Loss: 0.4986\n",
      "Epoch [61/1000], Loss: 1.8790, Val Loss: 0.4978\n",
      "Epoch [62/1000], Loss: 1.0275, Val Loss: 0.4972\n",
      "Epoch [63/1000], Loss: 0.1053, Val Loss: 0.4966\n",
      "Epoch [64/1000], Loss: 0.0682, Val Loss: 0.4959\n",
      "Epoch [65/1000], Loss: 0.0330, Val Loss: 0.4952\n",
      "Epoch [66/1000], Loss: 0.3886, Val Loss: 0.4946\n",
      "Epoch [67/1000], Loss: 0.4746, Val Loss: 0.4939\n",
      "Epoch [68/1000], Loss: 0.0761, Val Loss: 0.4933\n",
      "Epoch [69/1000], Loss: 0.5097, Val Loss: 0.4927\n",
      "Epoch [70/1000], Loss: 0.0191, Val Loss: 0.4921\n",
      "Epoch [71/1000], Loss: 0.0608, Val Loss: 0.4915\n",
      "Epoch [72/1000], Loss: 0.0402, Val Loss: 0.4909\n",
      "Epoch [73/1000], Loss: 0.1419, Val Loss: 0.4903\n",
      "Epoch [74/1000], Loss: 0.3285, Val Loss: 0.4897\n",
      "Epoch [75/1000], Loss: 0.8304, Val Loss: 0.4891\n",
      "Epoch [76/1000], Loss: 0.0394, Val Loss: 0.4886\n",
      "Epoch [77/1000], Loss: 0.3665, Val Loss: 0.4880\n",
      "Epoch [78/1000], Loss: 1.1659, Val Loss: 0.4874\n",
      "Epoch [79/1000], Loss: 0.3195, Val Loss: 0.4869\n",
      "Epoch [80/1000], Loss: 1.0999, Val Loss: 0.4864\n",
      "Epoch [81/1000], Loss: 0.0467, Val Loss: 0.4860\n",
      "Epoch [82/1000], Loss: 0.1138, Val Loss: 0.4855\n",
      "Epoch [83/1000], Loss: 0.2761, Val Loss: 0.4851\n",
      "Epoch [84/1000], Loss: 1.1456, Val Loss: 0.4847\n",
      "Epoch [85/1000], Loss: 0.3846, Val Loss: 0.4843\n",
      "Epoch [86/1000], Loss: 0.8515, Val Loss: 0.4838\n",
      "Epoch [87/1000], Loss: 1.0611, Val Loss: 0.4834\n",
      "Epoch [88/1000], Loss: 0.0832, Val Loss: 0.4830\n",
      "Epoch [89/1000], Loss: 0.0915, Val Loss: 0.4826\n",
      "Epoch [90/1000], Loss: 0.0428, Val Loss: 0.4822\n",
      "Epoch [91/1000], Loss: 0.9334, Val Loss: 0.4818\n",
      "Epoch [92/1000], Loss: 0.5855, Val Loss: 0.4815\n",
      "Epoch [93/1000], Loss: 0.5561, Val Loss: 0.4810\n",
      "Epoch [94/1000], Loss: 0.2859, Val Loss: 0.4807\n",
      "Epoch [95/1000], Loss: 0.7776, Val Loss: 0.4804\n",
      "Epoch [96/1000], Loss: 0.3917, Val Loss: 0.4801\n",
      "Epoch [97/1000], Loss: 1.0866, Val Loss: 0.4797\n",
      "Epoch [98/1000], Loss: 0.1837, Val Loss: 0.4794\n",
      "Epoch [99/1000], Loss: 0.0803, Val Loss: 0.4791\n",
      "Epoch [100/1000], Loss: 0.3560, Val Loss: 0.4787\n",
      "Epoch [101/1000], Loss: 0.8473, Val Loss: 0.4785\n",
      "Epoch [102/1000], Loss: 0.2880, Val Loss: 0.4783\n",
      "Epoch [103/1000], Loss: 0.1158, Val Loss: 0.4780\n",
      "Epoch [104/1000], Loss: 0.1597, Val Loss: 0.4777\n",
      "Epoch [105/1000], Loss: 0.1195, Val Loss: 0.4775\n",
      "Epoch [106/1000], Loss: 0.4336, Val Loss: 0.4772\n",
      "Epoch [107/1000], Loss: 0.4152, Val Loss: 0.4769\n",
      "Epoch [108/1000], Loss: 0.1325, Val Loss: 0.4766\n",
      "Epoch [109/1000], Loss: 0.0185, Val Loss: 0.4764\n",
      "Epoch [110/1000], Loss: 0.3650, Val Loss: 0.4761\n",
      "Epoch [111/1000], Loss: 0.7854, Val Loss: 0.4759\n",
      "Epoch [112/1000], Loss: 1.0608, Val Loss: 0.4757\n",
      "Epoch [113/1000], Loss: 0.8207, Val Loss: 0.4755\n",
      "Epoch [114/1000], Loss: 0.6008, Val Loss: 0.4753\n",
      "Epoch [115/1000], Loss: 0.2463, Val Loss: 0.4751\n",
      "Epoch [116/1000], Loss: 0.1079, Val Loss: 0.4748\n",
      "Epoch [117/1000], Loss: 0.5145, Val Loss: 0.4746\n",
      "Epoch [118/1000], Loss: 0.0880, Val Loss: 0.4743\n",
      "Epoch [119/1000], Loss: 1.1368, Val Loss: 0.4742\n",
      "Epoch [120/1000], Loss: 0.0211, Val Loss: 0.4740\n",
      "Epoch [121/1000], Loss: 0.5681, Val Loss: 0.4738\n",
      "Epoch [122/1000], Loss: 0.0804, Val Loss: 0.4736\n",
      "Epoch [123/1000], Loss: 0.1156, Val Loss: 0.4734\n",
      "Epoch [124/1000], Loss: 0.4970, Val Loss: 0.4732\n",
      "Epoch [125/1000], Loss: 0.3185, Val Loss: 0.4731\n",
      "Epoch [126/1000], Loss: 0.4291, Val Loss: 0.4729\n",
      "Epoch [127/1000], Loss: 2.2242, Val Loss: 0.4727\n",
      "Epoch [128/1000], Loss: 0.8092, Val Loss: 0.4726\n",
      "Epoch [129/1000], Loss: 0.1000, Val Loss: 0.4725\n",
      "Epoch [130/1000], Loss: 0.0486, Val Loss: 0.4723\n",
      "Epoch [131/1000], Loss: 0.0969, Val Loss: 0.4721\n",
      "Epoch [132/1000], Loss: 0.0786, Val Loss: 0.4720\n",
      "Epoch [133/1000], Loss: 0.9717, Val Loss: 0.4718\n",
      "Epoch [134/1000], Loss: 0.1264, Val Loss: 0.4717\n",
      "Epoch [135/1000], Loss: 0.2285, Val Loss: 0.4715\n",
      "Epoch [136/1000], Loss: 0.0790, Val Loss: 0.4713\n",
      "Epoch [137/1000], Loss: 0.0301, Val Loss: 0.4711\n",
      "Epoch [138/1000], Loss: 0.8411, Val Loss: 0.4710\n",
      "Epoch [139/1000], Loss: 0.1913, Val Loss: 0.4709\n",
      "Epoch [140/1000], Loss: 0.0823, Val Loss: 0.4707\n",
      "Epoch [141/1000], Loss: 0.4928, Val Loss: 0.4706\n",
      "Epoch [142/1000], Loss: 0.6235, Val Loss: 0.4704\n",
      "Epoch [143/1000], Loss: 0.3413, Val Loss: 0.4703\n",
      "Epoch [144/1000], Loss: 0.6298, Val Loss: 0.4701\n",
      "Epoch [145/1000], Loss: 0.1512, Val Loss: 0.4700\n",
      "Epoch [146/1000], Loss: 0.2230, Val Loss: 0.4698\n",
      "Epoch [147/1000], Loss: 0.0989, Val Loss: 0.4697\n",
      "Epoch [148/1000], Loss: 0.7123, Val Loss: 0.4696\n",
      "Epoch [149/1000], Loss: 0.0543, Val Loss: 0.4695\n",
      "Epoch [150/1000], Loss: 0.0167, Val Loss: 0.4694\n",
      "Epoch [151/1000], Loss: 0.7406, Val Loss: 0.4692\n",
      "Epoch [152/1000], Loss: 0.6074, Val Loss: 0.4691\n",
      "Epoch [153/1000], Loss: 0.5817, Val Loss: 0.4690\n",
      "Epoch [154/1000], Loss: 0.4116, Val Loss: 0.4689\n",
      "Epoch [155/1000], Loss: 0.0611, Val Loss: 0.4688\n",
      "Epoch [156/1000], Loss: 0.2635, Val Loss: 0.4686\n",
      "Epoch [157/1000], Loss: 0.1756, Val Loss: 0.4685\n",
      "Epoch [158/1000], Loss: 0.5684, Val Loss: 0.4683\n",
      "Epoch [159/1000], Loss: 0.0069, Val Loss: 0.4682\n",
      "Epoch [160/1000], Loss: 0.0055, Val Loss: 0.4681\n",
      "Epoch [161/1000], Loss: 0.0892, Val Loss: 0.4680\n",
      "Epoch [162/1000], Loss: 0.1756, Val Loss: 0.4679\n",
      "Epoch [163/1000], Loss: 0.2626, Val Loss: 0.4678\n",
      "Epoch [164/1000], Loss: 0.0414, Val Loss: 0.4676\n",
      "Epoch [165/1000], Loss: 0.8508, Val Loss: 0.4675\n",
      "Epoch [166/1000], Loss: 0.7168, Val Loss: 0.4674\n",
      "Epoch [167/1000], Loss: 0.7245, Val Loss: 0.4673\n",
      "Epoch [168/1000], Loss: 0.7358, Val Loss: 0.4673\n",
      "Epoch [169/1000], Loss: 0.0338, Val Loss: 0.4671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 0.0394, Val Loss: 0.4671\n",
      "Epoch [171/1000], Loss: 0.4693, Val Loss: 0.4669\n",
      "Epoch [172/1000], Loss: 0.2908, Val Loss: 0.4668\n",
      "Epoch [173/1000], Loss: 0.7873, Val Loss: 0.4667\n",
      "Epoch [174/1000], Loss: 0.5564, Val Loss: 0.4666\n",
      "Epoch [175/1000], Loss: 0.2885, Val Loss: 0.4665\n",
      "Epoch [176/1000], Loss: 1.8941, Val Loss: 0.4664\n",
      "Epoch [177/1000], Loss: 0.6427, Val Loss: 0.4663\n",
      "Epoch [178/1000], Loss: 0.0040, Val Loss: 0.4662\n",
      "Epoch [179/1000], Loss: 0.0209, Val Loss: 0.4662\n",
      "Epoch [180/1000], Loss: 0.1096, Val Loss: 0.4661\n",
      "Epoch [181/1000], Loss: 0.4404, Val Loss: 0.4660\n",
      "Epoch [182/1000], Loss: 0.0158, Val Loss: 0.4659\n",
      "Epoch [183/1000], Loss: 0.0279, Val Loss: 0.4658\n",
      "Epoch [184/1000], Loss: 0.4745, Val Loss: 0.4657\n",
      "Epoch [185/1000], Loss: 4.2661, Val Loss: 0.4656\n",
      "Epoch [186/1000], Loss: 0.6453, Val Loss: 0.4656\n",
      "Epoch [187/1000], Loss: 0.2759, Val Loss: 0.4655\n",
      "Epoch [188/1000], Loss: 4.0403, Val Loss: 0.4654\n",
      "Epoch [189/1000], Loss: 0.4305, Val Loss: 0.4654\n",
      "Epoch [190/1000], Loss: 3.3338, Val Loss: 0.4653\n",
      "Epoch [191/1000], Loss: 0.7180, Val Loss: 0.4652\n",
      "Epoch [192/1000], Loss: 1.9812, Val Loss: 0.4652\n",
      "Epoch [193/1000], Loss: 0.0055, Val Loss: 0.4651\n",
      "Epoch [194/1000], Loss: 0.0175, Val Loss: 0.4650\n",
      "Epoch [195/1000], Loss: 0.3280, Val Loss: 0.4649\n",
      "Epoch [196/1000], Loss: 0.1042, Val Loss: 0.4648\n",
      "Epoch [197/1000], Loss: 0.0532, Val Loss: 0.4647\n",
      "Epoch [198/1000], Loss: 0.5839, Val Loss: 0.4646\n",
      "Epoch [199/1000], Loss: 0.0716, Val Loss: 0.4646\n",
      "Epoch [200/1000], Loss: 1.0525, Val Loss: 0.4645\n",
      "Epoch [201/1000], Loss: 0.8222, Val Loss: 0.4644\n",
      "Epoch [202/1000], Loss: 0.5626, Val Loss: 0.4644\n",
      "Epoch [203/1000], Loss: 0.0950, Val Loss: 0.4643\n",
      "Epoch [204/1000], Loss: 0.1419, Val Loss: 0.4642\n",
      "Epoch [205/1000], Loss: 3.2999, Val Loss: 0.4641\n",
      "Epoch [206/1000], Loss: 1.3502, Val Loss: 0.4641\n",
      "Epoch [207/1000], Loss: 0.0030, Val Loss: 0.4641\n",
      "Epoch [208/1000], Loss: 0.7309, Val Loss: 0.4640\n",
      "Epoch [209/1000], Loss: 0.1185, Val Loss: 0.4639\n",
      "Epoch [210/1000], Loss: 0.0126, Val Loss: 0.4639\n",
      "Epoch [211/1000], Loss: 0.3211, Val Loss: 0.4638\n",
      "Epoch [212/1000], Loss: 0.6863, Val Loss: 0.4637\n",
      "Epoch [213/1000], Loss: 0.9986, Val Loss: 0.4636\n",
      "Epoch [214/1000], Loss: 0.9522, Val Loss: 0.4635\n",
      "Epoch [215/1000], Loss: 0.3752, Val Loss: 0.4634\n",
      "Epoch [216/1000], Loss: 0.0306, Val Loss: 0.4634\n",
      "Epoch [217/1000], Loss: 0.0554, Val Loss: 0.4633\n",
      "Epoch [218/1000], Loss: 0.0291, Val Loss: 0.4632\n",
      "Epoch [219/1000], Loss: 0.0530, Val Loss: 0.4631\n",
      "Epoch [220/1000], Loss: 0.0217, Val Loss: 0.4631\n",
      "Epoch [221/1000], Loss: 1.7431, Val Loss: 0.4630\n",
      "Epoch [222/1000], Loss: 0.3636, Val Loss: 0.4630\n",
      "Epoch [223/1000], Loss: 0.4804, Val Loss: 0.4629\n",
      "Epoch [224/1000], Loss: 0.3523, Val Loss: 0.4628\n",
      "Epoch [225/1000], Loss: 0.6340, Val Loss: 0.4628\n",
      "Epoch [226/1000], Loss: 0.6636, Val Loss: 0.4627\n",
      "Epoch [227/1000], Loss: 0.0700, Val Loss: 0.4626\n",
      "Epoch [228/1000], Loss: 0.1802, Val Loss: 0.4625\n",
      "Epoch [229/1000], Loss: 0.6487, Val Loss: 0.4625\n",
      "Epoch [230/1000], Loss: 0.5811, Val Loss: 0.4624\n",
      "Epoch [231/1000], Loss: 0.5753, Val Loss: 0.4623\n",
      "Epoch [232/1000], Loss: 0.3130, Val Loss: 0.4622\n",
      "Epoch [233/1000], Loss: 0.1219, Val Loss: 0.4622\n",
      "Epoch [234/1000], Loss: 0.3476, Val Loss: 0.4621\n",
      "Epoch [235/1000], Loss: 2.0367, Val Loss: 0.4620\n",
      "Epoch [236/1000], Loss: 0.2473, Val Loss: 0.4620\n",
      "Epoch [237/1000], Loss: 0.0357, Val Loss: 0.4619\n",
      "Epoch [238/1000], Loss: 0.0079, Val Loss: 0.4618\n",
      "Epoch [239/1000], Loss: 0.2202, Val Loss: 0.4618\n",
      "Epoch [240/1000], Loss: 0.0052, Val Loss: 0.4617\n",
      "Epoch [241/1000], Loss: 0.0423, Val Loss: 0.4616\n",
      "Epoch [242/1000], Loss: 0.0433, Val Loss: 0.4615\n",
      "Epoch [243/1000], Loss: 0.6328, Val Loss: 0.4615\n",
      "Epoch [244/1000], Loss: 0.3074, Val Loss: 0.4614\n",
      "Epoch [245/1000], Loss: 0.0066, Val Loss: 0.4613\n",
      "Epoch [246/1000], Loss: 0.0839, Val Loss: 0.4613\n",
      "Epoch [247/1000], Loss: 0.5698, Val Loss: 0.4612\n",
      "Epoch [248/1000], Loss: 0.4383, Val Loss: 0.4612\n",
      "Epoch [249/1000], Loss: 0.2396, Val Loss: 0.4611\n",
      "Epoch [250/1000], Loss: 0.4704, Val Loss: 0.4610\n",
      "Epoch [251/1000], Loss: 0.1203, Val Loss: 0.4609\n",
      "Epoch [252/1000], Loss: 0.5453, Val Loss: 0.4609\n",
      "Epoch [253/1000], Loss: 0.5231, Val Loss: 0.4608\n",
      "Epoch [254/1000], Loss: 0.4354, Val Loss: 0.4607\n",
      "Epoch [255/1000], Loss: 0.0559, Val Loss: 0.4607\n",
      "Epoch [256/1000], Loss: 0.5344, Val Loss: 0.4606\n",
      "Epoch [257/1000], Loss: 0.0428, Val Loss: 0.4605\n",
      "Epoch [258/1000], Loss: 0.8640, Val Loss: 0.4605\n",
      "Epoch [259/1000], Loss: 0.3156, Val Loss: 0.4604\n",
      "Epoch [260/1000], Loss: 0.4619, Val Loss: 0.4604\n",
      "Epoch [261/1000], Loss: 0.3691, Val Loss: 0.4603\n",
      "Epoch [262/1000], Loss: 1.0467, Val Loss: 0.4602\n",
      "Epoch [263/1000], Loss: 0.1282, Val Loss: 0.4602\n",
      "Epoch [264/1000], Loss: 1.1737, Val Loss: 0.4602\n",
      "Epoch [265/1000], Loss: 0.4810, Val Loss: 0.4601\n",
      "Epoch [266/1000], Loss: 0.7833, Val Loss: 0.4601\n",
      "Epoch [267/1000], Loss: 0.0248, Val Loss: 0.4600\n",
      "Epoch [268/1000], Loss: 0.1802, Val Loss: 0.4600\n",
      "Epoch [269/1000], Loss: 0.1771, Val Loss: 0.4599\n",
      "Epoch [270/1000], Loss: 0.6349, Val Loss: 0.4598\n",
      "Epoch [271/1000], Loss: 0.0390, Val Loss: 0.4598\n",
      "Epoch [272/1000], Loss: 0.5924, Val Loss: 0.4597\n",
      "Epoch [273/1000], Loss: 0.6448, Val Loss: 0.4597\n",
      "Epoch [274/1000], Loss: 0.0182, Val Loss: 0.4596\n",
      "Epoch [275/1000], Loss: 0.0363, Val Loss: 0.4596\n",
      "Epoch [276/1000], Loss: 0.4101, Val Loss: 0.4595\n",
      "Epoch [277/1000], Loss: 0.7029, Val Loss: 0.4594\n",
      "Epoch [278/1000], Loss: 0.1169, Val Loss: 0.4594\n",
      "Epoch [279/1000], Loss: 0.0565, Val Loss: 0.4593\n",
      "Epoch [280/1000], Loss: 2.3766, Val Loss: 0.4592\n",
      "Epoch [281/1000], Loss: 0.8216, Val Loss: 0.4592\n",
      "Epoch [282/1000], Loss: 0.0004, Val Loss: 0.4591\n",
      "Epoch [283/1000], Loss: 0.0689, Val Loss: 0.4590\n",
      "Epoch [284/1000], Loss: 0.1958, Val Loss: 0.4590\n",
      "Epoch [285/1000], Loss: 0.4658, Val Loss: 0.4589\n",
      "Epoch [286/1000], Loss: 0.0150, Val Loss: 0.4588\n",
      "Epoch [287/1000], Loss: 0.0409, Val Loss: 0.4588\n",
      "Epoch [288/1000], Loss: 0.3661, Val Loss: 0.4587\n",
      "Epoch [289/1000], Loss: 0.6607, Val Loss: 0.4587\n",
      "Epoch [290/1000], Loss: 0.0256, Val Loss: 0.4586\n",
      "Epoch [291/1000], Loss: 0.5462, Val Loss: 0.4586\n",
      "Epoch [292/1000], Loss: 0.0253, Val Loss: 0.4585\n",
      "Epoch [293/1000], Loss: 0.5858, Val Loss: 0.4585\n",
      "Epoch [294/1000], Loss: 0.5948, Val Loss: 0.4584\n",
      "Epoch [295/1000], Loss: 0.2195, Val Loss: 0.4583\n",
      "Epoch [296/1000], Loss: 0.0735, Val Loss: 0.4582\n",
      "Epoch [297/1000], Loss: 0.0027, Val Loss: 0.4581\n",
      "Epoch [298/1000], Loss: 1.3269, Val Loss: 0.4581\n",
      "Epoch [299/1000], Loss: 0.1042, Val Loss: 0.4581\n",
      "Epoch [300/1000], Loss: 0.5751, Val Loss: 0.4580\n",
      "Epoch [301/1000], Loss: 0.0054, Val Loss: 0.4579\n",
      "Epoch [302/1000], Loss: 0.0375, Val Loss: 0.4579\n",
      "Epoch [303/1000], Loss: 0.4284, Val Loss: 0.4578\n",
      "Epoch [304/1000], Loss: 0.6577, Val Loss: 0.4578\n",
      "Epoch [305/1000], Loss: 0.4042, Val Loss: 0.4578\n",
      "Epoch [306/1000], Loss: 0.3890, Val Loss: 0.4577\n",
      "Epoch [307/1000], Loss: 0.1459, Val Loss: 0.4576\n",
      "Epoch [308/1000], Loss: 0.0336, Val Loss: 0.4576\n",
      "Epoch [309/1000], Loss: 0.7320, Val Loss: 0.4575\n",
      "Epoch [310/1000], Loss: 2.7552, Val Loss: 0.4575\n",
      "Epoch [311/1000], Loss: 0.3161, Val Loss: 0.4574\n",
      "Epoch [312/1000], Loss: 0.1090, Val Loss: 0.4574\n",
      "Epoch [313/1000], Loss: 0.8744, Val Loss: 0.4573\n",
      "Epoch [314/1000], Loss: 0.0413, Val Loss: 0.4573\n",
      "Epoch [315/1000], Loss: 0.5149, Val Loss: 0.4573\n",
      "Epoch [316/1000], Loss: 0.0879, Val Loss: 0.4572\n",
      "Epoch [317/1000], Loss: 0.7422, Val Loss: 0.4572\n",
      "Epoch [318/1000], Loss: 0.1344, Val Loss: 0.4571\n",
      "Epoch [319/1000], Loss: 0.3554, Val Loss: 0.4571\n",
      "Epoch [320/1000], Loss: 0.1715, Val Loss: 0.4570\n",
      "Epoch [321/1000], Loss: 0.0017, Val Loss: 0.4570\n",
      "Epoch [322/1000], Loss: 0.0003, Val Loss: 0.4569\n",
      "Epoch [323/1000], Loss: 0.6220, Val Loss: 0.4569\n",
      "Epoch [324/1000], Loss: 0.1973, Val Loss: 0.4568\n",
      "Epoch [325/1000], Loss: 0.8243, Val Loss: 0.4568\n",
      "Epoch [326/1000], Loss: 0.2452, Val Loss: 0.4568\n",
      "Epoch [327/1000], Loss: 0.1858, Val Loss: 0.4567\n",
      "Epoch [328/1000], Loss: 0.7852, Val Loss: 0.4567\n",
      "Epoch [329/1000], Loss: 0.6257, Val Loss: 0.4566\n",
      "Epoch [330/1000], Loss: 0.1244, Val Loss: 0.4566\n",
      "Epoch [331/1000], Loss: 0.0124, Val Loss: 0.4565\n",
      "Epoch [332/1000], Loss: 0.0022, Val Loss: 0.4565\n",
      "Epoch [333/1000], Loss: 0.1197, Val Loss: 0.4565\n",
      "Epoch [334/1000], Loss: 0.0846, Val Loss: 0.4565\n",
      "Epoch [335/1000], Loss: 1.9818, Val Loss: 0.4564\n",
      "Epoch [336/1000], Loss: 0.9095, Val Loss: 0.4563\n",
      "Epoch [337/1000], Loss: 0.7986, Val Loss: 0.4563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [338/1000], Loss: 0.1113, Val Loss: 0.4563\n",
      "Epoch [339/1000], Loss: 0.0001, Val Loss: 0.4562\n",
      "Epoch [340/1000], Loss: 0.0495, Val Loss: 0.4562\n",
      "Epoch [341/1000], Loss: 0.2442, Val Loss: 0.4561\n",
      "Epoch [342/1000], Loss: 0.6151, Val Loss: 0.4561\n",
      "Epoch [343/1000], Loss: 0.0338, Val Loss: 0.4560\n",
      "Epoch [344/1000], Loss: 0.7034, Val Loss: 0.4560\n",
      "Epoch [345/1000], Loss: 0.4853, Val Loss: 0.4560\n",
      "Epoch [346/1000], Loss: 0.0558, Val Loss: 0.4559\n",
      "Epoch [347/1000], Loss: 0.0243, Val Loss: 0.4559\n",
      "Epoch [348/1000], Loss: 0.1927, Val Loss: 0.4558\n",
      "Epoch [349/1000], Loss: 0.4513, Val Loss: 0.4558\n",
      "Epoch [350/1000], Loss: 0.1663, Val Loss: 0.4558\n",
      "Epoch [351/1000], Loss: 0.3425, Val Loss: 0.4557\n",
      "Epoch [352/1000], Loss: 0.7290, Val Loss: 0.4557\n",
      "Epoch [353/1000], Loss: 0.3431, Val Loss: 0.4556\n",
      "Epoch [354/1000], Loss: 0.5237, Val Loss: 0.4556\n",
      "Epoch [355/1000], Loss: 0.0967, Val Loss: 0.4556\n",
      "Epoch [356/1000], Loss: 0.1097, Val Loss: 0.4556\n",
      "Epoch [357/1000], Loss: 0.5911, Val Loss: 0.4555\n",
      "Epoch [358/1000], Loss: 1.3746, Val Loss: 0.4555\n",
      "Epoch [359/1000], Loss: 0.1152, Val Loss: 0.4554\n",
      "Epoch [360/1000], Loss: 2.8422, Val Loss: 0.4554\n",
      "Epoch [361/1000], Loss: 0.7451, Val Loss: 0.4554\n",
      "Epoch [362/1000], Loss: 0.1087, Val Loss: 0.4553\n",
      "Epoch [363/1000], Loss: 1.1377, Val Loss: 0.4553\n",
      "Epoch [364/1000], Loss: 1.3659, Val Loss: 0.4552\n",
      "Epoch [365/1000], Loss: 0.2076, Val Loss: 0.4552\n",
      "Epoch [366/1000], Loss: 0.5868, Val Loss: 0.4552\n",
      "Epoch [367/1000], Loss: 0.9630, Val Loss: 0.4552\n",
      "Epoch [368/1000], Loss: 0.6187, Val Loss: 0.4551\n",
      "Epoch [369/1000], Loss: 1.2391, Val Loss: 0.4551\n",
      "Epoch [370/1000], Loss: 0.0416, Val Loss: 0.4550\n",
      "Epoch [371/1000], Loss: 0.8872, Val Loss: 0.4550\n",
      "Epoch [372/1000], Loss: 0.1758, Val Loss: 0.4550\n",
      "Epoch [373/1000], Loss: 0.6525, Val Loss: 0.4549\n",
      "Epoch [374/1000], Loss: 0.4626, Val Loss: 0.4549\n",
      "Epoch [375/1000], Loss: 0.9379, Val Loss: 0.4549\n",
      "Epoch [376/1000], Loss: 0.3436, Val Loss: 0.4548\n",
      "Epoch [377/1000], Loss: 0.5057, Val Loss: 0.4548\n",
      "Epoch [378/1000], Loss: 0.0263, Val Loss: 0.4547\n",
      "Epoch [379/1000], Loss: 0.0298, Val Loss: 0.4547\n",
      "Epoch [380/1000], Loss: 0.0292, Val Loss: 0.4547\n",
      "Epoch [381/1000], Loss: 0.0958, Val Loss: 0.4546\n",
      "Epoch [382/1000], Loss: 1.9825, Val Loss: 0.4546\n",
      "Epoch [383/1000], Loss: 0.3208, Val Loss: 0.4546\n",
      "Epoch [384/1000], Loss: 0.1672, Val Loss: 0.4545\n",
      "Epoch [385/1000], Loss: 2.3024, Val Loss: 0.4545\n",
      "Epoch [386/1000], Loss: 0.6479, Val Loss: 0.4545\n",
      "Epoch [387/1000], Loss: 0.0438, Val Loss: 0.4545\n",
      "Epoch [388/1000], Loss: 0.1203, Val Loss: 0.4544\n",
      "Epoch [389/1000], Loss: 0.1776, Val Loss: 0.4544\n",
      "Epoch [390/1000], Loss: 1.1804, Val Loss: 0.4544\n",
      "Epoch [391/1000], Loss: 0.5814, Val Loss: 0.4544\n",
      "Epoch [392/1000], Loss: 0.8955, Val Loss: 0.4543\n",
      "Epoch [393/1000], Loss: 0.0836, Val Loss: 0.4543\n",
      "Epoch [394/1000], Loss: 0.6830, Val Loss: 0.4543\n",
      "Epoch [395/1000], Loss: 0.5333, Val Loss: 0.4542\n",
      "Epoch [396/1000], Loss: 0.1324, Val Loss: 0.4542\n",
      "Epoch [397/1000], Loss: 0.4979, Val Loss: 0.4542\n",
      "Epoch [398/1000], Loss: 0.1729, Val Loss: 0.4541\n",
      "Epoch [399/1000], Loss: 1.1330, Val Loss: 0.4542\n",
      "Epoch [400/1000], Loss: 1.0522, Val Loss: 0.4541\n",
      "Epoch [401/1000], Loss: 0.1777, Val Loss: 0.4541\n",
      "Epoch [402/1000], Loss: 1.7931, Val Loss: 0.4541\n",
      "Epoch [403/1000], Loss: 0.5202, Val Loss: 0.4541\n",
      "Epoch [404/1000], Loss: 0.0436, Val Loss: 0.4540\n",
      "Epoch [405/1000], Loss: 0.0040, Val Loss: 0.4540\n",
      "Epoch [406/1000], Loss: 0.1032, Val Loss: 0.4540\n",
      "Epoch [407/1000], Loss: 0.4085, Val Loss: 0.4539\n",
      "Epoch [408/1000], Loss: 0.0195, Val Loss: 0.4539\n",
      "Epoch [409/1000], Loss: 0.4545, Val Loss: 0.4538\n",
      "Epoch [410/1000], Loss: 0.0427, Val Loss: 0.4538\n",
      "Epoch [411/1000], Loss: 0.8203, Val Loss: 0.4538\n",
      "Epoch [412/1000], Loss: 0.6637, Val Loss: 0.4538\n",
      "Epoch [413/1000], Loss: 1.0330, Val Loss: 0.4537\n",
      "Epoch [414/1000], Loss: 0.1716, Val Loss: 0.4537\n",
      "Epoch [415/1000], Loss: 0.5501, Val Loss: 0.4537\n",
      "Epoch [416/1000], Loss: 0.4026, Val Loss: 0.4536\n",
      "Epoch [417/1000], Loss: 0.4918, Val Loss: 0.4536\n",
      "Epoch [418/1000], Loss: 2.3937, Val Loss: 0.4536\n",
      "Epoch [419/1000], Loss: 0.6978, Val Loss: 0.4536\n",
      "Epoch [420/1000], Loss: 0.6351, Val Loss: 0.4535\n",
      "Epoch [421/1000], Loss: 0.5954, Val Loss: 0.4535\n",
      "Epoch [422/1000], Loss: 0.0114, Val Loss: 0.4534\n",
      "Epoch [423/1000], Loss: 0.0582, Val Loss: 0.4534\n",
      "Epoch [424/1000], Loss: 0.5377, Val Loss: 0.4534\n",
      "Epoch [425/1000], Loss: 0.5474, Val Loss: 0.4533\n",
      "Epoch [426/1000], Loss: 0.5838, Val Loss: 0.4533\n",
      "Epoch [427/1000], Loss: 1.0086, Val Loss: 0.4533\n",
      "Epoch [428/1000], Loss: 0.2128, Val Loss: 0.4532\n",
      "Epoch [429/1000], Loss: 0.7959, Val Loss: 0.4532\n",
      "Epoch [430/1000], Loss: 0.9341, Val Loss: 0.4532\n",
      "Epoch [431/1000], Loss: 0.6642, Val Loss: 0.4531\n",
      "Epoch [432/1000], Loss: 1.4977, Val Loss: 0.4531\n",
      "Epoch [433/1000], Loss: 0.0440, Val Loss: 0.4531\n",
      "Epoch [434/1000], Loss: 0.6822, Val Loss: 0.4531\n",
      "Epoch [435/1000], Loss: 0.4980, Val Loss: 0.4530\n",
      "Epoch [436/1000], Loss: 1.2011, Val Loss: 0.4530\n",
      "Epoch [437/1000], Loss: 0.6463, Val Loss: 0.4530\n",
      "Epoch [438/1000], Loss: 0.4803, Val Loss: 0.4529\n",
      "Epoch [439/1000], Loss: 0.3087, Val Loss: 0.4529\n",
      "Epoch [440/1000], Loss: 0.2180, Val Loss: 0.4528\n",
      "Epoch [441/1000], Loss: 0.0929, Val Loss: 0.4528\n",
      "Epoch [442/1000], Loss: 0.0168, Val Loss: 0.4528\n",
      "Epoch [443/1000], Loss: 0.6704, Val Loss: 0.4528\n",
      "Epoch [444/1000], Loss: 0.7705, Val Loss: 0.4527\n",
      "Epoch [445/1000], Loss: 0.3140, Val Loss: 0.4527\n",
      "Epoch [446/1000], Loss: 0.5909, Val Loss: 0.4527\n",
      "Epoch [447/1000], Loss: 0.8757, Val Loss: 0.4527\n",
      "Epoch [448/1000], Loss: 0.6490, Val Loss: 0.4527\n",
      "Epoch [449/1000], Loss: 0.1256, Val Loss: 0.4526\n",
      "Epoch [450/1000], Loss: 0.0475, Val Loss: 0.4526\n",
      "Epoch [451/1000], Loss: 0.0193, Val Loss: 0.4526\n",
      "Epoch [452/1000], Loss: 0.0203, Val Loss: 0.4525\n",
      "Epoch [453/1000], Loss: 0.4148, Val Loss: 0.4525\n",
      "Epoch [454/1000], Loss: 0.3276, Val Loss: 0.4525\n",
      "Epoch [455/1000], Loss: 0.6733, Val Loss: 0.4525\n",
      "Epoch [456/1000], Loss: 0.0157, Val Loss: 0.4524\n",
      "Epoch [457/1000], Loss: 0.0461, Val Loss: 0.4524\n",
      "Epoch [458/1000], Loss: 0.4158, Val Loss: 0.4524\n",
      "Epoch [459/1000], Loss: 0.0820, Val Loss: 0.4523\n",
      "Epoch [460/1000], Loss: 0.1452, Val Loss: 0.4523\n",
      "Epoch [461/1000], Loss: 0.0907, Val Loss: 0.4522\n",
      "Epoch [462/1000], Loss: 1.0904, Val Loss: 0.4522\n",
      "Epoch [463/1000], Loss: 0.0934, Val Loss: 0.4522\n",
      "Epoch [464/1000], Loss: 0.5502, Val Loss: 0.4522\n",
      "Epoch [465/1000], Loss: 0.0265, Val Loss: 0.4521\n",
      "Epoch [466/1000], Loss: 0.9120, Val Loss: 0.4521\n",
      "Epoch [467/1000], Loss: 0.6117, Val Loss: 0.4521\n",
      "Epoch [468/1000], Loss: 0.1454, Val Loss: 0.4521\n",
      "Epoch [469/1000], Loss: 0.0630, Val Loss: 0.4520\n",
      "Epoch [470/1000], Loss: 0.5384, Val Loss: 0.4520\n",
      "Epoch [471/1000], Loss: 0.0457, Val Loss: 0.4520\n",
      "Epoch [472/1000], Loss: 0.2973, Val Loss: 0.4519\n",
      "Epoch [473/1000], Loss: 1.2312, Val Loss: 0.4520\n",
      "Epoch [474/1000], Loss: 0.0197, Val Loss: 0.4520\n",
      "Epoch [475/1000], Loss: 0.0167, Val Loss: 0.4519\n",
      "Epoch [476/1000], Loss: 0.6106, Val Loss: 0.4519\n",
      "Epoch [477/1000], Loss: 0.3101, Val Loss: 0.4519\n",
      "Epoch [478/1000], Loss: 0.8458, Val Loss: 0.4519\n",
      "Epoch [479/1000], Loss: 0.0177, Val Loss: 0.4519\n",
      "Epoch [480/1000], Loss: 0.0056, Val Loss: 0.4518\n",
      "Epoch [481/1000], Loss: 0.8687, Val Loss: 0.4518\n",
      "Epoch [482/1000], Loss: 0.7466, Val Loss: 0.4518\n",
      "Epoch [483/1000], Loss: 0.0040, Val Loss: 0.4518\n",
      "Epoch [484/1000], Loss: 0.1460, Val Loss: 0.4518\n",
      "Epoch [485/1000], Loss: 0.0393, Val Loss: 0.4517\n",
      "Epoch [486/1000], Loss: 0.0796, Val Loss: 0.4517\n",
      "Epoch [487/1000], Loss: 0.0812, Val Loss: 0.4517\n",
      "Epoch [488/1000], Loss: 0.7833, Val Loss: 0.4517\n",
      "Epoch [489/1000], Loss: 0.0865, Val Loss: 0.4516\n",
      "Epoch [490/1000], Loss: 0.5378, Val Loss: 0.4516\n",
      "Epoch [491/1000], Loss: 1.6800, Val Loss: 0.4516\n",
      "Epoch [492/1000], Loss: 0.5812, Val Loss: 0.4516\n",
      "Epoch [493/1000], Loss: 0.8403, Val Loss: 0.4515\n",
      "Epoch [494/1000], Loss: 2.6028, Val Loss: 0.4515\n",
      "Epoch [495/1000], Loss: 1.6386, Val Loss: 0.4515\n",
      "Epoch [496/1000], Loss: 0.0474, Val Loss: 0.4515\n",
      "Epoch [497/1000], Loss: 0.0731, Val Loss: 0.4514\n",
      "Epoch [498/1000], Loss: 0.0039, Val Loss: 0.4515\n",
      "Epoch [499/1000], Loss: 0.0010, Val Loss: 0.4514\n",
      "Epoch [500/1000], Loss: 0.6864, Val Loss: 0.4514\n",
      "Epoch [501/1000], Loss: 1.0337, Val Loss: 0.4514\n",
      "Epoch [502/1000], Loss: 2.2871, Val Loss: 0.4514\n",
      "Epoch [503/1000], Loss: 0.2216, Val Loss: 0.4514\n",
      "Epoch [504/1000], Loss: 0.2583, Val Loss: 0.4514\n",
      "Epoch [505/1000], Loss: 0.2932, Val Loss: 0.4513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [506/1000], Loss: 0.0841, Val Loss: 0.4513\n",
      "Epoch [507/1000], Loss: 0.0921, Val Loss: 0.4513\n",
      "Epoch [508/1000], Loss: 0.0919, Val Loss: 0.4513\n",
      "Epoch [509/1000], Loss: 0.3248, Val Loss: 0.4512\n",
      "Epoch [510/1000], Loss: 0.8983, Val Loss: 0.4512\n",
      "Epoch [511/1000], Loss: 0.0411, Val Loss: 0.4512\n",
      "Epoch [512/1000], Loss: 3.1509, Val Loss: 0.4512\n",
      "Epoch [513/1000], Loss: 0.6296, Val Loss: 0.4512\n",
      "Epoch [514/1000], Loss: 1.3773, Val Loss: 0.4512\n",
      "Epoch [515/1000], Loss: 0.6818, Val Loss: 0.4511\n",
      "Epoch [516/1000], Loss: 0.6671, Val Loss: 0.4511\n",
      "Epoch [517/1000], Loss: 0.0463, Val Loss: 0.4511\n",
      "Epoch [518/1000], Loss: 0.6765, Val Loss: 0.4511\n",
      "Epoch [519/1000], Loss: 0.6047, Val Loss: 0.4511\n",
      "Epoch [520/1000], Loss: 0.1849, Val Loss: 0.4510\n",
      "Epoch [521/1000], Loss: 0.0158, Val Loss: 0.4510\n",
      "Epoch [522/1000], Loss: 0.0271, Val Loss: 0.4510\n",
      "Epoch [523/1000], Loss: 1.7140, Val Loss: 0.4510\n",
      "Epoch [524/1000], Loss: 0.8591, Val Loss: 0.4511\n",
      "Epoch [525/1000], Loss: 0.4079, Val Loss: 0.4510\n",
      "Epoch [526/1000], Loss: 0.3542, Val Loss: 0.4510\n",
      "Epoch [527/1000], Loss: 0.0674, Val Loss: 0.4510\n",
      "Epoch [528/1000], Loss: 0.0676, Val Loss: 0.4510\n",
      "Epoch [529/1000], Loss: 0.2034, Val Loss: 0.4510\n",
      "Epoch [530/1000], Loss: 0.6415, Val Loss: 0.4510\n",
      "Epoch [531/1000], Loss: 0.5223, Val Loss: 0.4509\n",
      "Epoch [532/1000], Loss: 0.1274, Val Loss: 0.4509\n",
      "Epoch [533/1000], Loss: 0.3718, Val Loss: 0.4509\n",
      "Epoch [534/1000], Loss: 0.0228, Val Loss: 0.4509\n",
      "Epoch [535/1000], Loss: 0.4240, Val Loss: 0.4508\n",
      "Epoch [536/1000], Loss: 0.1269, Val Loss: 0.4508\n",
      "Epoch [537/1000], Loss: 0.3403, Val Loss: 0.4508\n",
      "Epoch [538/1000], Loss: 0.0264, Val Loss: 0.4508\n",
      "Epoch [539/1000], Loss: 1.1786, Val Loss: 0.4508\n",
      "Epoch [540/1000], Loss: 0.0972, Val Loss: 0.4508\n",
      "Epoch [541/1000], Loss: 0.5152, Val Loss: 0.4508\n",
      "Epoch [542/1000], Loss: 0.0004, Val Loss: 0.4507\n",
      "Epoch [543/1000], Loss: 0.5874, Val Loss: 0.4507\n",
      "Epoch [544/1000], Loss: 0.0438, Val Loss: 0.4507\n",
      "Epoch [545/1000], Loss: 0.6228, Val Loss: 0.4507\n",
      "Epoch [546/1000], Loss: 1.2949, Val Loss: 0.4507\n",
      "Epoch [547/1000], Loss: 0.1685, Val Loss: 0.4507\n",
      "Epoch [548/1000], Loss: 0.9867, Val Loss: 0.4506\n",
      "Epoch [549/1000], Loss: 0.1566, Val Loss: 0.4506\n",
      "Epoch [550/1000], Loss: 0.7660, Val Loss: 0.4506\n",
      "Epoch [551/1000], Loss: 0.0011, Val Loss: 0.4506\n",
      "Epoch [552/1000], Loss: 0.0064, Val Loss: 0.4505\n",
      "Epoch [553/1000], Loss: 0.1593, Val Loss: 0.4505\n",
      "Epoch [554/1000], Loss: 0.3748, Val Loss: 0.4505\n",
      "Epoch [555/1000], Loss: 0.9356, Val Loss: 0.4505\n",
      "Epoch [556/1000], Loss: 0.2272, Val Loss: 0.4504\n",
      "Epoch [557/1000], Loss: 1.6042, Val Loss: 0.4504\n",
      "Epoch [558/1000], Loss: 0.2749, Val Loss: 0.4504\n",
      "Epoch [559/1000], Loss: 0.1664, Val Loss: 0.4504\n",
      "Epoch [560/1000], Loss: 0.8978, Val Loss: 0.4504\n",
      "Epoch [561/1000], Loss: 0.2227, Val Loss: 0.4503\n",
      "Epoch [562/1000], Loss: 0.0619, Val Loss: 0.4503\n",
      "Epoch [563/1000], Loss: 0.5576, Val Loss: 0.4503\n",
      "Epoch [564/1000], Loss: 0.6148, Val Loss: 0.4503\n",
      "Epoch [565/1000], Loss: 0.6913, Val Loss: 0.4502\n",
      "Epoch [566/1000], Loss: 0.4383, Val Loss: 0.4502\n",
      "Epoch [567/1000], Loss: 0.2309, Val Loss: 0.4502\n",
      "Epoch [568/1000], Loss: 0.2755, Val Loss: 0.4502\n",
      "Epoch [569/1000], Loss: 0.1001, Val Loss: 0.4502\n",
      "Epoch [570/1000], Loss: 0.0337, Val Loss: 0.4502\n",
      "Epoch [571/1000], Loss: 0.6413, Val Loss: 0.4502\n",
      "Epoch [572/1000], Loss: 0.0940, Val Loss: 0.4502\n",
      "Epoch [573/1000], Loss: 0.2274, Val Loss: 0.4501\n",
      "Epoch [574/1000], Loss: 0.4988, Val Loss: 0.4501\n",
      "Epoch [575/1000], Loss: 0.4861, Val Loss: 0.4501\n",
      "Epoch [576/1000], Loss: 0.7242, Val Loss: 0.4501\n",
      "Epoch [577/1000], Loss: 0.1126, Val Loss: 0.4501\n",
      "Epoch [578/1000], Loss: 0.0623, Val Loss: 0.4501\n",
      "Epoch [579/1000], Loss: 0.6585, Val Loss: 0.4501\n",
      "Epoch [580/1000], Loss: 0.0204, Val Loss: 0.4501\n",
      "Epoch [581/1000], Loss: 0.0431, Val Loss: 0.4500\n",
      "Epoch [582/1000], Loss: 1.1047, Val Loss: 0.4500\n",
      "Epoch [583/1000], Loss: 0.1487, Val Loss: 0.4500\n",
      "Epoch [584/1000], Loss: 0.3748, Val Loss: 0.4500\n",
      "Epoch [585/1000], Loss: 0.0348, Val Loss: 0.4499\n",
      "Epoch [586/1000], Loss: 0.0576, Val Loss: 0.4499\n",
      "Epoch [587/1000], Loss: 1.0493, Val Loss: 0.4499\n",
      "Epoch [588/1000], Loss: 3.1149, Val Loss: 0.4500\n",
      "Epoch [589/1000], Loss: 0.3349, Val Loss: 0.4500\n",
      "Epoch [590/1000], Loss: 0.7141, Val Loss: 0.4500\n",
      "Epoch [591/1000], Loss: 0.8807, Val Loss: 0.4499\n",
      "Epoch [592/1000], Loss: 0.6618, Val Loss: 0.4499\n",
      "Epoch [593/1000], Loss: 0.0725, Val Loss: 0.4499\n",
      "Epoch [594/1000], Loss: 0.0525, Val Loss: 0.4499\n",
      "Epoch [595/1000], Loss: 0.1210, Val Loss: 0.4499\n",
      "Epoch [596/1000], Loss: 0.1881, Val Loss: 0.4499\n",
      "Epoch [597/1000], Loss: 0.0237, Val Loss: 0.4499\n",
      "Epoch [598/1000], Loss: 0.7800, Val Loss: 0.4498\n",
      "Epoch [599/1000], Loss: 0.8808, Val Loss: 0.4498\n",
      "Epoch [600/1000], Loss: 0.0046, Val Loss: 0.4498\n",
      "Epoch [601/1000], Loss: 2.0243, Val Loss: 0.4498\n",
      "Epoch [602/1000], Loss: 0.0003, Val Loss: 0.4498\n",
      "Epoch [603/1000], Loss: 0.4512, Val Loss: 0.4498\n",
      "Epoch [604/1000], Loss: 0.1750, Val Loss: 0.4498\n",
      "Epoch [605/1000], Loss: 0.0078, Val Loss: 0.4498\n",
      "Epoch [606/1000], Loss: 0.3309, Val Loss: 0.4498\n",
      "Epoch [607/1000], Loss: 0.0024, Val Loss: 0.4498\n",
      "Epoch [608/1000], Loss: 0.8841, Val Loss: 0.4498\n",
      "Epoch [609/1000], Loss: 1.0734, Val Loss: 0.4497\n",
      "Epoch [610/1000], Loss: 1.5126, Val Loss: 0.4497\n",
      "Epoch [611/1000], Loss: 0.0885, Val Loss: 0.4497\n",
      "Epoch [612/1000], Loss: 0.2381, Val Loss: 0.4498\n",
      "Epoch [613/1000], Loss: 0.0021, Val Loss: 0.4497\n",
      "Epoch [614/1000], Loss: 0.1206, Val Loss: 0.4497\n",
      "Epoch [615/1000], Loss: 0.7809, Val Loss: 0.4496\n",
      "Epoch [616/1000], Loss: 0.0184, Val Loss: 0.4496\n",
      "Epoch [617/1000], Loss: 1.2094, Val Loss: 0.4496\n",
      "Epoch [618/1000], Loss: 0.5408, Val Loss: 0.4496\n",
      "Epoch [619/1000], Loss: 0.2185, Val Loss: 0.4496\n",
      "Epoch [620/1000], Loss: 0.7844, Val Loss: 0.4496\n",
      "Epoch [621/1000], Loss: 0.8482, Val Loss: 0.4495\n",
      "Epoch [622/1000], Loss: 1.4629, Val Loss: 0.4495\n",
      "Epoch [623/1000], Loss: 1.3281, Val Loss: 0.4495\n",
      "Epoch [624/1000], Loss: 0.1152, Val Loss: 0.4496\n",
      "Epoch [625/1000], Loss: 0.6862, Val Loss: 0.4496\n",
      "Epoch [626/1000], Loss: 0.0547, Val Loss: 0.4496\n",
      "Epoch [627/1000], Loss: 0.6665, Val Loss: 0.4495\n",
      "Epoch [628/1000], Loss: 1.2109, Val Loss: 0.4495\n",
      "Epoch [629/1000], Loss: 0.7696, Val Loss: 0.4495\n",
      "Epoch [630/1000], Loss: 0.1198, Val Loss: 0.4495\n",
      "Epoch [631/1000], Loss: 0.7355, Val Loss: 0.4495\n",
      "Epoch [632/1000], Loss: 0.0117, Val Loss: 0.4495\n",
      "Epoch [633/1000], Loss: 0.1067, Val Loss: 0.4495\n",
      "Epoch [634/1000], Loss: 0.2112, Val Loss: 0.4495\n",
      "Epoch [635/1000], Loss: 0.5720, Val Loss: 0.4495\n",
      "Epoch [636/1000], Loss: 0.1021, Val Loss: 0.4495\n",
      "Epoch [637/1000], Loss: 0.3455, Val Loss: 0.4495\n",
      "Epoch [638/1000], Loss: 0.3865, Val Loss: 0.4494\n",
      "Epoch [639/1000], Loss: 0.0128, Val Loss: 0.4495\n",
      "Epoch [640/1000], Loss: 3.2966, Val Loss: 0.4495\n",
      "Epoch [641/1000], Loss: 0.3368, Val Loss: 0.4494\n",
      "Epoch [642/1000], Loss: 0.5379, Val Loss: 0.4494\n",
      "Epoch [643/1000], Loss: 0.5359, Val Loss: 0.4494\n",
      "Epoch [644/1000], Loss: 0.3060, Val Loss: 0.4494\n",
      "Epoch [645/1000], Loss: 0.0831, Val Loss: 0.4493\n",
      "Epoch [646/1000], Loss: 0.2642, Val Loss: 0.4493\n",
      "Epoch [647/1000], Loss: 0.0025, Val Loss: 0.4493\n",
      "Epoch [648/1000], Loss: 0.0500, Val Loss: 0.4493\n",
      "Epoch [649/1000], Loss: 0.0594, Val Loss: 0.4492\n",
      "Epoch [650/1000], Loss: 0.0069, Val Loss: 0.4492\n",
      "Epoch [651/1000], Loss: 0.1082, Val Loss: 0.4492\n",
      "Epoch [652/1000], Loss: 1.6847, Val Loss: 0.4492\n",
      "Epoch [653/1000], Loss: 0.0737, Val Loss: 0.4492\n",
      "Epoch [654/1000], Loss: 0.8933, Val Loss: 0.4492\n",
      "Epoch [655/1000], Loss: 2.0331, Val Loss: 0.4491\n",
      "Epoch [656/1000], Loss: 0.3542, Val Loss: 0.4491\n",
      "Epoch [657/1000], Loss: 2.4905, Val Loss: 0.4491\n",
      "Epoch [658/1000], Loss: 1.0242, Val Loss: 0.4491\n",
      "Epoch [659/1000], Loss: 0.6196, Val Loss: 0.4491\n",
      "Epoch [660/1000], Loss: 0.1986, Val Loss: 0.4491\n",
      "Epoch [661/1000], Loss: 0.5775, Val Loss: 0.4490\n",
      "Epoch [662/1000], Loss: 0.3324, Val Loss: 0.4491\n",
      "Epoch [663/1000], Loss: 0.8272, Val Loss: 0.4490\n",
      "Epoch [664/1000], Loss: 0.5479, Val Loss: 0.4490\n",
      "Epoch [665/1000], Loss: 0.0052, Val Loss: 0.4490\n",
      "Epoch [666/1000], Loss: 0.6022, Val Loss: 0.4490\n",
      "Epoch [667/1000], Loss: 0.4656, Val Loss: 0.4490\n",
      "Epoch [668/1000], Loss: 0.1817, Val Loss: 0.4490\n",
      "Epoch [669/1000], Loss: 0.0008, Val Loss: 0.4489\n",
      "Epoch [670/1000], Loss: 0.0295, Val Loss: 0.4489\n",
      "Epoch [671/1000], Loss: 0.2184, Val Loss: 0.4489\n",
      "Epoch [672/1000], Loss: 0.5653, Val Loss: 0.4489\n",
      "Epoch [673/1000], Loss: 0.2670, Val Loss: 0.4489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [674/1000], Loss: 0.0095, Val Loss: 0.4489\n",
      "Epoch [675/1000], Loss: 0.1030, Val Loss: 0.4489\n",
      "Epoch [676/1000], Loss: 1.0222, Val Loss: 0.4489\n",
      "Epoch [677/1000], Loss: 0.6477, Val Loss: 0.4489\n",
      "Epoch [678/1000], Loss: 0.0025, Val Loss: 0.4489\n",
      "Epoch [679/1000], Loss: 0.3356, Val Loss: 0.4489\n",
      "Epoch [680/1000], Loss: 0.0204, Val Loss: 0.4488\n",
      "Epoch [681/1000], Loss: 0.6546, Val Loss: 0.4488\n",
      "Epoch [682/1000], Loss: 0.4413, Val Loss: 0.4488\n",
      "Epoch [683/1000], Loss: 0.3378, Val Loss: 0.4488\n",
      "Epoch [684/1000], Loss: 0.4266, Val Loss: 0.4488\n",
      "Epoch [685/1000], Loss: 0.5238, Val Loss: 0.4488\n",
      "Epoch [686/1000], Loss: 0.4474, Val Loss: 0.4488\n",
      "Epoch [687/1000], Loss: 0.0066, Val Loss: 0.4487\n",
      "Epoch [688/1000], Loss: 0.0588, Val Loss: 0.4487\n",
      "Epoch [689/1000], Loss: 0.1000, Val Loss: 0.4487\n",
      "Epoch [690/1000], Loss: 0.1349, Val Loss: 0.4487\n",
      "Epoch [691/1000], Loss: 1.2518, Val Loss: 0.4487\n",
      "Epoch [692/1000], Loss: 1.4657, Val Loss: 0.4487\n",
      "Epoch [693/1000], Loss: 0.5582, Val Loss: 0.4487\n",
      "Epoch [694/1000], Loss: 0.6283, Val Loss: 0.4487\n",
      "Epoch [695/1000], Loss: 0.2904, Val Loss: 0.4487\n",
      "Epoch [696/1000], Loss: 0.0179, Val Loss: 0.4487\n",
      "Epoch [697/1000], Loss: 0.2880, Val Loss: 0.4487\n",
      "Epoch [698/1000], Loss: 0.0620, Val Loss: 0.4487\n",
      "Epoch [699/1000], Loss: 1.1096, Val Loss: 0.4487\n",
      "Epoch [700/1000], Loss: 0.0576, Val Loss: 0.4487\n",
      "Epoch [701/1000], Loss: 0.0038, Val Loss: 0.4487\n",
      "Epoch [702/1000], Loss: 0.0013, Val Loss: 0.4486\n",
      "Epoch [703/1000], Loss: 0.2011, Val Loss: 0.4486\n",
      "Epoch [704/1000], Loss: 0.8935, Val Loss: 0.4486\n",
      "Epoch [705/1000], Loss: 0.4521, Val Loss: 0.4486\n",
      "Epoch [706/1000], Loss: 0.7842, Val Loss: 0.4486\n",
      "Epoch [707/1000], Loss: 0.4244, Val Loss: 0.4486\n",
      "Epoch [708/1000], Loss: 1.3854, Val Loss: 0.4485\n",
      "Epoch [709/1000], Loss: 0.1624, Val Loss: 0.4486\n",
      "Epoch [710/1000], Loss: 0.3345, Val Loss: 0.4485\n",
      "Epoch [711/1000], Loss: 0.0609, Val Loss: 0.4485\n",
      "Epoch [712/1000], Loss: 0.4006, Val Loss: 0.4485\n",
      "Epoch [713/1000], Loss: 0.6362, Val Loss: 0.4485\n",
      "Epoch [714/1000], Loss: 0.4231, Val Loss: 0.4485\n",
      "Epoch [715/1000], Loss: 0.2983, Val Loss: 0.4485\n",
      "Epoch [716/1000], Loss: 0.7214, Val Loss: 0.4485\n",
      "Epoch [717/1000], Loss: 0.7398, Val Loss: 0.4485\n",
      "Epoch [718/1000], Loss: 0.3544, Val Loss: 0.4485\n",
      "Epoch [719/1000], Loss: 0.1644, Val Loss: 0.4485\n",
      "Epoch [720/1000], Loss: 0.4487, Val Loss: 0.4485\n",
      "Epoch [721/1000], Loss: 0.5580, Val Loss: 0.4485\n",
      "Epoch [722/1000], Loss: 0.9597, Val Loss: 0.4485\n",
      "Epoch [723/1000], Loss: 0.8557, Val Loss: 0.4484\n",
      "Epoch [724/1000], Loss: 0.0785, Val Loss: 0.4484\n",
      "Epoch [725/1000], Loss: 0.6225, Val Loss: 0.4484\n",
      "Epoch [726/1000], Loss: 0.1693, Val Loss: 0.4484\n",
      "Epoch [727/1000], Loss: 2.4732, Val Loss: 0.4484\n",
      "Epoch [728/1000], Loss: 0.0298, Val Loss: 0.4484\n",
      "Epoch [729/1000], Loss: 0.0136, Val Loss: 0.4484\n",
      "Epoch [730/1000], Loss: 0.2722, Val Loss: 0.4484\n",
      "Epoch [731/1000], Loss: 0.1365, Val Loss: 0.4484\n",
      "Epoch [732/1000], Loss: 0.0857, Val Loss: 0.4484\n",
      "Epoch [733/1000], Loss: 0.2621, Val Loss: 0.4484\n",
      "Epoch [734/1000], Loss: 0.0108, Val Loss: 0.4484\n",
      "Epoch [735/1000], Loss: 0.0816, Val Loss: 0.4484\n",
      "Epoch [736/1000], Loss: 0.0076, Val Loss: 0.4483\n",
      "Epoch [737/1000], Loss: 1.3674, Val Loss: 0.4483\n",
      "Epoch [738/1000], Loss: 0.9233, Val Loss: 0.4483\n",
      "Epoch [739/1000], Loss: 0.8421, Val Loss: 0.4483\n",
      "Epoch [740/1000], Loss: 0.0715, Val Loss: 0.4483\n",
      "Epoch [741/1000], Loss: 0.4998, Val Loss: 0.4483\n",
      "Epoch [742/1000], Loss: 0.6841, Val Loss: 0.4483\n",
      "Epoch [743/1000], Loss: 0.0018, Val Loss: 0.4483\n",
      "Epoch [744/1000], Loss: 0.4185, Val Loss: 0.4483\n",
      "Epoch [745/1000], Loss: 0.8146, Val Loss: 0.4483\n",
      "Epoch [746/1000], Loss: 0.6934, Val Loss: 0.4483\n",
      "Epoch [747/1000], Loss: 0.5679, Val Loss: 0.4483\n",
      "Epoch [748/1000], Loss: 0.6149, Val Loss: 0.4483\n",
      "Epoch [749/1000], Loss: 0.6763, Val Loss: 0.4483\n",
      "Epoch [750/1000], Loss: 0.3181, Val Loss: 0.4483\n",
      "Epoch [751/1000], Loss: 0.9627, Val Loss: 0.4483\n",
      "Epoch [752/1000], Loss: 0.0745, Val Loss: 0.4482\n",
      "Epoch [753/1000], Loss: 0.0530, Val Loss: 0.4482\n",
      "Epoch [754/1000], Loss: 0.0352, Val Loss: 0.4482\n",
      "Epoch [755/1000], Loss: 0.0236, Val Loss: 0.4482\n",
      "Epoch [756/1000], Loss: 1.0398, Val Loss: 0.4482\n",
      "Epoch [757/1000], Loss: 0.3756, Val Loss: 0.4482\n",
      "Epoch [758/1000], Loss: 0.0948, Val Loss: 0.4482\n",
      "Epoch [759/1000], Loss: 0.0369, Val Loss: 0.4482\n",
      "Epoch [760/1000], Loss: 0.5558, Val Loss: 0.4482\n",
      "Epoch [761/1000], Loss: 0.0464, Val Loss: 0.4482\n",
      "Epoch [762/1000], Loss: 0.0193, Val Loss: 0.4482\n",
      "Epoch [763/1000], Loss: 0.0108, Val Loss: 0.4481\n",
      "Epoch [764/1000], Loss: 0.8656, Val Loss: 0.4481\n",
      "Epoch [765/1000], Loss: 0.1085, Val Loss: 0.4482\n",
      "Epoch [766/1000], Loss: 0.0029, Val Loss: 0.4482\n",
      "Epoch [767/1000], Loss: 0.2175, Val Loss: 0.4481\n",
      "Epoch [768/1000], Loss: 0.2445, Val Loss: 0.4482\n",
      "Epoch [769/1000], Loss: 0.3047, Val Loss: 0.4482\n",
      "Epoch [770/1000], Loss: 1.0490, Val Loss: 0.4481\n",
      "Epoch [771/1000], Loss: 0.6929, Val Loss: 0.4481\n",
      "Epoch [772/1000], Loss: 1.1554, Val Loss: 0.4481\n",
      "Epoch [773/1000], Loss: 0.1595, Val Loss: 0.4481\n",
      "Epoch [774/1000], Loss: 0.0320, Val Loss: 0.4481\n",
      "Epoch [775/1000], Loss: 0.0565, Val Loss: 0.4481\n",
      "Epoch [776/1000], Loss: 0.0175, Val Loss: 0.4482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:23:17,004] Trial 52 finished with value: 0.8238604791060222 and parameters: {'hidden_size': 82, 'dropout_prob': 0.6685399213003373, 'learning_rate': 1.0667428923841552e-05, 'weight_decay': 7.940954151073346e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 777\n",
      "Epoch [1/1000], Loss: 0.6877, Val Loss: 0.7052\n",
      "Epoch [2/1000], Loss: 0.6565, Val Loss: 0.5768\n",
      "Epoch [3/1000], Loss: 0.5337, Val Loss: 0.5118\n",
      "Epoch [4/1000], Loss: 0.1179, Val Loss: 0.4970\n",
      "Epoch [5/1000], Loss: 0.8707, Val Loss: 0.4872\n",
      "Epoch [6/1000], Loss: 0.0686, Val Loss: 0.4797\n",
      "Epoch [7/1000], Loss: 0.4818, Val Loss: 0.4738\n",
      "Epoch [8/1000], Loss: 0.6047, Val Loss: 0.4703\n",
      "Epoch [9/1000], Loss: 0.7411, Val Loss: 0.4674\n",
      "Epoch [10/1000], Loss: 0.9054, Val Loss: 0.4658\n",
      "Epoch [11/1000], Loss: 0.0083, Val Loss: 0.4647\n",
      "Epoch [12/1000], Loss: 0.0311, Val Loss: 0.4637\n",
      "Epoch [13/1000], Loss: 0.7654, Val Loss: 0.4627\n",
      "Epoch [14/1000], Loss: 1.5263, Val Loss: 0.4619\n",
      "Epoch [15/1000], Loss: 0.0438, Val Loss: 0.4613\n",
      "Epoch [16/1000], Loss: 0.6124, Val Loss: 0.4604\n",
      "Epoch [17/1000], Loss: 0.8853, Val Loss: 0.4597\n",
      "Epoch [18/1000], Loss: 1.2465, Val Loss: 0.4587\n",
      "Epoch [19/1000], Loss: 0.1922, Val Loss: 0.4579\n",
      "Epoch [20/1000], Loss: 0.0123, Val Loss: 0.4575\n",
      "Epoch [21/1000], Loss: 0.4672, Val Loss: 0.4572\n",
      "Epoch [22/1000], Loss: 0.6246, Val Loss: 0.4565\n",
      "Epoch [23/1000], Loss: 0.3151, Val Loss: 0.4559\n",
      "Epoch [24/1000], Loss: 0.9022, Val Loss: 0.4553\n",
      "Epoch [25/1000], Loss: 0.4956, Val Loss: 0.4550\n",
      "Epoch [26/1000], Loss: 0.0779, Val Loss: 0.4547\n",
      "Epoch [27/1000], Loss: 0.1423, Val Loss: 0.4542\n",
      "Epoch [28/1000], Loss: 0.6355, Val Loss: 0.4541\n",
      "Epoch [29/1000], Loss: 1.6354, Val Loss: 0.4538\n",
      "Epoch [30/1000], Loss: 0.5241, Val Loss: 0.4531\n",
      "Epoch [31/1000], Loss: 0.0052, Val Loss: 0.4530\n",
      "Epoch [32/1000], Loss: 0.1649, Val Loss: 0.4526\n",
      "Epoch [33/1000], Loss: 0.0434, Val Loss: 0.4521\n",
      "Epoch [34/1000], Loss: 0.1282, Val Loss: 0.4518\n",
      "Epoch [35/1000], Loss: 0.6777, Val Loss: 0.4513\n",
      "Epoch [36/1000], Loss: 1.6624, Val Loss: 0.4510\n",
      "Epoch [37/1000], Loss: 0.0159, Val Loss: 0.4509\n",
      "Epoch [38/1000], Loss: 0.0007, Val Loss: 0.4505\n",
      "Epoch [39/1000], Loss: 0.9011, Val Loss: 0.4504\n",
      "Epoch [40/1000], Loss: 0.2383, Val Loss: 0.4502\n",
      "Epoch [41/1000], Loss: 0.2236, Val Loss: 0.4497\n",
      "Epoch [42/1000], Loss: 0.0089, Val Loss: 0.4496\n",
      "Epoch [43/1000], Loss: 0.8305, Val Loss: 0.4495\n",
      "Epoch [44/1000], Loss: 0.0164, Val Loss: 0.4497\n",
      "Epoch [45/1000], Loss: 0.0703, Val Loss: 0.4492\n",
      "Epoch [46/1000], Loss: 0.7048, Val Loss: 0.4490\n",
      "Epoch [47/1000], Loss: 0.8077, Val Loss: 0.4486\n",
      "Epoch [48/1000], Loss: 0.0140, Val Loss: 0.4483\n",
      "Epoch [49/1000], Loss: 0.9732, Val Loss: 0.4484\n",
      "Epoch [50/1000], Loss: 0.7388, Val Loss: 0.4485\n",
      "Epoch [51/1000], Loss: 0.0351, Val Loss: 0.4483\n",
      "Epoch [52/1000], Loss: 0.9639, Val Loss: 0.4482\n",
      "Epoch [53/1000], Loss: 0.0604, Val Loss: 0.4480\n",
      "Epoch [54/1000], Loss: 0.1980, Val Loss: 0.4479\n",
      "Epoch [55/1000], Loss: 0.0016, Val Loss: 0.4480\n",
      "Epoch [56/1000], Loss: 0.3826, Val Loss: 0.4481\n",
      "Epoch [57/1000], Loss: 1.5203, Val Loss: 0.4480\n",
      "Epoch [58/1000], Loss: 1.4417, Val Loss: 0.4479\n",
      "Epoch [59/1000], Loss: 3.1984, Val Loss: 0.4474\n",
      "Epoch [60/1000], Loss: 0.0364, Val Loss: 0.4471\n",
      "Epoch [61/1000], Loss: 0.5277, Val Loss: 0.4470\n",
      "Epoch [62/1000], Loss: 0.4184, Val Loss: 0.4468\n",
      "Epoch [63/1000], Loss: 0.8742, Val Loss: 0.4473\n",
      "Epoch [64/1000], Loss: 0.0155, Val Loss: 0.4475\n",
      "Epoch [65/1000], Loss: 0.1144, Val Loss: 0.4471\n",
      "Epoch [66/1000], Loss: 1.2207, Val Loss: 0.4471\n",
      "Epoch [67/1000], Loss: 0.5440, Val Loss: 0.4470\n",
      "Epoch [68/1000], Loss: 0.0037, Val Loss: 0.4473\n",
      "Epoch [69/1000], Loss: 0.5301, Val Loss: 0.4472\n",
      "Epoch [70/1000], Loss: 0.8188, Val Loss: 0.4469\n",
      "Epoch [71/1000], Loss: 0.5617, Val Loss: 0.4468\n",
      "Epoch [72/1000], Loss: 1.9530, Val Loss: 0.4467\n",
      "Epoch [73/1000], Loss: 1.0756, Val Loss: 0.4469\n",
      "Epoch [74/1000], Loss: 0.0458, Val Loss: 0.4469\n",
      "Epoch [75/1000], Loss: 0.2269, Val Loss: 0.4466\n",
      "Epoch [76/1000], Loss: 0.0007, Val Loss: 0.4465\n",
      "Epoch [77/1000], Loss: 0.3919, Val Loss: 0.4468\n",
      "Epoch [78/1000], Loss: 2.3397, Val Loss: 0.4468\n",
      "Epoch [79/1000], Loss: 0.8366, Val Loss: 0.4463\n",
      "Epoch [80/1000], Loss: 0.1658, Val Loss: 0.4465\n",
      "Epoch [81/1000], Loss: 0.1680, Val Loss: 0.4466\n",
      "Epoch [82/1000], Loss: 0.2537, Val Loss: 0.4468\n",
      "Epoch [83/1000], Loss: 0.7400, Val Loss: 0.4469\n",
      "Epoch [84/1000], Loss: 1.0189, Val Loss: 0.4467\n",
      "Epoch [85/1000], Loss: 0.0036, Val Loss: 0.4466\n",
      "Epoch [86/1000], Loss: 0.0111, Val Loss: 0.4466\n",
      "Epoch [87/1000], Loss: 0.3854, Val Loss: 0.4470\n",
      "Epoch [88/1000], Loss: 0.0538, Val Loss: 0.4469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:23:45,704] Trial 53 finished with value: 0.8222265838078682 and parameters: {'hidden_size': 84, 'dropout_prob': 0.7210938925092781, 'learning_rate': 0.00018968681163882535, 'weight_decay': 8.212704996848795e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 89\n",
      "Epoch [1/1000], Loss: 0.6246, Val Loss: 0.7312\n",
      "Epoch [2/1000], Loss: 0.7423, Val Loss: 0.6919\n",
      "Epoch [3/1000], Loss: 0.7840, Val Loss: 0.6317\n",
      "Epoch [4/1000], Loss: 0.5858, Val Loss: 0.5743\n",
      "Epoch [5/1000], Loss: 0.2686, Val Loss: 0.5382\n",
      "Epoch [6/1000], Loss: 0.8296, Val Loss: 0.5213\n",
      "Epoch [7/1000], Loss: 1.2782, Val Loss: 0.5122\n",
      "Epoch [8/1000], Loss: 0.0295, Val Loss: 0.5065\n",
      "Epoch [9/1000], Loss: 0.1792, Val Loss: 0.5017\n",
      "Epoch [10/1000], Loss: 1.0033, Val Loss: 0.4977\n",
      "Epoch [11/1000], Loss: 0.7134, Val Loss: 0.4941\n",
      "Epoch [12/1000], Loss: 0.0615, Val Loss: 0.4907\n",
      "Epoch [13/1000], Loss: 0.5412, Val Loss: 0.4878\n",
      "Epoch [14/1000], Loss: 0.4172, Val Loss: 0.4853\n",
      "Epoch [15/1000], Loss: 0.4257, Val Loss: 0.4829\n",
      "Epoch [16/1000], Loss: 0.0228, Val Loss: 0.4810\n",
      "Epoch [17/1000], Loss: 0.0046, Val Loss: 0.4793\n",
      "Epoch [18/1000], Loss: 1.0670, Val Loss: 0.4774\n",
      "Epoch [19/1000], Loss: 0.5949, Val Loss: 0.4758\n",
      "Epoch [20/1000], Loss: 0.8698, Val Loss: 0.4746\n",
      "Epoch [21/1000], Loss: 0.6173, Val Loss: 0.4734\n",
      "Epoch [22/1000], Loss: 0.4819, Val Loss: 0.4723\n",
      "Epoch [23/1000], Loss: 0.9330, Val Loss: 0.4712\n",
      "Epoch [24/1000], Loss: 0.0433, Val Loss: 0.4701\n",
      "Epoch [25/1000], Loss: 0.2483, Val Loss: 0.4693\n",
      "Epoch [26/1000], Loss: 0.7095, Val Loss: 0.4684\n",
      "Epoch [27/1000], Loss: 0.5510, Val Loss: 0.4677\n",
      "Epoch [28/1000], Loss: 0.4406, Val Loss: 0.4670\n",
      "Epoch [29/1000], Loss: 0.7853, Val Loss: 0.4665\n",
      "Epoch [30/1000], Loss: 0.0214, Val Loss: 0.4660\n",
      "Epoch [31/1000], Loss: 0.4066, Val Loss: 0.4653\n",
      "Epoch [32/1000], Loss: 0.5498, Val Loss: 0.4648\n",
      "Epoch [33/1000], Loss: 0.0349, Val Loss: 0.4644\n",
      "Epoch [34/1000], Loss: 0.5610, Val Loss: 0.4639\n",
      "Epoch [35/1000], Loss: 0.5361, Val Loss: 0.4635\n",
      "Epoch [36/1000], Loss: 0.8089, Val Loss: 0.4632\n",
      "Epoch [37/1000], Loss: 0.0479, Val Loss: 0.4628\n",
      "Epoch [38/1000], Loss: 0.0366, Val Loss: 0.4624\n",
      "Epoch [39/1000], Loss: 0.2877, Val Loss: 0.4620\n",
      "Epoch [40/1000], Loss: 0.0171, Val Loss: 0.4617\n",
      "Epoch [41/1000], Loss: 0.1250, Val Loss: 0.4614\n",
      "Epoch [42/1000], Loss: 0.5565, Val Loss: 0.4610\n",
      "Epoch [43/1000], Loss: 0.0757, Val Loss: 0.4606\n",
      "Epoch [44/1000], Loss: 0.1679, Val Loss: 0.4604\n",
      "Epoch [45/1000], Loss: 0.0940, Val Loss: 0.4603\n",
      "Epoch [46/1000], Loss: 1.7725, Val Loss: 0.4600\n",
      "Epoch [47/1000], Loss: 0.8553, Val Loss: 0.4598\n",
      "Epoch [48/1000], Loss: 0.1084, Val Loss: 0.4596\n",
      "Epoch [49/1000], Loss: 0.7345, Val Loss: 0.4593\n",
      "Epoch [50/1000], Loss: 3.2615, Val Loss: 0.4591\n",
      "Epoch [51/1000], Loss: 0.6628, Val Loss: 0.4590\n",
      "Epoch [52/1000], Loss: 0.9097, Val Loss: 0.4587\n",
      "Epoch [53/1000], Loss: 0.1220, Val Loss: 0.4585\n",
      "Epoch [54/1000], Loss: 0.0361, Val Loss: 0.4582\n",
      "Epoch [55/1000], Loss: 0.8617, Val Loss: 0.4581\n",
      "Epoch [56/1000], Loss: 0.2161, Val Loss: 0.4580\n",
      "Epoch [57/1000], Loss: 0.0041, Val Loss: 0.4578\n",
      "Epoch [58/1000], Loss: 0.6579, Val Loss: 0.4577\n",
      "Epoch [59/1000], Loss: 0.0034, Val Loss: 0.4576\n",
      "Epoch [60/1000], Loss: 0.0940, Val Loss: 0.4574\n",
      "Epoch [61/1000], Loss: 0.0026, Val Loss: 0.4573\n",
      "Epoch [62/1000], Loss: 0.0091, Val Loss: 0.4571\n",
      "Epoch [63/1000], Loss: 0.4396, Val Loss: 0.4570\n",
      "Epoch [64/1000], Loss: 0.6322, Val Loss: 0.4567\n",
      "Epoch [65/1000], Loss: 0.0811, Val Loss: 0.4566\n",
      "Epoch [66/1000], Loss: 2.0294, Val Loss: 0.4562\n",
      "Epoch [67/1000], Loss: 0.5147, Val Loss: 0.4562\n",
      "Epoch [68/1000], Loss: 0.0086, Val Loss: 0.4560\n",
      "Epoch [69/1000], Loss: 0.0046, Val Loss: 0.4558\n",
      "Epoch [70/1000], Loss: 0.8169, Val Loss: 0.4556\n",
      "Epoch [71/1000], Loss: 0.0000, Val Loss: 0.4554\n",
      "Epoch [72/1000], Loss: 0.0680, Val Loss: 0.4553\n",
      "Epoch [73/1000], Loss: 1.9711, Val Loss: 0.4552\n",
      "Epoch [74/1000], Loss: 0.0020, Val Loss: 0.4552\n",
      "Epoch [75/1000], Loss: 1.3896, Val Loss: 0.4551\n",
      "Epoch [76/1000], Loss: 0.4272, Val Loss: 0.4551\n",
      "Epoch [77/1000], Loss: 0.0434, Val Loss: 0.4548\n",
      "Epoch [78/1000], Loss: 0.9721, Val Loss: 0.4547\n",
      "Epoch [79/1000], Loss: 0.3842, Val Loss: 0.4546\n",
      "Epoch [80/1000], Loss: 0.7810, Val Loss: 0.4545\n",
      "Epoch [81/1000], Loss: 2.8458, Val Loss: 0.4543\n",
      "Epoch [82/1000], Loss: 0.0303, Val Loss: 0.4542\n",
      "Epoch [83/1000], Loss: 0.7768, Val Loss: 0.4542\n",
      "Epoch [84/1000], Loss: 0.5930, Val Loss: 0.4541\n",
      "Epoch [85/1000], Loss: 0.0474, Val Loss: 0.4540\n",
      "Epoch [86/1000], Loss: 0.7813, Val Loss: 0.4539\n",
      "Epoch [87/1000], Loss: 0.0802, Val Loss: 0.4537\n",
      "Epoch [88/1000], Loss: 0.0198, Val Loss: 0.4537\n",
      "Epoch [89/1000], Loss: 0.8912, Val Loss: 0.4537\n",
      "Epoch [90/1000], Loss: 0.1087, Val Loss: 0.4537\n",
      "Epoch [91/1000], Loss: 0.0226, Val Loss: 0.4537\n",
      "Epoch [92/1000], Loss: 0.1242, Val Loss: 0.4536\n",
      "Epoch [93/1000], Loss: 0.3799, Val Loss: 0.4536\n",
      "Epoch [94/1000], Loss: 0.0607, Val Loss: 0.4534\n",
      "Epoch [95/1000], Loss: 0.0029, Val Loss: 0.4534\n",
      "Epoch [96/1000], Loss: 0.6451, Val Loss: 0.4532\n",
      "Epoch [97/1000], Loss: 0.0464, Val Loss: 0.4530\n",
      "Epoch [98/1000], Loss: 0.0130, Val Loss: 0.4530\n",
      "Epoch [99/1000], Loss: 0.4795, Val Loss: 0.4529\n",
      "Epoch [100/1000], Loss: 0.0457, Val Loss: 0.4527\n",
      "Epoch [101/1000], Loss: 0.5911, Val Loss: 0.4526\n",
      "Epoch [102/1000], Loss: 0.5414, Val Loss: 0.4525\n",
      "Epoch [103/1000], Loss: 0.1086, Val Loss: 0.4525\n",
      "Epoch [104/1000], Loss: 0.1622, Val Loss: 0.4524\n",
      "Epoch [105/1000], Loss: 0.3733, Val Loss: 0.4524\n",
      "Epoch [106/1000], Loss: 0.6432, Val Loss: 0.4523\n",
      "Epoch [107/1000], Loss: 1.7112, Val Loss: 0.4522\n",
      "Epoch [108/1000], Loss: 0.0108, Val Loss: 0.4520\n",
      "Epoch [109/1000], Loss: 0.0031, Val Loss: 0.4520\n",
      "Epoch [110/1000], Loss: 0.2831, Val Loss: 0.4520\n",
      "Epoch [111/1000], Loss: 0.0936, Val Loss: 0.4520\n",
      "Epoch [112/1000], Loss: 0.0299, Val Loss: 0.4519\n",
      "Epoch [113/1000], Loss: 0.5637, Val Loss: 0.4519\n",
      "Epoch [114/1000], Loss: 0.0598, Val Loss: 0.4518\n",
      "Epoch [115/1000], Loss: 0.6451, Val Loss: 0.4517\n",
      "Epoch [116/1000], Loss: 0.4533, Val Loss: 0.4518\n",
      "Epoch [117/1000], Loss: 0.0074, Val Loss: 0.4516\n",
      "Epoch [118/1000], Loss: 0.7180, Val Loss: 0.4516\n",
      "Epoch [119/1000], Loss: 0.0664, Val Loss: 0.4516\n",
      "Epoch [120/1000], Loss: 0.2644, Val Loss: 0.4516\n",
      "Epoch [121/1000], Loss: 0.5078, Val Loss: 0.4515\n",
      "Epoch [122/1000], Loss: 0.0717, Val Loss: 0.4514\n",
      "Epoch [123/1000], Loss: 0.0335, Val Loss: 0.4513\n",
      "Epoch [124/1000], Loss: 0.0169, Val Loss: 0.4512\n",
      "Epoch [125/1000], Loss: 0.0132, Val Loss: 0.4511\n",
      "Epoch [126/1000], Loss: 2.7929, Val Loss: 0.4510\n",
      "Epoch [127/1000], Loss: 0.0746, Val Loss: 0.4509\n",
      "Epoch [128/1000], Loss: 0.1364, Val Loss: 0.4508\n",
      "Epoch [129/1000], Loss: 0.3998, Val Loss: 0.4507\n",
      "Epoch [130/1000], Loss: 0.7681, Val Loss: 0.4507\n",
      "Epoch [131/1000], Loss: 1.0225, Val Loss: 0.4505\n",
      "Epoch [132/1000], Loss: 0.6078, Val Loss: 0.4505\n",
      "Epoch [133/1000], Loss: 0.4622, Val Loss: 0.4503\n",
      "Epoch [134/1000], Loss: 0.6729, Val Loss: 0.4503\n",
      "Epoch [135/1000], Loss: 0.0080, Val Loss: 0.4503\n",
      "Epoch [136/1000], Loss: 0.9177, Val Loss: 0.4503\n",
      "Epoch [137/1000], Loss: 0.2574, Val Loss: 0.4503\n",
      "Epoch [138/1000], Loss: 0.0112, Val Loss: 0.4502\n",
      "Epoch [139/1000], Loss: 0.0650, Val Loss: 0.4502\n",
      "Epoch [140/1000], Loss: 0.1424, Val Loss: 0.4502\n",
      "Epoch [141/1000], Loss: 0.2817, Val Loss: 0.4502\n",
      "Epoch [142/1000], Loss: 0.6995, Val Loss: 0.4502\n",
      "Epoch [143/1000], Loss: 0.1508, Val Loss: 0.4502\n",
      "Epoch [144/1000], Loss: 0.7278, Val Loss: 0.4502\n",
      "Epoch [145/1000], Loss: 0.2170, Val Loss: 0.4501\n",
      "Epoch [146/1000], Loss: 0.0286, Val Loss: 0.4501\n",
      "Epoch [147/1000], Loss: 0.2139, Val Loss: 0.4501\n",
      "Epoch [148/1000], Loss: 0.6763, Val Loss: 0.4498\n",
      "Epoch [149/1000], Loss: 0.6940, Val Loss: 0.4498\n",
      "Epoch [150/1000], Loss: 0.0076, Val Loss: 0.4497\n",
      "Epoch [151/1000], Loss: 1.4852, Val Loss: 0.4497\n",
      "Epoch [152/1000], Loss: 0.5955, Val Loss: 0.4496\n",
      "Epoch [153/1000], Loss: 0.2258, Val Loss: 0.4496\n",
      "Epoch [154/1000], Loss: 0.3337, Val Loss: 0.4495\n",
      "Epoch [155/1000], Loss: 0.2136, Val Loss: 0.4495\n",
      "Epoch [156/1000], Loss: 0.2240, Val Loss: 0.4495\n",
      "Epoch [157/1000], Loss: 0.0258, Val Loss: 0.4495\n",
      "Epoch [158/1000], Loss: 0.9822, Val Loss: 0.4494\n",
      "Epoch [159/1000], Loss: 0.1450, Val Loss: 0.4493\n",
      "Epoch [160/1000], Loss: 0.1872, Val Loss: 0.4492\n",
      "Epoch [161/1000], Loss: 0.3623, Val Loss: 0.4492\n",
      "Epoch [162/1000], Loss: 0.0439, Val Loss: 0.4492\n",
      "Epoch [163/1000], Loss: 0.0798, Val Loss: 0.4492\n",
      "Epoch [164/1000], Loss: 0.0190, Val Loss: 0.4492\n",
      "Epoch [165/1000], Loss: 0.1585, Val Loss: 0.4491\n",
      "Epoch [166/1000], Loss: 0.0300, Val Loss: 0.4492\n",
      "Epoch [167/1000], Loss: 0.4974, Val Loss: 0.4492\n",
      "Epoch [168/1000], Loss: 1.2739, Val Loss: 0.4491\n",
      "Epoch [169/1000], Loss: 0.0280, Val Loss: 0.4491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 1.2589, Val Loss: 0.4491\n",
      "Epoch [171/1000], Loss: 0.0080, Val Loss: 0.4492\n",
      "Epoch [172/1000], Loss: 0.0647, Val Loss: 0.4491\n",
      "Epoch [173/1000], Loss: 0.1694, Val Loss: 0.4491\n",
      "Epoch [174/1000], Loss: 1.0906, Val Loss: 0.4492\n",
      "Epoch [175/1000], Loss: 0.3452, Val Loss: 0.4493\n",
      "Epoch [176/1000], Loss: 0.0390, Val Loss: 0.4492\n",
      "Epoch [177/1000], Loss: 0.4127, Val Loss: 0.4492\n",
      "Epoch [178/1000], Loss: 0.8402, Val Loss: 0.4490\n",
      "Epoch [179/1000], Loss: 0.0008, Val Loss: 0.4488\n",
      "Epoch [180/1000], Loss: 0.4759, Val Loss: 0.4488\n",
      "Epoch [181/1000], Loss: 0.0387, Val Loss: 0.4487\n",
      "Epoch [182/1000], Loss: 1.0628, Val Loss: 0.4487\n",
      "Epoch [183/1000], Loss: 0.0122, Val Loss: 0.4487\n",
      "Epoch [184/1000], Loss: 0.6584, Val Loss: 0.4487\n",
      "Epoch [185/1000], Loss: 0.0850, Val Loss: 0.4485\n",
      "Epoch [186/1000], Loss: 0.3612, Val Loss: 0.4485\n",
      "Epoch [187/1000], Loss: 0.7249, Val Loss: 0.4486\n",
      "Epoch [188/1000], Loss: 0.8056, Val Loss: 0.4489\n",
      "Epoch [189/1000], Loss: 2.1264, Val Loss: 0.4489\n",
      "Epoch [190/1000], Loss: 0.0748, Val Loss: 0.4489\n",
      "Epoch [191/1000], Loss: 4.5542, Val Loss: 0.4488\n",
      "Epoch [192/1000], Loss: 0.4895, Val Loss: 0.4488\n",
      "Epoch [193/1000], Loss: 1.0306, Val Loss: 0.4486\n",
      "Epoch [194/1000], Loss: 0.0435, Val Loss: 0.4486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:24:42,967] Trial 54 finished with value: 0.8223438368245843 and parameters: {'hidden_size': 93, 'dropout_prob': 0.6573316499468408, 'learning_rate': 5.167912996931938e-05, 'weight_decay': 7.731522868814022e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 195\n",
      "Epoch [1/1000], Loss: 0.9547, Val Loss: 0.4647\n",
      "Epoch [2/1000], Loss: 0.0176, Val Loss: 0.4571\n",
      "Epoch [3/1000], Loss: 0.3261, Val Loss: 0.4531\n",
      "Epoch [4/1000], Loss: 0.5162, Val Loss: 0.4523\n",
      "Epoch [5/1000], Loss: 0.1603, Val Loss: 0.4594\n",
      "Epoch [6/1000], Loss: 0.0328, Val Loss: 0.4523\n",
      "Epoch [7/1000], Loss: 0.0086, Val Loss: 0.4482\n",
      "Epoch [8/1000], Loss: 0.8917, Val Loss: 0.4490\n",
      "Epoch [9/1000], Loss: 0.0214, Val Loss: 0.4468\n",
      "Epoch [10/1000], Loss: 0.0526, Val Loss: 0.4483\n",
      "Epoch [11/1000], Loss: 0.1969, Val Loss: 0.4480\n",
      "Epoch [12/1000], Loss: 0.9263, Val Loss: 0.4479\n",
      "Epoch [13/1000], Loss: 0.0555, Val Loss: 0.4475\n",
      "Epoch [14/1000], Loss: 0.1513, Val Loss: 0.4506\n",
      "Epoch [15/1000], Loss: 0.2047, Val Loss: 0.4465\n",
      "Epoch [16/1000], Loss: 0.0182, Val Loss: 0.4486\n",
      "Epoch [17/1000], Loss: 1.0488, Val Loss: 0.4481\n",
      "Epoch [18/1000], Loss: 0.2189, Val Loss: 0.4497\n",
      "Epoch [19/1000], Loss: 0.3400, Val Loss: 0.4483\n",
      "Epoch [20/1000], Loss: 0.0284, Val Loss: 0.4505\n",
      "Epoch [21/1000], Loss: 0.3068, Val Loss: 0.4487\n",
      "Epoch [22/1000], Loss: 0.2186, Val Loss: 0.4515\n",
      "Epoch [23/1000], Loss: 0.0215, Val Loss: 0.4527\n",
      "Epoch [24/1000], Loss: 0.0242, Val Loss: 0.4477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:24:50,475] Trial 55 finished with value: 0.821464439199213 and parameters: {'hidden_size': 102, 'dropout_prob': 0.582284104334263, 'learning_rate': 0.0019124346557545056, 'weight_decay': 1.3879768942110391e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 25\n",
      "Epoch [1/1000], Loss: 0.1538, Val Loss: 0.4851\n",
      "Epoch [2/1000], Loss: 0.1203, Val Loss: 0.4660\n",
      "Epoch [3/1000], Loss: 0.7381, Val Loss: 0.4635\n",
      "Epoch [4/1000], Loss: 0.0088, Val Loss: 0.4567\n",
      "Epoch [5/1000], Loss: 0.1314, Val Loss: 0.4560\n",
      "Epoch [6/1000], Loss: 0.4457, Val Loss: 0.4530\n",
      "Epoch [7/1000], Loss: 0.2757, Val Loss: 0.4521\n",
      "Epoch [8/1000], Loss: 0.3917, Val Loss: 0.4518\n",
      "Epoch [9/1000], Loss: 0.0090, Val Loss: 0.4512\n",
      "Epoch [10/1000], Loss: 0.1184, Val Loss: 0.4503\n",
      "Epoch [11/1000], Loss: 0.5345, Val Loss: 0.4518\n",
      "Epoch [12/1000], Loss: 0.0010, Val Loss: 0.4492\n",
      "Epoch [13/1000], Loss: 0.3143, Val Loss: 0.4506\n",
      "Epoch [14/1000], Loss: 1.8301, Val Loss: 0.4491\n",
      "Epoch [15/1000], Loss: 0.0138, Val Loss: 0.4480\n",
      "Epoch [16/1000], Loss: 0.3445, Val Loss: 0.4495\n",
      "Epoch [17/1000], Loss: 0.0058, Val Loss: 0.4493\n",
      "Epoch [18/1000], Loss: 1.7929, Val Loss: 0.4521\n",
      "Epoch [19/1000], Loss: 0.4401, Val Loss: 0.4482\n",
      "Epoch [20/1000], Loss: 0.0974, Val Loss: 0.4503\n",
      "Epoch [21/1000], Loss: 0.0045, Val Loss: 0.4512\n",
      "Epoch [22/1000], Loss: 0.5595, Val Loss: 0.4480\n",
      "Epoch [23/1000], Loss: 1.0698, Val Loss: 0.4505\n",
      "Epoch [24/1000], Loss: 0.5226, Val Loss: 0.4502\n",
      "Epoch [25/1000], Loss: 0.0033, Val Loss: 0.4501\n",
      "Epoch [26/1000], Loss: 0.1592, Val Loss: 0.4477\n",
      "Epoch [27/1000], Loss: 0.0370, Val Loss: 0.4493\n",
      "Epoch [28/1000], Loss: 0.0298, Val Loss: 0.4468\n",
      "Epoch [29/1000], Loss: 0.0060, Val Loss: 0.4476\n",
      "Epoch [30/1000], Loss: 1.0581, Val Loss: 0.4486\n",
      "Epoch [31/1000], Loss: 1.0442, Val Loss: 0.4485\n",
      "Epoch [32/1000], Loss: 0.1133, Val Loss: 0.4479\n",
      "Epoch [33/1000], Loss: 0.1101, Val Loss: 0.4466\n",
      "Epoch [34/1000], Loss: 0.0389, Val Loss: 0.4497\n",
      "Epoch [35/1000], Loss: 0.0439, Val Loss: 0.4495\n",
      "Epoch [36/1000], Loss: 1.2740, Val Loss: 0.4471\n",
      "Epoch [37/1000], Loss: 0.0151, Val Loss: 0.4541\n",
      "Epoch [38/1000], Loss: 0.0055, Val Loss: 0.4500\n",
      "Epoch [39/1000], Loss: 0.4947, Val Loss: 0.4478\n",
      "Epoch [40/1000], Loss: 0.4784, Val Loss: 0.4478\n",
      "Epoch [41/1000], Loss: 0.6715, Val Loss: 0.4474\n",
      "Epoch [42/1000], Loss: 0.0143, Val Loss: 0.4486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:25:02,881] Trial 56 finished with value: 0.8212324821444047 and parameters: {'hidden_size': 120, 'dropout_prob': 0.7654170625838257, 'learning_rate': 0.0011395227215768103, 'weight_decay': 8.48885342322252e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 43\n",
      "Epoch [1/1000], Loss: 0.1183, Val Loss: 0.5181\n",
      "Epoch [2/1000], Loss: 0.8227, Val Loss: 0.4856\n",
      "Epoch [3/1000], Loss: 0.0006, Val Loss: 0.4742\n",
      "Epoch [4/1000], Loss: 0.7744, Val Loss: 0.4659\n",
      "Epoch [5/1000], Loss: 0.6517, Val Loss: 0.4624\n",
      "Epoch [6/1000], Loss: 0.0235, Val Loss: 0.4609\n",
      "Epoch [7/1000], Loss: 0.2540, Val Loss: 0.4591\n",
      "Epoch [8/1000], Loss: 0.4326, Val Loss: 0.4562\n",
      "Epoch [9/1000], Loss: 0.4275, Val Loss: 0.4559\n",
      "Epoch [10/1000], Loss: 1.1951, Val Loss: 0.4553\n",
      "Epoch [11/1000], Loss: 0.1496, Val Loss: 0.4546\n",
      "Epoch [12/1000], Loss: 1.1344, Val Loss: 0.4536\n",
      "Epoch [13/1000], Loss: 0.0038, Val Loss: 0.4538\n",
      "Epoch [14/1000], Loss: 0.8397, Val Loss: 0.4531\n",
      "Epoch [15/1000], Loss: 0.9456, Val Loss: 0.4520\n",
      "Epoch [16/1000], Loss: 0.7701, Val Loss: 0.4516\n",
      "Epoch [17/1000], Loss: 0.7258, Val Loss: 0.4518\n",
      "Epoch [18/1000], Loss: 0.2693, Val Loss: 0.4506\n",
      "Epoch [19/1000], Loss: 0.0166, Val Loss: 0.4507\n",
      "Epoch [20/1000], Loss: 3.3030, Val Loss: 0.4509\n",
      "Epoch [21/1000], Loss: 0.2378, Val Loss: 0.4508\n",
      "Epoch [22/1000], Loss: 0.3390, Val Loss: 0.4509\n",
      "Epoch [23/1000], Loss: 0.1751, Val Loss: 0.4504\n",
      "Epoch [24/1000], Loss: 1.2122, Val Loss: 0.4510\n",
      "Epoch [25/1000], Loss: 0.0119, Val Loss: 0.4509\n",
      "Epoch [26/1000], Loss: 0.5292, Val Loss: 0.4504\n",
      "Epoch [27/1000], Loss: 0.1136, Val Loss: 0.4510\n",
      "Epoch [28/1000], Loss: 1.2095, Val Loss: 0.4507\n",
      "Epoch [29/1000], Loss: 0.0236, Val Loss: 0.4501\n",
      "Epoch [30/1000], Loss: 0.5224, Val Loss: 0.4503\n",
      "Epoch [31/1000], Loss: 1.0296, Val Loss: 0.4500\n",
      "Epoch [32/1000], Loss: 0.2269, Val Loss: 0.4501\n",
      "Epoch [33/1000], Loss: 0.0296, Val Loss: 0.4502\n",
      "Epoch [34/1000], Loss: 1.0355, Val Loss: 0.4510\n",
      "Epoch [35/1000], Loss: 0.1223, Val Loss: 0.4509\n",
      "Epoch [36/1000], Loss: 0.0005, Val Loss: 0.4499\n",
      "Epoch [37/1000], Loss: 0.0185, Val Loss: 0.4498\n",
      "Epoch [38/1000], Loss: 0.1027, Val Loss: 0.4498\n",
      "Epoch [39/1000], Loss: 0.3106, Val Loss: 0.4490\n",
      "Epoch [40/1000], Loss: 0.0852, Val Loss: 0.4500\n",
      "Epoch [41/1000], Loss: 0.0435, Val Loss: 0.4493\n",
      "Epoch [42/1000], Loss: 3.7297, Val Loss: 0.4498\n",
      "Epoch [43/1000], Loss: 0.0240, Val Loss: 0.4489\n",
      "Epoch [44/1000], Loss: 0.1437, Val Loss: 0.4495\n",
      "Epoch [45/1000], Loss: 0.5393, Val Loss: 0.4487\n",
      "Epoch [46/1000], Loss: 0.4004, Val Loss: 0.4488\n",
      "Epoch [47/1000], Loss: 0.5819, Val Loss: 0.4492\n",
      "Epoch [48/1000], Loss: 0.1640, Val Loss: 0.4481\n",
      "Epoch [49/1000], Loss: 0.1443, Val Loss: 0.4479\n",
      "Epoch [50/1000], Loss: 0.9732, Val Loss: 0.4474\n",
      "Epoch [51/1000], Loss: 0.0550, Val Loss: 0.4481\n",
      "Epoch [52/1000], Loss: 0.0574, Val Loss: 0.4489\n",
      "Epoch [53/1000], Loss: 3.0313, Val Loss: 0.4499\n",
      "Epoch [54/1000], Loss: 0.2459, Val Loss: 0.4478\n",
      "Epoch [55/1000], Loss: 0.7599, Val Loss: 0.4472\n",
      "Epoch [56/1000], Loss: 0.1578, Val Loss: 0.4470\n",
      "Epoch [57/1000], Loss: 0.0073, Val Loss: 0.4485\n",
      "Epoch [58/1000], Loss: 0.0141, Val Loss: 0.4483\n",
      "Epoch [59/1000], Loss: 0.0885, Val Loss: 0.4481\n",
      "Epoch [60/1000], Loss: 0.4891, Val Loss: 0.4492\n",
      "Epoch [61/1000], Loss: 0.5306, Val Loss: 0.4495\n",
      "Epoch [62/1000], Loss: 0.0945, Val Loss: 0.4490\n",
      "Epoch [63/1000], Loss: 1.3369, Val Loss: 0.4490\n",
      "Epoch [64/1000], Loss: 0.1821, Val Loss: 0.4499\n",
      "Epoch [65/1000], Loss: 1.4451, Val Loss: 0.4484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:25:21,964] Trial 57 finished with value: 0.820302104946548 and parameters: {'hidden_size': 70, 'dropout_prob': 0.6964406391199542, 'learning_rate': 0.0006044701306017251, 'weight_decay': 9.641206212754168e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 66\n",
      "Epoch [1/1000], Loss: 1.0651, Val Loss: 0.5213\n",
      "Epoch [2/1000], Loss: 0.1924, Val Loss: 0.5031\n",
      "Epoch [3/1000], Loss: 0.1777, Val Loss: 0.4792\n",
      "Epoch [4/1000], Loss: 0.2444, Val Loss: 0.4826\n",
      "Epoch [5/1000], Loss: 0.0421, Val Loss: 0.4785\n",
      "Epoch [6/1000], Loss: 0.6707, Val Loss: 0.4698\n",
      "Epoch [7/1000], Loss: 0.0002, Val Loss: 0.4707\n",
      "Epoch [8/1000], Loss: 0.0285, Val Loss: 0.4686\n",
      "Epoch [9/1000], Loss: 0.6637, Val Loss: 0.4712\n",
      "Epoch [10/1000], Loss: 0.6962, Val Loss: 0.4718\n",
      "Epoch [11/1000], Loss: 0.9352, Val Loss: 0.4704\n",
      "Epoch [12/1000], Loss: 1.2735, Val Loss: 0.4673\n",
      "Epoch [13/1000], Loss: 0.1833, Val Loss: 0.4609\n",
      "Epoch [14/1000], Loss: 0.2190, Val Loss: 0.4632\n",
      "Epoch [15/1000], Loss: 0.0120, Val Loss: 0.4634\n",
      "Epoch [16/1000], Loss: 1.1976, Val Loss: 0.4671\n",
      "Epoch [17/1000], Loss: 0.1072, Val Loss: 0.4660\n",
      "Epoch [18/1000], Loss: 0.0353, Val Loss: 0.4676\n",
      "Epoch [19/1000], Loss: 0.0206, Val Loss: 0.4572\n",
      "Epoch [20/1000], Loss: 0.2660, Val Loss: 0.4586\n",
      "Epoch [21/1000], Loss: 0.8821, Val Loss: 0.4616\n",
      "Epoch [22/1000], Loss: 0.6644, Val Loss: 0.4643\n",
      "Epoch [23/1000], Loss: 0.3893, Val Loss: 0.4601\n",
      "Epoch [24/1000], Loss: 0.7692, Val Loss: 0.4632\n",
      "Epoch [25/1000], Loss: 0.1382, Val Loss: 0.4606\n",
      "Epoch [26/1000], Loss: 0.4256, Val Loss: 0.4580\n",
      "Epoch [27/1000], Loss: 0.7314, Val Loss: 0.4613\n",
      "Epoch [28/1000], Loss: 0.0227, Val Loss: 0.4707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:25:29,592] Trial 58 finished with value: 0.8171260775807134 and parameters: {'hidden_size': 31, 'dropout_prob': 0.7955898280075171, 'learning_rate': 0.0007953249168848325, 'weight_decay': 9.20322098714915e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 29\n",
      "Epoch [1/1000], Loss: 0.2530, Val Loss: 0.4671\n",
      "Epoch [2/1000], Loss: 0.6243, Val Loss: 0.4574\n",
      "Epoch [3/1000], Loss: 1.4153, Val Loss: 0.4567\n",
      "Epoch [4/1000], Loss: 0.1901, Val Loss: 0.4543\n",
      "Epoch [5/1000], Loss: 0.5253, Val Loss: 0.4498\n",
      "Epoch [6/1000], Loss: 0.3068, Val Loss: 0.4473\n",
      "Epoch [7/1000], Loss: 0.2679, Val Loss: 0.4467\n",
      "Epoch [8/1000], Loss: 0.4717, Val Loss: 0.4473\n",
      "Epoch [9/1000], Loss: 0.0393, Val Loss: 0.4500\n",
      "Epoch [10/1000], Loss: 0.3462, Val Loss: 0.4536\n",
      "Epoch [11/1000], Loss: 0.2399, Val Loss: 0.4479\n",
      "Epoch [12/1000], Loss: 0.6179, Val Loss: 0.4467\n",
      "Epoch [13/1000], Loss: 0.1194, Val Loss: 0.4461\n",
      "Epoch [14/1000], Loss: 0.0085, Val Loss: 0.4502\n",
      "Epoch [15/1000], Loss: 0.0244, Val Loss: 0.4489\n",
      "Epoch [16/1000], Loss: 0.0097, Val Loss: 0.4491\n",
      "Epoch [17/1000], Loss: 0.1000, Val Loss: 0.4524\n",
      "Epoch [18/1000], Loss: 0.5353, Val Loss: 0.4470\n",
      "Epoch [19/1000], Loss: 1.0112, Val Loss: 0.4470\n",
      "Epoch [20/1000], Loss: 0.0720, Val Loss: 0.4488\n",
      "Epoch [21/1000], Loss: 1.4718, Val Loss: 0.4497\n",
      "Epoch [22/1000], Loss: 0.2684, Val Loss: 0.4476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:25:36,990] Trial 59 finished with value: 0.8224508939268036 and parameters: {'hidden_size': 128, 'dropout_prob': 0.560388869349679, 'learning_rate': 0.0014172165355052697, 'weight_decay': 7.87315020464322e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 23\n",
      "Epoch [1/1000], Loss: 0.4446, Val Loss: 0.4801\n",
      "Epoch [2/1000], Loss: 0.4484, Val Loss: 0.4584\n",
      "Epoch [3/1000], Loss: 0.4658, Val Loss: 0.4528\n",
      "Epoch [4/1000], Loss: 0.4614, Val Loss: 0.4560\n",
      "Epoch [5/1000], Loss: 0.0697, Val Loss: 0.4515\n",
      "Epoch [6/1000], Loss: 0.2286, Val Loss: 0.4506\n",
      "Epoch [7/1000], Loss: 0.8569, Val Loss: 0.4473\n",
      "Epoch [8/1000], Loss: 0.4531, Val Loss: 0.4501\n",
      "Epoch [9/1000], Loss: 0.5342, Val Loss: 0.4540\n",
      "Epoch [10/1000], Loss: 0.3481, Val Loss: 0.4542\n",
      "Epoch [11/1000], Loss: 0.0462, Val Loss: 0.4525\n",
      "Epoch [12/1000], Loss: 0.0755, Val Loss: 0.4516\n",
      "Epoch [13/1000], Loss: 0.4228, Val Loss: 0.4621\n",
      "Epoch [14/1000], Loss: 0.4201, Val Loss: 0.4668\n",
      "Epoch [15/1000], Loss: 1.4101, Val Loss: 0.4607\n",
      "Epoch [16/1000], Loss: 0.0301, Val Loss: 0.4557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:25:42,007] Trial 60 finished with value: 0.8206079823814597 and parameters: {'hidden_size': 45, 'dropout_prob': 0.4730769241270528, 'learning_rate': 0.0073953162479071, 'weight_decay': 7.353444930378208e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 17\n",
      "Epoch [1/1000], Loss: 0.3283, Val Loss: 0.4832\n",
      "Epoch [2/1000], Loss: 0.0498, Val Loss: 0.4677\n",
      "Epoch [3/1000], Loss: 0.2282, Val Loss: 0.4618\n",
      "Epoch [4/1000], Loss: 0.6092, Val Loss: 0.4554\n",
      "Epoch [5/1000], Loss: 0.6356, Val Loss: 0.4533\n",
      "Epoch [6/1000], Loss: 0.0685, Val Loss: 0.4521\n",
      "Epoch [7/1000], Loss: 0.0121, Val Loss: 0.4505\n",
      "Epoch [8/1000], Loss: 0.9132, Val Loss: 0.4495\n",
      "Epoch [9/1000], Loss: 0.0572, Val Loss: 0.4512\n",
      "Epoch [10/1000], Loss: 0.0927, Val Loss: 0.4504\n",
      "Epoch [11/1000], Loss: 0.0036, Val Loss: 0.4497\n",
      "Epoch [12/1000], Loss: 0.2197, Val Loss: 0.4488\n",
      "Epoch [13/1000], Loss: 0.0281, Val Loss: 0.4504\n",
      "Epoch [14/1000], Loss: 0.0643, Val Loss: 0.4502\n",
      "Epoch [15/1000], Loss: 0.3871, Val Loss: 0.4485\n",
      "Epoch [16/1000], Loss: 0.0518, Val Loss: 0.4507\n",
      "Epoch [17/1000], Loss: 0.1623, Val Loss: 0.4482\n",
      "Epoch [18/1000], Loss: 0.2732, Val Loss: 0.4481\n",
      "Epoch [19/1000], Loss: 0.2513, Val Loss: 0.4474\n",
      "Epoch [20/1000], Loss: 0.0032, Val Loss: 0.4491\n",
      "Epoch [21/1000], Loss: 0.0898, Val Loss: 0.4516\n",
      "Epoch [22/1000], Loss: 0.0733, Val Loss: 0.4475\n",
      "Epoch [23/1000], Loss: 0.3878, Val Loss: 0.4479\n",
      "Epoch [24/1000], Loss: 0.1700, Val Loss: 0.4485\n",
      "Epoch [25/1000], Loss: 0.3937, Val Loss: 0.4501\n",
      "Epoch [26/1000], Loss: 0.0815, Val Loss: 0.4502\n",
      "Epoch [27/1000], Loss: 0.3310, Val Loss: 0.4475\n",
      "Epoch [28/1000], Loss: 0.0053, Val Loss: 0.4481\n",
      "Epoch [29/1000], Loss: 0.1610, Val Loss: 0.4457\n",
      "Epoch [30/1000], Loss: 1.0732, Val Loss: 0.4487\n",
      "Epoch [31/1000], Loss: 0.0304, Val Loss: 0.4476\n",
      "Epoch [32/1000], Loss: 1.2569, Val Loss: 0.4506\n",
      "Epoch [33/1000], Loss: 0.3609, Val Loss: 0.4479\n",
      "Epoch [34/1000], Loss: 0.1018, Val Loss: 0.4489\n",
      "Epoch [35/1000], Loss: 0.3331, Val Loss: 0.4484\n",
      "Epoch [36/1000], Loss: 0.0489, Val Loss: 0.4487\n",
      "Epoch [37/1000], Loss: 0.1259, Val Loss: 0.4472\n",
      "Epoch [38/1000], Loss: 1.0290, Val Loss: 0.4475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:25:53,706] Trial 61 finished with value: 0.8221908981071283 and parameters: {'hidden_size': 78, 'dropout_prob': 0.664996572212778, 'learning_rate': 0.0009602185901460214, 'weight_decay': 9.045642030454615e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 39\n",
      "Epoch [1/1000], Loss: 0.3933, Val Loss: 0.5002\n",
      "Epoch [2/1000], Loss: 0.4069, Val Loss: 0.4764\n",
      "Epoch [3/1000], Loss: 0.6716, Val Loss: 0.4688\n",
      "Epoch [4/1000], Loss: 0.1767, Val Loss: 0.4659\n",
      "Epoch [5/1000], Loss: 0.6166, Val Loss: 0.4629\n",
      "Epoch [6/1000], Loss: 0.1668, Val Loss: 0.4614\n",
      "Epoch [7/1000], Loss: 0.7618, Val Loss: 0.4605\n",
      "Epoch [8/1000], Loss: 0.5968, Val Loss: 0.4571\n",
      "Epoch [9/1000], Loss: 0.0467, Val Loss: 0.4559\n",
      "Epoch [10/1000], Loss: 0.0017, Val Loss: 0.4534\n",
      "Epoch [11/1000], Loss: 0.0684, Val Loss: 0.4538\n",
      "Epoch [12/1000], Loss: 0.5122, Val Loss: 0.4505\n",
      "Epoch [13/1000], Loss: 0.0231, Val Loss: 0.4497\n",
      "Epoch [14/1000], Loss: 0.0075, Val Loss: 0.4515\n",
      "Epoch [15/1000], Loss: 0.5311, Val Loss: 0.4500\n",
      "Epoch [16/1000], Loss: 0.0665, Val Loss: 0.4512\n",
      "Epoch [17/1000], Loss: 0.0436, Val Loss: 0.4499\n",
      "Epoch [18/1000], Loss: 0.3395, Val Loss: 0.4510\n",
      "Epoch [19/1000], Loss: 0.4114, Val Loss: 0.4507\n",
      "Epoch [20/1000], Loss: 1.1970, Val Loss: 0.4488\n",
      "Epoch [21/1000], Loss: 0.8853, Val Loss: 0.4500\n",
      "Epoch [22/1000], Loss: 0.6616, Val Loss: 0.4499\n",
      "Epoch [23/1000], Loss: 0.3260, Val Loss: 0.4505\n",
      "Epoch [24/1000], Loss: 0.0017, Val Loss: 0.4521\n",
      "Epoch [25/1000], Loss: 1.0427, Val Loss: 0.4483\n",
      "Epoch [26/1000], Loss: 0.2780, Val Loss: 0.4504\n",
      "Epoch [27/1000], Loss: 0.7833, Val Loss: 0.4506\n",
      "Epoch [28/1000], Loss: 0.2551, Val Loss: 0.4498\n",
      "Epoch [29/1000], Loss: 0.9680, Val Loss: 0.4502\n",
      "Epoch [30/1000], Loss: 2.2818, Val Loss: 0.4492\n",
      "Epoch [31/1000], Loss: 0.8586, Val Loss: 0.4487\n",
      "Epoch [32/1000], Loss: 0.2129, Val Loss: 0.4510\n",
      "Epoch [33/1000], Loss: 0.7943, Val Loss: 0.4511\n",
      "Epoch [34/1000], Loss: 0.2968, Val Loss: 0.4504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:26:03,511] Trial 62 finished with value: 0.8210846413841972 and parameters: {'hidden_size': 86, 'dropout_prob': 0.6271729370026287, 'learning_rate': 0.0005295642069364217, 'weight_decay': 8.878155060912304e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 35\n",
      "Epoch [1/1000], Loss: 0.6657, Val Loss: 0.6919\n",
      "Epoch [2/1000], Loss: 0.5872, Val Loss: 0.6734\n",
      "Epoch [3/1000], Loss: 0.8704, Val Loss: 0.6499\n",
      "Epoch [4/1000], Loss: 0.4487, Val Loss: 0.6237\n",
      "Epoch [5/1000], Loss: 0.3282, Val Loss: 0.5951\n",
      "Epoch [6/1000], Loss: 1.0214, Val Loss: 0.5705\n",
      "Epoch [7/1000], Loss: 0.5205, Val Loss: 0.5526\n",
      "Epoch [8/1000], Loss: 0.6178, Val Loss: 0.5397\n",
      "Epoch [9/1000], Loss: 0.4619, Val Loss: 0.5307\n",
      "Epoch [10/1000], Loss: 0.9485, Val Loss: 0.5240\n",
      "Epoch [11/1000], Loss: 0.2343, Val Loss: 0.5187\n",
      "Epoch [12/1000], Loss: 0.3476, Val Loss: 0.5136\n",
      "Epoch [13/1000], Loss: 0.2633, Val Loss: 0.5088\n",
      "Epoch [14/1000], Loss: 0.2779, Val Loss: 0.5047\n",
      "Epoch [15/1000], Loss: 0.0566, Val Loss: 0.5008\n",
      "Epoch [16/1000], Loss: 0.0020, Val Loss: 0.4979\n",
      "Epoch [17/1000], Loss: 0.1040, Val Loss: 0.4950\n",
      "Epoch [18/1000], Loss: 0.2139, Val Loss: 0.4924\n",
      "Epoch [19/1000], Loss: 0.1903, Val Loss: 0.4900\n",
      "Epoch [20/1000], Loss: 0.7408, Val Loss: 0.4879\n",
      "Epoch [21/1000], Loss: 1.1317, Val Loss: 0.4861\n",
      "Epoch [22/1000], Loss: 0.3912, Val Loss: 0.4843\n",
      "Epoch [23/1000], Loss: 0.5608, Val Loss: 0.4828\n",
      "Epoch [24/1000], Loss: 0.3658, Val Loss: 0.4814\n",
      "Epoch [25/1000], Loss: 2.3932, Val Loss: 0.4801\n",
      "Epoch [26/1000], Loss: 0.8002, Val Loss: 0.4792\n",
      "Epoch [27/1000], Loss: 0.4966, Val Loss: 0.4783\n",
      "Epoch [28/1000], Loss: 0.5222, Val Loss: 0.4771\n",
      "Epoch [29/1000], Loss: 0.1172, Val Loss: 0.4761\n",
      "Epoch [30/1000], Loss: 0.4861, Val Loss: 0.4752\n",
      "Epoch [31/1000], Loss: 0.2658, Val Loss: 0.4745\n",
      "Epoch [32/1000], Loss: 0.0986, Val Loss: 0.4736\n",
      "Epoch [33/1000], Loss: 0.1540, Val Loss: 0.4728\n",
      "Epoch [34/1000], Loss: 0.0793, Val Loss: 0.4720\n",
      "Epoch [35/1000], Loss: 0.2310, Val Loss: 0.4714\n",
      "Epoch [36/1000], Loss: 0.1426, Val Loss: 0.4707\n",
      "Epoch [37/1000], Loss: 0.0716, Val Loss: 0.4701\n",
      "Epoch [38/1000], Loss: 0.1207, Val Loss: 0.4696\n",
      "Epoch [39/1000], Loss: 0.0144, Val Loss: 0.4690\n",
      "Epoch [40/1000], Loss: 0.1718, Val Loss: 0.4686\n",
      "Epoch [41/1000], Loss: 0.3641, Val Loss: 0.4681\n",
      "Epoch [42/1000], Loss: 0.3203, Val Loss: 0.4677\n",
      "Epoch [43/1000], Loss: 0.1819, Val Loss: 0.4672\n",
      "Epoch [44/1000], Loss: 0.5760, Val Loss: 0.4666\n",
      "Epoch [45/1000], Loss: 0.0762, Val Loss: 0.4661\n",
      "Epoch [46/1000], Loss: 0.0551, Val Loss: 0.4658\n",
      "Epoch [47/1000], Loss: 1.7754, Val Loss: 0.4656\n",
      "Epoch [48/1000], Loss: 0.6546, Val Loss: 0.4653\n",
      "Epoch [49/1000], Loss: 0.5042, Val Loss: 0.4650\n",
      "Epoch [50/1000], Loss: 1.5359, Val Loss: 0.4646\n",
      "Epoch [51/1000], Loss: 0.0373, Val Loss: 0.4644\n",
      "Epoch [52/1000], Loss: 0.4489, Val Loss: 0.4640\n",
      "Epoch [53/1000], Loss: 1.5415, Val Loss: 0.4636\n",
      "Epoch [54/1000], Loss: 0.1523, Val Loss: 0.4634\n",
      "Epoch [55/1000], Loss: 0.0417, Val Loss: 0.4630\n",
      "Epoch [56/1000], Loss: 0.0858, Val Loss: 0.4628\n",
      "Epoch [57/1000], Loss: 0.4842, Val Loss: 0.4625\n",
      "Epoch [58/1000], Loss: 0.0390, Val Loss: 0.4621\n",
      "Epoch [59/1000], Loss: 0.2396, Val Loss: 0.4618\n",
      "Epoch [60/1000], Loss: 0.4325, Val Loss: 0.4616\n",
      "Epoch [61/1000], Loss: 1.5243, Val Loss: 0.4613\n",
      "Epoch [62/1000], Loss: 0.0147, Val Loss: 0.4612\n",
      "Epoch [63/1000], Loss: 0.0246, Val Loss: 0.4610\n",
      "Epoch [64/1000], Loss: 0.1040, Val Loss: 0.4608\n",
      "Epoch [65/1000], Loss: 0.4495, Val Loss: 0.4606\n",
      "Epoch [66/1000], Loss: 0.3567, Val Loss: 0.4603\n",
      "Epoch [67/1000], Loss: 0.0892, Val Loss: 0.4600\n",
      "Epoch [68/1000], Loss: 0.2751, Val Loss: 0.4598\n",
      "Epoch [69/1000], Loss: 0.0965, Val Loss: 0.4595\n",
      "Epoch [70/1000], Loss: 0.0192, Val Loss: 0.4594\n",
      "Epoch [71/1000], Loss: 1.0825, Val Loss: 0.4592\n",
      "Epoch [72/1000], Loss: 0.0111, Val Loss: 0.4591\n",
      "Epoch [73/1000], Loss: 0.5905, Val Loss: 0.4589\n",
      "Epoch [74/1000], Loss: 0.8696, Val Loss: 0.4588\n",
      "Epoch [75/1000], Loss: 0.5677, Val Loss: 0.4585\n",
      "Epoch [76/1000], Loss: 1.0804, Val Loss: 0.4583\n",
      "Epoch [77/1000], Loss: 1.0066, Val Loss: 0.4581\n",
      "Epoch [78/1000], Loss: 0.0775, Val Loss: 0.4579\n",
      "Epoch [79/1000], Loss: 0.0404, Val Loss: 0.4578\n",
      "Epoch [80/1000], Loss: 0.5062, Val Loss: 0.4575\n",
      "Epoch [81/1000], Loss: 0.3986, Val Loss: 0.4574\n",
      "Epoch [82/1000], Loss: 0.6489, Val Loss: 0.4572\n",
      "Epoch [83/1000], Loss: 0.8616, Val Loss: 0.4571\n",
      "Epoch [84/1000], Loss: 0.6452, Val Loss: 0.4569\n",
      "Epoch [85/1000], Loss: 0.1975, Val Loss: 0.4569\n",
      "Epoch [86/1000], Loss: 0.0379, Val Loss: 0.4568\n",
      "Epoch [87/1000], Loss: 0.5109, Val Loss: 0.4567\n",
      "Epoch [88/1000], Loss: 0.5011, Val Loss: 0.4565\n",
      "Epoch [89/1000], Loss: 0.5725, Val Loss: 0.4564\n",
      "Epoch [90/1000], Loss: 0.8055, Val Loss: 0.4563\n",
      "Epoch [91/1000], Loss: 0.6052, Val Loss: 0.4561\n",
      "Epoch [92/1000], Loss: 1.0270, Val Loss: 0.4559\n",
      "Epoch [93/1000], Loss: 0.0016, Val Loss: 0.4559\n",
      "Epoch [94/1000], Loss: 0.1854, Val Loss: 0.4558\n",
      "Epoch [95/1000], Loss: 0.0629, Val Loss: 0.4557\n",
      "Epoch [96/1000], Loss: 0.0099, Val Loss: 0.4556\n",
      "Epoch [97/1000], Loss: 0.1483, Val Loss: 0.4556\n",
      "Epoch [98/1000], Loss: 0.0400, Val Loss: 0.4555\n",
      "Epoch [99/1000], Loss: 0.0459, Val Loss: 0.4555\n",
      "Epoch [100/1000], Loss: 0.0712, Val Loss: 0.4553\n",
      "Epoch [101/1000], Loss: 1.4487, Val Loss: 0.4552\n",
      "Epoch [102/1000], Loss: 0.5904, Val Loss: 0.4552\n",
      "Epoch [103/1000], Loss: 0.0188, Val Loss: 0.4549\n",
      "Epoch [104/1000], Loss: 0.1182, Val Loss: 0.4549\n",
      "Epoch [105/1000], Loss: 0.0052, Val Loss: 0.4547\n",
      "Epoch [106/1000], Loss: 0.0820, Val Loss: 0.4545\n",
      "Epoch [107/1000], Loss: 0.0164, Val Loss: 0.4544\n",
      "Epoch [108/1000], Loss: 0.1235, Val Loss: 0.4544\n",
      "Epoch [109/1000], Loss: 0.7404, Val Loss: 0.4543\n",
      "Epoch [110/1000], Loss: 0.0418, Val Loss: 0.4542\n",
      "Epoch [111/1000], Loss: 0.2934, Val Loss: 0.4542\n",
      "Epoch [112/1000], Loss: 0.0266, Val Loss: 0.4542\n",
      "Epoch [113/1000], Loss: 0.2223, Val Loss: 0.4542\n",
      "Epoch [114/1000], Loss: 0.0027, Val Loss: 0.4541\n",
      "Epoch [115/1000], Loss: 0.1472, Val Loss: 0.4541\n",
      "Epoch [116/1000], Loss: 0.0389, Val Loss: 0.4540\n",
      "Epoch [117/1000], Loss: 0.1234, Val Loss: 0.4540\n",
      "Epoch [118/1000], Loss: 0.0116, Val Loss: 0.4539\n",
      "Epoch [119/1000], Loss: 0.0589, Val Loss: 0.4538\n",
      "Epoch [120/1000], Loss: 0.7791, Val Loss: 0.4537\n",
      "Epoch [121/1000], Loss: 1.5922, Val Loss: 0.4535\n",
      "Epoch [122/1000], Loss: 0.1204, Val Loss: 0.4535\n",
      "Epoch [123/1000], Loss: 0.7469, Val Loss: 0.4534\n",
      "Epoch [124/1000], Loss: 0.2420, Val Loss: 0.4532\n",
      "Epoch [125/1000], Loss: 0.2091, Val Loss: 0.4532\n",
      "Epoch [126/1000], Loss: 0.0837, Val Loss: 0.4531\n",
      "Epoch [127/1000], Loss: 0.7558, Val Loss: 0.4530\n",
      "Epoch [128/1000], Loss: 0.5638, Val Loss: 0.4529\n",
      "Epoch [129/1000], Loss: 0.7547, Val Loss: 0.4529\n",
      "Epoch [130/1000], Loss: 0.0102, Val Loss: 0.4528\n",
      "Epoch [131/1000], Loss: 0.1726, Val Loss: 0.4527\n",
      "Epoch [132/1000], Loss: 0.1990, Val Loss: 0.4526\n",
      "Epoch [133/1000], Loss: 0.3843, Val Loss: 0.4525\n",
      "Epoch [134/1000], Loss: 0.9986, Val Loss: 0.4524\n",
      "Epoch [135/1000], Loss: 0.0123, Val Loss: 0.4524\n",
      "Epoch [136/1000], Loss: 0.0084, Val Loss: 0.4523\n",
      "Epoch [137/1000], Loss: 1.2855, Val Loss: 0.4523\n",
      "Epoch [138/1000], Loss: 0.4084, Val Loss: 0.4523\n",
      "Epoch [139/1000], Loss: 0.7123, Val Loss: 0.4522\n",
      "Epoch [140/1000], Loss: 0.0446, Val Loss: 0.4521\n",
      "Epoch [141/1000], Loss: 0.7249, Val Loss: 0.4521\n",
      "Epoch [142/1000], Loss: 0.0974, Val Loss: 0.4521\n",
      "Epoch [143/1000], Loss: 0.5611, Val Loss: 0.4520\n",
      "Epoch [144/1000], Loss: 0.0873, Val Loss: 0.4521\n",
      "Epoch [145/1000], Loss: 0.0822, Val Loss: 0.4520\n",
      "Epoch [146/1000], Loss: 0.5001, Val Loss: 0.4520\n",
      "Epoch [147/1000], Loss: 0.4954, Val Loss: 0.4520\n",
      "Epoch [148/1000], Loss: 0.9014, Val Loss: 0.4519\n",
      "Epoch [149/1000], Loss: 0.0245, Val Loss: 0.4518\n",
      "Epoch [150/1000], Loss: 0.1207, Val Loss: 0.4516\n",
      "Epoch [151/1000], Loss: 0.8057, Val Loss: 0.4516\n",
      "Epoch [152/1000], Loss: 0.1425, Val Loss: 0.4517\n",
      "Epoch [153/1000], Loss: 0.1074, Val Loss: 0.4515\n",
      "Epoch [154/1000], Loss: 0.4180, Val Loss: 0.4514\n",
      "Epoch [155/1000], Loss: 0.0902, Val Loss: 0.4514\n",
      "Epoch [156/1000], Loss: 0.3679, Val Loss: 0.4512\n",
      "Epoch [157/1000], Loss: 0.0101, Val Loss: 0.4511\n",
      "Epoch [158/1000], Loss: 0.4928, Val Loss: 0.4511\n",
      "Epoch [159/1000], Loss: 1.2178, Val Loss: 0.4509\n",
      "Epoch [160/1000], Loss: 0.1317, Val Loss: 0.4509\n",
      "Epoch [161/1000], Loss: 0.2648, Val Loss: 0.4508\n",
      "Epoch [162/1000], Loss: 0.4660, Val Loss: 0.4508\n",
      "Epoch [163/1000], Loss: 0.0575, Val Loss: 0.4507\n",
      "Epoch [164/1000], Loss: 0.5835, Val Loss: 0.4507\n",
      "Epoch [165/1000], Loss: 0.3546, Val Loss: 0.4506\n",
      "Epoch [166/1000], Loss: 0.0171, Val Loss: 0.4505\n",
      "Epoch [167/1000], Loss: 1.1901, Val Loss: 0.4504\n",
      "Epoch [168/1000], Loss: 0.0114, Val Loss: 0.4505\n",
      "Epoch [169/1000], Loss: 0.0582, Val Loss: 0.4505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 0.6333, Val Loss: 0.4506\n",
      "Epoch [171/1000], Loss: 0.3891, Val Loss: 0.4504\n",
      "Epoch [172/1000], Loss: 0.0230, Val Loss: 0.4503\n",
      "Epoch [173/1000], Loss: 0.0063, Val Loss: 0.4502\n",
      "Epoch [174/1000], Loss: 1.0656, Val Loss: 0.4503\n",
      "Epoch [175/1000], Loss: 0.0043, Val Loss: 0.4503\n",
      "Epoch [176/1000], Loss: 0.4787, Val Loss: 0.4503\n",
      "Epoch [177/1000], Loss: 1.2858, Val Loss: 0.4503\n",
      "Epoch [178/1000], Loss: 0.7106, Val Loss: 0.4503\n",
      "Epoch [179/1000], Loss: 4.6005, Val Loss: 0.4504\n",
      "Epoch [180/1000], Loss: 0.0211, Val Loss: 0.4503\n",
      "Epoch [181/1000], Loss: 0.0901, Val Loss: 0.4502\n",
      "Epoch [182/1000], Loss: 1.4536, Val Loss: 0.4501\n",
      "Epoch [183/1000], Loss: 0.9892, Val Loss: 0.4500\n",
      "Epoch [184/1000], Loss: 1.3240, Val Loss: 0.4500\n",
      "Epoch [185/1000], Loss: 0.0920, Val Loss: 0.4500\n",
      "Epoch [186/1000], Loss: 0.0314, Val Loss: 0.4499\n",
      "Epoch [187/1000], Loss: 0.0948, Val Loss: 0.4498\n",
      "Epoch [188/1000], Loss: 0.4870, Val Loss: 0.4497\n",
      "Epoch [189/1000], Loss: 0.9548, Val Loss: 0.4497\n",
      "Epoch [190/1000], Loss: 0.5423, Val Loss: 0.4498\n",
      "Epoch [191/1000], Loss: 0.0593, Val Loss: 0.4498\n",
      "Epoch [192/1000], Loss: 0.0353, Val Loss: 0.4498\n",
      "Epoch [193/1000], Loss: 0.0598, Val Loss: 0.4496\n",
      "Epoch [194/1000], Loss: 0.6063, Val Loss: 0.4497\n",
      "Epoch [195/1000], Loss: 0.4620, Val Loss: 0.4496\n",
      "Epoch [196/1000], Loss: 0.3415, Val Loss: 0.4496\n",
      "Epoch [197/1000], Loss: 0.2961, Val Loss: 0.4497\n",
      "Epoch [198/1000], Loss: 0.6079, Val Loss: 0.4496\n",
      "Epoch [199/1000], Loss: 0.2871, Val Loss: 0.4497\n",
      "Epoch [200/1000], Loss: 0.3823, Val Loss: 0.4497\n",
      "Epoch [201/1000], Loss: 0.0474, Val Loss: 0.4496\n",
      "Epoch [202/1000], Loss: 0.1294, Val Loss: 0.4496\n",
      "Epoch [203/1000], Loss: 0.2575, Val Loss: 0.4495\n",
      "Epoch [204/1000], Loss: 1.4966, Val Loss: 0.4494\n",
      "Epoch [205/1000], Loss: 0.4954, Val Loss: 0.4494\n",
      "Epoch [206/1000], Loss: 0.2237, Val Loss: 0.4495\n",
      "Epoch [207/1000], Loss: 0.0016, Val Loss: 0.4496\n",
      "Epoch [208/1000], Loss: 0.5507, Val Loss: 0.4495\n",
      "Epoch [209/1000], Loss: 0.0497, Val Loss: 0.4494\n",
      "Epoch [210/1000], Loss: 1.1138, Val Loss: 0.4493\n",
      "Epoch [211/1000], Loss: 0.8606, Val Loss: 0.4493\n",
      "Epoch [212/1000], Loss: 0.0045, Val Loss: 0.4491\n",
      "Epoch [213/1000], Loss: 0.0128, Val Loss: 0.4491\n",
      "Epoch [214/1000], Loss: 0.6207, Val Loss: 0.4493\n",
      "Epoch [215/1000], Loss: 0.0073, Val Loss: 0.4493\n",
      "Epoch [216/1000], Loss: 0.9644, Val Loss: 0.4492\n",
      "Epoch [217/1000], Loss: 0.4286, Val Loss: 0.4492\n",
      "Epoch [218/1000], Loss: 0.0367, Val Loss: 0.4491\n",
      "Epoch [219/1000], Loss: 0.7236, Val Loss: 0.4492\n",
      "Epoch [220/1000], Loss: 0.4725, Val Loss: 0.4491\n",
      "Epoch [221/1000], Loss: 0.1473, Val Loss: 0.4491\n",
      "Epoch [222/1000], Loss: 0.6210, Val Loss: 0.4491\n",
      "Epoch [223/1000], Loss: 0.8352, Val Loss: 0.4490\n",
      "Epoch [224/1000], Loss: 0.5252, Val Loss: 0.4490\n",
      "Epoch [225/1000], Loss: 0.3279, Val Loss: 0.4491\n",
      "Epoch [226/1000], Loss: 0.9607, Val Loss: 0.4490\n",
      "Epoch [227/1000], Loss: 1.2653, Val Loss: 0.4490\n",
      "Epoch [228/1000], Loss: 0.0061, Val Loss: 0.4490\n",
      "Epoch [229/1000], Loss: 0.3234, Val Loss: 0.4489\n",
      "Epoch [230/1000], Loss: 0.0074, Val Loss: 0.4490\n",
      "Epoch [231/1000], Loss: 0.5054, Val Loss: 0.4489\n",
      "Epoch [232/1000], Loss: 2.1766, Val Loss: 0.4488\n",
      "Epoch [233/1000], Loss: 0.0815, Val Loss: 0.4488\n",
      "Epoch [234/1000], Loss: 0.0160, Val Loss: 0.4488\n",
      "Epoch [235/1000], Loss: 1.8624, Val Loss: 0.4489\n",
      "Epoch [236/1000], Loss: 0.7631, Val Loss: 0.4490\n",
      "Epoch [237/1000], Loss: 0.8982, Val Loss: 0.4490\n",
      "Epoch [238/1000], Loss: 0.6329, Val Loss: 0.4489\n",
      "Epoch [239/1000], Loss: 0.1649, Val Loss: 0.4488\n",
      "Epoch [240/1000], Loss: 1.3971, Val Loss: 0.4487\n",
      "Epoch [241/1000], Loss: 0.0296, Val Loss: 0.4488\n",
      "Epoch [242/1000], Loss: 0.0075, Val Loss: 0.4487\n",
      "Epoch [243/1000], Loss: 0.2096, Val Loss: 0.4488\n",
      "Epoch [244/1000], Loss: 0.2629, Val Loss: 0.4486\n",
      "Epoch [245/1000], Loss: 1.6011, Val Loss: 0.4487\n",
      "Epoch [246/1000], Loss: 0.6646, Val Loss: 0.4486\n",
      "Epoch [247/1000], Loss: 0.0000, Val Loss: 0.4486\n",
      "Epoch [248/1000], Loss: 0.1760, Val Loss: 0.4486\n",
      "Epoch [249/1000], Loss: 0.3446, Val Loss: 0.4486\n",
      "Epoch [250/1000], Loss: 0.0057, Val Loss: 0.4487\n",
      "Epoch [251/1000], Loss: 0.0280, Val Loss: 0.4487\n",
      "Epoch [252/1000], Loss: 0.0077, Val Loss: 0.4487\n",
      "Epoch [253/1000], Loss: 0.1610, Val Loss: 0.4487\n",
      "Epoch [254/1000], Loss: 0.0050, Val Loss: 0.4487\n",
      "Epoch [255/1000], Loss: 1.3624, Val Loss: 0.4488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:27:19,804] Trial 63 finished with value: 0.8215944371090503 and parameters: {'hidden_size': 91, 'dropout_prob': 0.735537020964784, 'learning_rate': 4.9799368353042244e-05, 'weight_decay': 9.970548093801178e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 256\n",
      "Epoch [1/1000], Loss: 0.0672, Val Loss: 0.4678\n",
      "Epoch [2/1000], Loss: 0.4086, Val Loss: 0.4624\n",
      "Epoch [3/1000], Loss: 0.6902, Val Loss: 0.4575\n",
      "Epoch [4/1000], Loss: 0.1742, Val Loss: 0.4558\n",
      "Epoch [5/1000], Loss: 0.4277, Val Loss: 0.4580\n",
      "Epoch [6/1000], Loss: 0.1100, Val Loss: 0.4521\n",
      "Epoch [7/1000], Loss: 1.1933, Val Loss: 0.4496\n",
      "Epoch [8/1000], Loss: 0.4892, Val Loss: 0.4516\n",
      "Epoch [9/1000], Loss: 0.1204, Val Loss: 0.4539\n",
      "Epoch [10/1000], Loss: 0.5174, Val Loss: 0.4522\n",
      "Epoch [11/1000], Loss: 0.1125, Val Loss: 0.4507\n",
      "Epoch [12/1000], Loss: 0.0008, Val Loss: 0.4505\n",
      "Epoch [13/1000], Loss: 0.6256, Val Loss: 0.4501\n",
      "Epoch [14/1000], Loss: 0.0035, Val Loss: 0.4533\n",
      "Epoch [15/1000], Loss: 0.0026, Val Loss: 0.4492\n",
      "Epoch [16/1000], Loss: 1.4699, Val Loss: 0.4508\n",
      "Epoch [17/1000], Loss: 0.2206, Val Loss: 0.4496\n",
      "Epoch [18/1000], Loss: 0.0269, Val Loss: 0.4514\n",
      "Epoch [19/1000], Loss: 0.1208, Val Loss: 0.4503\n",
      "Epoch [20/1000], Loss: 1.0259, Val Loss: 0.4491\n",
      "Epoch [21/1000], Loss: 0.4510, Val Loss: 0.4482\n",
      "Epoch [22/1000], Loss: 0.4294, Val Loss: 0.4472\n",
      "Epoch [23/1000], Loss: 2.8514, Val Loss: 0.4493\n",
      "Epoch [24/1000], Loss: 0.5557, Val Loss: 0.4493\n",
      "Epoch [25/1000], Loss: 0.7928, Val Loss: 0.4516\n",
      "Epoch [26/1000], Loss: 0.3693, Val Loss: 0.4473\n",
      "Epoch [27/1000], Loss: 2.5092, Val Loss: 0.4523\n",
      "Epoch [28/1000], Loss: 0.9751, Val Loss: 0.4481\n",
      "Epoch [29/1000], Loss: 0.3152, Val Loss: 0.4478\n",
      "Epoch [30/1000], Loss: 0.4551, Val Loss: 0.4491\n",
      "Epoch [31/1000], Loss: 0.1929, Val Loss: 0.4465\n",
      "Epoch [32/1000], Loss: 2.5081, Val Loss: 0.4510\n",
      "Epoch [33/1000], Loss: 0.1742, Val Loss: 0.4494\n",
      "Epoch [34/1000], Loss: 0.2526, Val Loss: 0.4472\n",
      "Epoch [35/1000], Loss: 0.3418, Val Loss: 0.4491\n",
      "Epoch [36/1000], Loss: 0.0124, Val Loss: 0.4470\n",
      "Epoch [37/1000], Loss: 0.0262, Val Loss: 0.4476\n",
      "Epoch [38/1000], Loss: 0.1671, Val Loss: 0.4505\n",
      "Epoch [39/1000], Loss: 0.4055, Val Loss: 0.4464\n",
      "Epoch [40/1000], Loss: 0.2824, Val Loss: 0.4473\n",
      "Epoch [41/1000], Loss: 0.0221, Val Loss: 0.4483\n",
      "Epoch [42/1000], Loss: 1.2973, Val Loss: 0.4501\n",
      "Epoch [43/1000], Loss: 0.5418, Val Loss: 0.4475\n",
      "Epoch [44/1000], Loss: 0.2417, Val Loss: 0.4475\n",
      "Epoch [45/1000], Loss: 1.0400, Val Loss: 0.4485\n",
      "Epoch [46/1000], Loss: 0.0102, Val Loss: 0.4489\n",
      "Epoch [47/1000], Loss: 0.7070, Val Loss: 0.4453\n",
      "Epoch [48/1000], Loss: 0.5189, Val Loss: 0.4456\n",
      "Epoch [49/1000], Loss: 1.2160, Val Loss: 0.4475\n",
      "Epoch [50/1000], Loss: 0.8291, Val Loss: 0.4460\n",
      "Epoch [51/1000], Loss: 0.8777, Val Loss: 0.4546\n",
      "Epoch [52/1000], Loss: 2.6380, Val Loss: 0.4484\n",
      "Epoch [53/1000], Loss: 0.0005, Val Loss: 0.4484\n",
      "Epoch [54/1000], Loss: 0.3418, Val Loss: 0.4468\n",
      "Epoch [55/1000], Loss: 0.3946, Val Loss: 0.4477\n",
      "Epoch [56/1000], Loss: 0.3328, Val Loss: 0.4484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:27:35,979] Trial 64 finished with value: 0.8216938472753967 and parameters: {'hidden_size': 74, 'dropout_prob': 0.6629963804314875, 'learning_rate': 0.0019276208598931293, 'weight_decay': 9.605359592141133e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 57\n",
      "Epoch [1/1000], Loss: 0.5844, Val Loss: 0.4836\n",
      "Epoch [2/1000], Loss: 0.1559, Val Loss: 0.4697\n",
      "Epoch [3/1000], Loss: 0.0862, Val Loss: 0.4633\n",
      "Epoch [4/1000], Loss: 0.2194, Val Loss: 0.4624\n",
      "Epoch [5/1000], Loss: 0.4752, Val Loss: 0.4594\n",
      "Epoch [6/1000], Loss: 0.6330, Val Loss: 0.4563\n",
      "Epoch [7/1000], Loss: 0.7065, Val Loss: 0.4547\n",
      "Epoch [8/1000], Loss: 1.0213, Val Loss: 0.4551\n",
      "Epoch [9/1000], Loss: 0.8602, Val Loss: 0.4584\n",
      "Epoch [10/1000], Loss: 0.7949, Val Loss: 0.4518\n",
      "Epoch [11/1000], Loss: 1.4259, Val Loss: 0.4532\n",
      "Epoch [12/1000], Loss: 0.0576, Val Loss: 0.4524\n",
      "Epoch [13/1000], Loss: 0.4978, Val Loss: 0.4509\n",
      "Epoch [14/1000], Loss: 0.0782, Val Loss: 0.4503\n",
      "Epoch [15/1000], Loss: 0.3631, Val Loss: 0.4518\n",
      "Epoch [16/1000], Loss: 0.0795, Val Loss: 0.4553\n",
      "Epoch [17/1000], Loss: 0.4991, Val Loss: 0.4510\n",
      "Epoch [18/1000], Loss: 0.0826, Val Loss: 0.4492\n",
      "Epoch [19/1000], Loss: 0.0251, Val Loss: 0.4515\n",
      "Epoch [20/1000], Loss: 0.1036, Val Loss: 0.4524\n",
      "Epoch [21/1000], Loss: 0.0150, Val Loss: 0.4509\n",
      "Epoch [22/1000], Loss: 0.0040, Val Loss: 0.4510\n",
      "Epoch [23/1000], Loss: 0.0019, Val Loss: 0.4506\n",
      "Epoch [24/1000], Loss: 0.5865, Val Loss: 0.4515\n",
      "Epoch [25/1000], Loss: 0.0050, Val Loss: 0.4505\n",
      "Epoch [26/1000], Loss: 0.3641, Val Loss: 0.4523\n",
      "Epoch [27/1000], Loss: 0.3774, Val Loss: 0.4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:27:44,050] Trial 65 finished with value: 0.8189358524039418 and parameters: {'hidden_size': 83, 'dropout_prob': 0.6863144660092261, 'learning_rate': 0.0010767971641192594, 'weight_decay': 8.324913731650871e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 28\n",
      "Epoch [1/1000], Loss: 0.2786, Val Loss: 0.5245\n",
      "Epoch [2/1000], Loss: 0.3452, Val Loss: 0.4750\n",
      "Epoch [3/1000], Loss: 0.7076, Val Loss: 0.4644\n",
      "Epoch [4/1000], Loss: 0.2357, Val Loss: 0.4582\n",
      "Epoch [5/1000], Loss: 0.6968, Val Loss: 0.4550\n",
      "Epoch [6/1000], Loss: 0.2861, Val Loss: 0.4527\n",
      "Epoch [7/1000], Loss: 0.1576, Val Loss: 0.4516\n",
      "Epoch [8/1000], Loss: 0.6425, Val Loss: 0.4501\n",
      "Epoch [9/1000], Loss: 0.0245, Val Loss: 0.4490\n",
      "Epoch [10/1000], Loss: 0.4087, Val Loss: 0.4487\n",
      "Epoch [11/1000], Loss: 0.1014, Val Loss: 0.4472\n",
      "Epoch [12/1000], Loss: 0.3603, Val Loss: 0.4482\n",
      "Epoch [13/1000], Loss: 0.7679, Val Loss: 0.4470\n",
      "Epoch [14/1000], Loss: 0.7890, Val Loss: 0.4462\n",
      "Epoch [15/1000], Loss: 0.1646, Val Loss: 0.4480\n",
      "Epoch [16/1000], Loss: 0.0520, Val Loss: 0.4467\n",
      "Epoch [17/1000], Loss: 0.1315, Val Loss: 0.4469\n",
      "Epoch [18/1000], Loss: 1.6815, Val Loss: 0.4449\n",
      "Epoch [19/1000], Loss: 0.6818, Val Loss: 0.4451\n",
      "Epoch [20/1000], Loss: 0.1312, Val Loss: 0.4459\n",
      "Epoch [21/1000], Loss: 0.1758, Val Loss: 0.4472\n",
      "Epoch [22/1000], Loss: 0.2544, Val Loss: 0.4456\n",
      "Epoch [23/1000], Loss: 3.4614, Val Loss: 0.4457\n",
      "Epoch [24/1000], Loss: 0.3251, Val Loss: 0.4457\n",
      "Epoch [25/1000], Loss: 0.0039, Val Loss: 0.4467\n",
      "Epoch [26/1000], Loss: 1.0263, Val Loss: 0.4487\n",
      "Epoch [27/1000], Loss: 0.0341, Val Loss: 0.4472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:27:52,622] Trial 66 finished with value: 0.8216352207670387 and parameters: {'hidden_size': 96, 'dropout_prob': 0.2085435779009347, 'learning_rate': 0.0003277274672711832, 'weight_decay': 6.298497324440608e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 28\n",
      "Epoch [1/1000], Loss: 0.9424, Val Loss: 0.5016\n",
      "Epoch [2/1000], Loss: 0.6172, Val Loss: 0.4703\n",
      "Epoch [3/1000], Loss: 0.7018, Val Loss: 0.4611\n",
      "Epoch [4/1000], Loss: 0.0433, Val Loss: 0.4584\n",
      "Epoch [5/1000], Loss: 0.0284, Val Loss: 0.4540\n",
      "Epoch [6/1000], Loss: 1.3244, Val Loss: 0.4522\n",
      "Epoch [7/1000], Loss: 0.7844, Val Loss: 0.4523\n",
      "Epoch [8/1000], Loss: 0.4104, Val Loss: 0.4515\n",
      "Epoch [9/1000], Loss: 0.2537, Val Loss: 0.4493\n",
      "Epoch [10/1000], Loss: 0.0037, Val Loss: 0.4480\n",
      "Epoch [11/1000], Loss: 0.8928, Val Loss: 0.4488\n",
      "Epoch [12/1000], Loss: 0.2870, Val Loss: 0.4478\n",
      "Epoch [13/1000], Loss: 0.1573, Val Loss: 0.4479\n",
      "Epoch [14/1000], Loss: 0.3903, Val Loss: 0.4470\n",
      "Epoch [15/1000], Loss: 0.0027, Val Loss: 0.4474\n",
      "Epoch [16/1000], Loss: 0.5983, Val Loss: 0.4459\n",
      "Epoch [17/1000], Loss: 0.2895, Val Loss: 0.4456\n",
      "Epoch [18/1000], Loss: 0.4692, Val Loss: 0.4454\n",
      "Epoch [19/1000], Loss: 0.0266, Val Loss: 0.4477\n",
      "Epoch [20/1000], Loss: 0.0452, Val Loss: 0.4478\n",
      "Epoch [21/1000], Loss: 0.6720, Val Loss: 0.4488\n",
      "Epoch [22/1000], Loss: 1.0236, Val Loss: 0.4472\n",
      "Epoch [23/1000], Loss: 0.9762, Val Loss: 0.4476\n",
      "Epoch [24/1000], Loss: 0.0131, Val Loss: 0.4489\n",
      "Epoch [25/1000], Loss: 0.8969, Val Loss: 0.4488\n",
      "Epoch [26/1000], Loss: 0.1910, Val Loss: 0.4473\n",
      "Epoch [27/1000], Loss: 0.0396, Val Loss: 0.4474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:28:01,360] Trial 67 finished with value: 0.8226140285587565 and parameters: {'hidden_size': 65, 'dropout_prob': 0.6032195533065229, 'learning_rate': 0.0007478124698854673, 'weight_decay': 6.909841199572087e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 28\n",
      "Epoch [1/1000], Loss: 0.0343, Val Loss: 0.4614\n",
      "Epoch [2/1000], Loss: 1.0635, Val Loss: 0.4536\n",
      "Epoch [3/1000], Loss: 0.0522, Val Loss: 0.4624\n",
      "Epoch [4/1000], Loss: 0.2100, Val Loss: 0.4534\n",
      "Epoch [5/1000], Loss: 0.3718, Val Loss: 0.4542\n",
      "Epoch [6/1000], Loss: 0.0157, Val Loss: 0.4535\n",
      "Epoch [7/1000], Loss: 0.2592, Val Loss: 0.4521\n",
      "Epoch [8/1000], Loss: 0.0134, Val Loss: 0.4523\n",
      "Epoch [9/1000], Loss: 0.2747, Val Loss: 0.4539\n",
      "Epoch [10/1000], Loss: 0.2222, Val Loss: 0.4579\n",
      "Epoch [11/1000], Loss: 0.3523, Val Loss: 0.4527\n",
      "Epoch [12/1000], Loss: 0.1456, Val Loss: 0.4514\n",
      "Epoch [13/1000], Loss: 0.0981, Val Loss: 0.4551\n",
      "Epoch [14/1000], Loss: 0.3527, Val Loss: 0.4534\n",
      "Epoch [15/1000], Loss: 0.1307, Val Loss: 0.4495\n",
      "Epoch [16/1000], Loss: 0.0135, Val Loss: 0.4494\n",
      "Epoch [17/1000], Loss: 0.6590, Val Loss: 0.4532\n",
      "Epoch [18/1000], Loss: 0.0019, Val Loss: 0.4502\n",
      "Epoch [19/1000], Loss: 0.2296, Val Loss: 0.4537\n",
      "Epoch [20/1000], Loss: 0.2071, Val Loss: 0.4480\n",
      "Epoch [21/1000], Loss: 0.3739, Val Loss: 0.4521\n",
      "Epoch [22/1000], Loss: 0.4703, Val Loss: 0.4515\n",
      "Epoch [23/1000], Loss: 0.0067, Val Loss: 0.4482\n",
      "Epoch [24/1000], Loss: 0.3871, Val Loss: 0.4512\n",
      "Epoch [25/1000], Loss: 0.3006, Val Loss: 0.4502\n",
      "Epoch [26/1000], Loss: 0.0146, Val Loss: 0.4502\n",
      "Epoch [27/1000], Loss: 0.5950, Val Loss: 0.4472\n",
      "Epoch [28/1000], Loss: 0.0763, Val Loss: 0.4533\n",
      "Epoch [29/1000], Loss: 0.0064, Val Loss: 0.4493\n",
      "Epoch [30/1000], Loss: 0.0391, Val Loss: 0.4497\n",
      "Epoch [31/1000], Loss: 0.2636, Val Loss: 0.4523\n",
      "Epoch [32/1000], Loss: 0.4872, Val Loss: 0.4540\n",
      "Epoch [33/1000], Loss: 0.0549, Val Loss: 0.4486\n",
      "Epoch [34/1000], Loss: 0.0326, Val Loss: 0.4499\n",
      "Epoch [35/1000], Loss: 0.0104, Val Loss: 0.4518\n",
      "Epoch [36/1000], Loss: 0.3148, Val Loss: 0.4492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:28:11,866] Trial 68 finished with value: 0.8207558231416672 and parameters: {'hidden_size': 61, 'dropout_prob': 0.6404846337730182, 'learning_rate': 0.005149298671872984, 'weight_decay': 8.534712924948076e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 37\n",
      "Epoch [1/1000], Loss: 0.0269, Val Loss: 0.4722\n",
      "Epoch [2/1000], Loss: 0.1049, Val Loss: 0.4624\n",
      "Epoch [3/1000], Loss: 0.0239, Val Loss: 0.4554\n",
      "Epoch [4/1000], Loss: 0.5794, Val Loss: 0.4597\n",
      "Epoch [5/1000], Loss: 0.5552, Val Loss: 0.4554\n",
      "Epoch [6/1000], Loss: 0.9115, Val Loss: 0.4502\n",
      "Epoch [7/1000], Loss: 0.3054, Val Loss: 0.4501\n",
      "Epoch [8/1000], Loss: 0.5374, Val Loss: 0.4523\n",
      "Epoch [9/1000], Loss: 0.7149, Val Loss: 0.4603\n",
      "Epoch [10/1000], Loss: 0.5111, Val Loss: 0.4514\n",
      "Epoch [11/1000], Loss: 0.3828, Val Loss: 0.4462\n",
      "Epoch [12/1000], Loss: 0.3490, Val Loss: 0.4458\n",
      "Epoch [13/1000], Loss: 0.2532, Val Loss: 0.4450\n",
      "Epoch [14/1000], Loss: 0.7914, Val Loss: 0.4466\n",
      "Epoch [15/1000], Loss: 0.4335, Val Loss: 0.4505\n",
      "Epoch [16/1000], Loss: 0.6701, Val Loss: 0.4473\n",
      "Epoch [17/1000], Loss: 0.3204, Val Loss: 0.4501\n",
      "Epoch [18/1000], Loss: 0.8874, Val Loss: 0.4480\n",
      "Epoch [19/1000], Loss: 0.4143, Val Loss: 0.4488\n",
      "Epoch [20/1000], Loss: 1.4578, Val Loss: 0.4525\n",
      "Epoch [21/1000], Loss: 0.9483, Val Loss: 0.4520\n",
      "Epoch [22/1000], Loss: 0.0645, Val Loss: 0.4514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:28:18,436] Trial 69 finished with value: 0.8197948582003192 and parameters: {'hidden_size': 89, 'dropout_prob': 0.7156449364038124, 'learning_rate': 0.0015785419104129368, 'weight_decay': 5.339721123133855e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 23\n",
      "Epoch [1/1000], Loss: 0.7044, Val Loss: 0.4798\n",
      "Epoch [2/1000], Loss: 0.5173, Val Loss: 0.4638\n",
      "Epoch [3/1000], Loss: 2.1651, Val Loss: 0.4616\n",
      "Epoch [4/1000], Loss: 0.2088, Val Loss: 0.4587\n",
      "Epoch [5/1000], Loss: 0.0018, Val Loss: 0.4566\n",
      "Epoch [6/1000], Loss: 0.1044, Val Loss: 0.4527\n",
      "Epoch [7/1000], Loss: 0.2672, Val Loss: 0.4524\n",
      "Epoch [8/1000], Loss: 2.8259, Val Loss: 0.4525\n",
      "Epoch [9/1000], Loss: 0.4612, Val Loss: 0.4506\n",
      "Epoch [10/1000], Loss: 0.6465, Val Loss: 0.4518\n",
      "Epoch [11/1000], Loss: 0.3646, Val Loss: 0.4495\n",
      "Epoch [12/1000], Loss: 0.0312, Val Loss: 0.4475\n",
      "Epoch [13/1000], Loss: 0.0035, Val Loss: 0.4467\n",
      "Epoch [14/1000], Loss: 0.4069, Val Loss: 0.4464\n",
      "Epoch [15/1000], Loss: 0.3245, Val Loss: 0.4470\n",
      "Epoch [16/1000], Loss: 0.1280, Val Loss: 0.4460\n",
      "Epoch [17/1000], Loss: 0.0186, Val Loss: 0.4458\n",
      "Epoch [18/1000], Loss: 1.0047, Val Loss: 0.4472\n",
      "Epoch [19/1000], Loss: 0.0527, Val Loss: 0.4473\n",
      "Epoch [20/1000], Loss: 0.8396, Val Loss: 0.4468\n",
      "Epoch [21/1000], Loss: 0.1465, Val Loss: 0.4471\n",
      "Epoch [22/1000], Loss: 0.0143, Val Loss: 0.4465\n",
      "Epoch [23/1000], Loss: 0.2930, Val Loss: 0.4464\n",
      "Epoch [24/1000], Loss: 0.0222, Val Loss: 0.4475\n",
      "Epoch [25/1000], Loss: 0.2765, Val Loss: 0.4472\n",
      "Epoch [26/1000], Loss: 0.4157, Val Loss: 0.4459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:28:25,689] Trial 70 finished with value: 0.823128922240858 and parameters: {'hidden_size': 76, 'dropout_prob': 0.7526830122736794, 'learning_rate': 0.0012096706567924246, 'weight_decay': 7.969938630193244e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 27\n",
      "Epoch [1/1000], Loss: 0.1700, Val Loss: 0.4902\n",
      "Epoch [2/1000], Loss: 0.0866, Val Loss: 0.4722\n",
      "Epoch [3/1000], Loss: 0.0826, Val Loss: 0.4657\n",
      "Epoch [4/1000], Loss: 0.2651, Val Loss: 0.4625\n",
      "Epoch [5/1000], Loss: 0.0282, Val Loss: 0.4550\n",
      "Epoch [6/1000], Loss: 0.2187, Val Loss: 0.4556\n",
      "Epoch [7/1000], Loss: 0.9376, Val Loss: 0.4554\n",
      "Epoch [8/1000], Loss: 0.3458, Val Loss: 0.4532\n",
      "Epoch [9/1000], Loss: 0.6717, Val Loss: 0.4518\n",
      "Epoch [10/1000], Loss: 0.0126, Val Loss: 0.4503\n",
      "Epoch [11/1000], Loss: 0.0256, Val Loss: 0.4502\n",
      "Epoch [12/1000], Loss: 0.7183, Val Loss: 0.4512\n",
      "Epoch [13/1000], Loss: 0.0058, Val Loss: 0.4511\n",
      "Epoch [14/1000], Loss: 0.5310, Val Loss: 0.4522\n",
      "Epoch [15/1000], Loss: 0.2165, Val Loss: 0.4508\n",
      "Epoch [16/1000], Loss: 0.5937, Val Loss: 0.4505\n",
      "Epoch [17/1000], Loss: 0.6916, Val Loss: 0.4499\n",
      "Epoch [18/1000], Loss: 1.1587, Val Loss: 0.4495\n",
      "Epoch [19/1000], Loss: 0.0301, Val Loss: 0.4505\n",
      "Epoch [20/1000], Loss: 0.3908, Val Loss: 0.4504\n",
      "Epoch [21/1000], Loss: 0.0214, Val Loss: 0.4480\n",
      "Epoch [22/1000], Loss: 0.0850, Val Loss: 0.4492\n",
      "Epoch [23/1000], Loss: 0.0038, Val Loss: 0.4500\n",
      "Epoch [24/1000], Loss: 0.0450, Val Loss: 0.4508\n",
      "Epoch [25/1000], Loss: 0.2667, Val Loss: 0.4518\n",
      "Epoch [26/1000], Loss: 0.0006, Val Loss: 0.4511\n",
      "Epoch [27/1000], Loss: 0.4928, Val Loss: 0.4488\n",
      "Epoch [28/1000], Loss: 0.1919, Val Loss: 0.4489\n",
      "Epoch [29/1000], Loss: 0.2560, Val Loss: 0.4492\n",
      "Epoch [30/1000], Loss: 0.9232, Val Loss: 0.4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:28:33,979] Trial 71 finished with value: 0.8214160086053518 and parameters: {'hidden_size': 78, 'dropout_prob': 0.7456686999023775, 'learning_rate': 0.00111087430499536, 'weight_decay': 7.566574294685076e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 31\n",
      "Epoch [1/1000], Loss: 0.2665, Val Loss: 0.5485\n",
      "Epoch [2/1000], Loss: 0.9336, Val Loss: 0.5014\n",
      "Epoch [3/1000], Loss: 0.0320, Val Loss: 0.4836\n",
      "Epoch [4/1000], Loss: 0.0094, Val Loss: 0.4737\n",
      "Epoch [5/1000], Loss: 0.0443, Val Loss: 0.4680\n",
      "Epoch [6/1000], Loss: 0.0377, Val Loss: 0.4634\n",
      "Epoch [7/1000], Loss: 0.6699, Val Loss: 0.4601\n",
      "Epoch [8/1000], Loss: 1.3050, Val Loss: 0.4573\n",
      "Epoch [9/1000], Loss: 0.0068, Val Loss: 0.4560\n",
      "Epoch [10/1000], Loss: 0.4244, Val Loss: 0.4540\n",
      "Epoch [11/1000], Loss: 0.7481, Val Loss: 0.4530\n",
      "Epoch [12/1000], Loss: 0.0294, Val Loss: 0.4518\n",
      "Epoch [13/1000], Loss: 1.4716, Val Loss: 0.4505\n",
      "Epoch [14/1000], Loss: 0.4034, Val Loss: 0.4505\n",
      "Epoch [15/1000], Loss: 0.1616, Val Loss: 0.4508\n",
      "Epoch [16/1000], Loss: 1.2321, Val Loss: 0.4501\n",
      "Epoch [17/1000], Loss: 0.6407, Val Loss: 0.4502\n",
      "Epoch [18/1000], Loss: 0.1842, Val Loss: 0.4504\n",
      "Epoch [19/1000], Loss: 0.0656, Val Loss: 0.4491\n",
      "Epoch [20/1000], Loss: 0.0076, Val Loss: 0.4489\n",
      "Epoch [21/1000], Loss: 0.1483, Val Loss: 0.4488\n",
      "Epoch [22/1000], Loss: 0.3859, Val Loss: 0.4489\n",
      "Epoch [23/1000], Loss: 0.0525, Val Loss: 0.4484\n",
      "Epoch [24/1000], Loss: 0.0051, Val Loss: 0.4479\n",
      "Epoch [25/1000], Loss: 0.7923, Val Loss: 0.4474\n",
      "Epoch [26/1000], Loss: 0.6087, Val Loss: 0.4470\n",
      "Epoch [27/1000], Loss: 0.2679, Val Loss: 0.4473\n",
      "Epoch [28/1000], Loss: 0.0598, Val Loss: 0.4470\n",
      "Epoch [29/1000], Loss: 0.1374, Val Loss: 0.4475\n",
      "Epoch [30/1000], Loss: 1.5025, Val Loss: 0.4468\n",
      "Epoch [31/1000], Loss: 0.5212, Val Loss: 0.4467\n",
      "Epoch [32/1000], Loss: 0.0170, Val Loss: 0.4461\n",
      "Epoch [33/1000], Loss: 0.7698, Val Loss: 0.4458\n",
      "Epoch [34/1000], Loss: 0.0439, Val Loss: 0.4463\n",
      "Epoch [35/1000], Loss: 0.4199, Val Loss: 0.4460\n",
      "Epoch [36/1000], Loss: 0.1931, Val Loss: 0.4460\n",
      "Epoch [37/1000], Loss: 1.2770, Val Loss: 0.4457\n",
      "Epoch [38/1000], Loss: 0.8528, Val Loss: 0.4453\n",
      "Epoch [39/1000], Loss: 0.8527, Val Loss: 0.4451\n",
      "Epoch [40/1000], Loss: 1.1115, Val Loss: 0.4456\n",
      "Epoch [41/1000], Loss: 0.3576, Val Loss: 0.4457\n",
      "Epoch [42/1000], Loss: 0.1030, Val Loss: 0.4457\n",
      "Epoch [43/1000], Loss: 1.1980, Val Loss: 0.4457\n",
      "Epoch [44/1000], Loss: 0.2541, Val Loss: 0.4458\n",
      "Epoch [45/1000], Loss: 3.0455, Val Loss: 0.4459\n",
      "Epoch [46/1000], Loss: 0.0964, Val Loss: 0.4455\n",
      "Epoch [47/1000], Loss: 0.2646, Val Loss: 0.4450\n",
      "Epoch [48/1000], Loss: 0.1332, Val Loss: 0.4457\n",
      "Epoch [49/1000], Loss: 1.2521, Val Loss: 0.4457\n",
      "Epoch [50/1000], Loss: 0.0020, Val Loss: 0.4456\n",
      "Epoch [51/1000], Loss: 0.2864, Val Loss: 0.4454\n",
      "Epoch [52/1000], Loss: 0.0541, Val Loss: 0.4454\n",
      "Epoch [53/1000], Loss: 0.2341, Val Loss: 0.4455\n",
      "Epoch [54/1000], Loss: 0.3264, Val Loss: 0.4449\n",
      "Epoch [55/1000], Loss: 1.0212, Val Loss: 0.4449\n",
      "Epoch [56/1000], Loss: 0.4941, Val Loss: 0.4455\n",
      "Epoch [57/1000], Loss: 0.0478, Val Loss: 0.4459\n",
      "Epoch [58/1000], Loss: 0.0192, Val Loss: 0.4459\n",
      "Epoch [59/1000], Loss: 2.7870, Val Loss: 0.4457\n",
      "Epoch [60/1000], Loss: 1.4713, Val Loss: 0.4453\n",
      "Epoch [61/1000], Loss: 0.4946, Val Loss: 0.4448\n",
      "Epoch [62/1000], Loss: 0.0601, Val Loss: 0.4451\n",
      "Epoch [63/1000], Loss: 0.5142, Val Loss: 0.4448\n",
      "Epoch [64/1000], Loss: 0.0046, Val Loss: 0.4451\n",
      "Epoch [65/1000], Loss: 0.4270, Val Loss: 0.4450\n",
      "Epoch [66/1000], Loss: 1.5069, Val Loss: 0.4453\n",
      "Epoch [67/1000], Loss: 0.1420, Val Loss: 0.4458\n",
      "Epoch [68/1000], Loss: 0.4914, Val Loss: 0.4459\n",
      "Epoch [69/1000], Loss: 0.0342, Val Loss: 0.4457\n",
      "Epoch [70/1000], Loss: 0.1345, Val Loss: 0.4464\n",
      "Epoch [71/1000], Loss: 0.1015, Val Loss: 0.4460\n",
      "Epoch [72/1000], Loss: 0.7568, Val Loss: 0.4465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:28:55,132] Trial 72 finished with value: 0.8233557813384177 and parameters: {'hidden_size': 100, 'dropout_prob': 0.725024464399964, 'learning_rate': 0.00031500509339881346, 'weight_decay': 7.90012916688751e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 73\n",
      "Epoch [1/1000], Loss: 0.4639, Val Loss: 0.5500\n",
      "Epoch [2/1000], Loss: 0.4848, Val Loss: 0.5012\n",
      "Epoch [3/1000], Loss: 0.3041, Val Loss: 0.4846\n",
      "Epoch [4/1000], Loss: 0.7747, Val Loss: 0.4742\n",
      "Epoch [5/1000], Loss: 0.7542, Val Loss: 0.4691\n",
      "Epoch [6/1000], Loss: 0.3020, Val Loss: 0.4652\n",
      "Epoch [7/1000], Loss: 0.0129, Val Loss: 0.4621\n",
      "Epoch [8/1000], Loss: 0.3937, Val Loss: 0.4606\n",
      "Epoch [9/1000], Loss: 0.0772, Val Loss: 0.4583\n",
      "Epoch [10/1000], Loss: 4.1729, Val Loss: 0.4575\n",
      "Epoch [11/1000], Loss: 0.0360, Val Loss: 0.4552\n",
      "Epoch [12/1000], Loss: 0.5283, Val Loss: 0.4546\n",
      "Epoch [13/1000], Loss: 0.5637, Val Loss: 0.4537\n",
      "Epoch [14/1000], Loss: 0.0897, Val Loss: 0.4534\n",
      "Epoch [15/1000], Loss: 0.5984, Val Loss: 0.4531\n",
      "Epoch [16/1000], Loss: 0.8569, Val Loss: 0.4518\n",
      "Epoch [17/1000], Loss: 0.8362, Val Loss: 0.4511\n",
      "Epoch [18/1000], Loss: 1.1239, Val Loss: 0.4510\n",
      "Epoch [19/1000], Loss: 0.2194, Val Loss: 0.4495\n",
      "Epoch [20/1000], Loss: 0.6693, Val Loss: 0.4492\n",
      "Epoch [21/1000], Loss: 0.6438, Val Loss: 0.4484\n",
      "Epoch [22/1000], Loss: 0.5137, Val Loss: 0.4484\n",
      "Epoch [23/1000], Loss: 0.0010, Val Loss: 0.4481\n",
      "Epoch [24/1000], Loss: 0.0870, Val Loss: 0.4481\n",
      "Epoch [25/1000], Loss: 0.5237, Val Loss: 0.4477\n",
      "Epoch [26/1000], Loss: 0.0088, Val Loss: 0.4474\n",
      "Epoch [27/1000], Loss: 0.0052, Val Loss: 0.4476\n",
      "Epoch [28/1000], Loss: 0.7002, Val Loss: 0.4473\n",
      "Epoch [29/1000], Loss: 0.1450, Val Loss: 0.4468\n",
      "Epoch [30/1000], Loss: 0.5073, Val Loss: 0.4467\n",
      "Epoch [31/1000], Loss: 1.1274, Val Loss: 0.4464\n",
      "Epoch [32/1000], Loss: 0.0328, Val Loss: 0.4457\n",
      "Epoch [33/1000], Loss: 0.3055, Val Loss: 0.4458\n",
      "Epoch [34/1000], Loss: 0.8729, Val Loss: 0.4460\n",
      "Epoch [35/1000], Loss: 0.6898, Val Loss: 0.4458\n",
      "Epoch [36/1000], Loss: 0.0899, Val Loss: 0.4463\n",
      "Epoch [37/1000], Loss: 0.7075, Val Loss: 0.4466\n",
      "Epoch [38/1000], Loss: 0.4961, Val Loss: 0.4451\n",
      "Epoch [39/1000], Loss: 0.0065, Val Loss: 0.4451\n",
      "Epoch [40/1000], Loss: 1.4005, Val Loss: 0.4452\n",
      "Epoch [41/1000], Loss: 0.8705, Val Loss: 0.4452\n",
      "Epoch [42/1000], Loss: 0.5361, Val Loss: 0.4456\n",
      "Epoch [43/1000], Loss: 0.0937, Val Loss: 0.4455\n",
      "Epoch [44/1000], Loss: 0.0440, Val Loss: 0.4457\n",
      "Epoch [45/1000], Loss: 0.8543, Val Loss: 0.4458\n",
      "Epoch [46/1000], Loss: 0.0735, Val Loss: 0.4457\n",
      "Epoch [47/1000], Loss: 0.1094, Val Loss: 0.4459\n",
      "Epoch [48/1000], Loss: 0.1491, Val Loss: 0.4460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:29:10,176] Trial 73 finished with value: 0.824627721671926 and parameters: {'hidden_size': 102, 'dropout_prob': 0.7711879572382435, 'learning_rate': 0.00028684342110371023, 'weight_decay': 8.0098355298659e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 49\n",
      "Epoch [1/1000], Loss: 0.3268, Val Loss: 0.5465\n",
      "Epoch [2/1000], Loss: 0.3544, Val Loss: 0.5110\n",
      "Epoch [3/1000], Loss: 0.1012, Val Loss: 0.4905\n",
      "Epoch [4/1000], Loss: 0.0388, Val Loss: 0.4785\n",
      "Epoch [5/1000], Loss: 0.8763, Val Loss: 0.4718\n",
      "Epoch [6/1000], Loss: 0.2914, Val Loss: 0.4682\n",
      "Epoch [7/1000], Loss: 0.1450, Val Loss: 0.4657\n",
      "Epoch [8/1000], Loss: 0.0407, Val Loss: 0.4640\n",
      "Epoch [9/1000], Loss: 0.4710, Val Loss: 0.4623\n",
      "Epoch [10/1000], Loss: 2.4419, Val Loss: 0.4617\n",
      "Epoch [11/1000], Loss: 0.5952, Val Loss: 0.4604\n",
      "Epoch [12/1000], Loss: 0.0972, Val Loss: 0.4599\n",
      "Epoch [13/1000], Loss: 0.0892, Val Loss: 0.4577\n",
      "Epoch [14/1000], Loss: 1.2801, Val Loss: 0.4566\n",
      "Epoch [15/1000], Loss: 1.5984, Val Loss: 0.4564\n",
      "Epoch [16/1000], Loss: 0.5781, Val Loss: 0.4563\n",
      "Epoch [17/1000], Loss: 0.3237, Val Loss: 0.4547\n",
      "Epoch [18/1000], Loss: 0.6033, Val Loss: 0.4539\n",
      "Epoch [19/1000], Loss: 0.5460, Val Loss: 0.4533\n",
      "Epoch [20/1000], Loss: 0.0577, Val Loss: 0.4538\n",
      "Epoch [21/1000], Loss: 0.4040, Val Loss: 0.4527\n",
      "Epoch [22/1000], Loss: 0.2269, Val Loss: 0.4523\n",
      "Epoch [23/1000], Loss: 0.2700, Val Loss: 0.4520\n",
      "Epoch [24/1000], Loss: 0.1467, Val Loss: 0.4514\n",
      "Epoch [25/1000], Loss: 0.0563, Val Loss: 0.4512\n",
      "Epoch [26/1000], Loss: 0.4192, Val Loss: 0.4509\n",
      "Epoch [27/1000], Loss: 0.0218, Val Loss: 0.4513\n",
      "Epoch [28/1000], Loss: 0.0040, Val Loss: 0.4509\n",
      "Epoch [29/1000], Loss: 0.7243, Val Loss: 0.4511\n",
      "Epoch [30/1000], Loss: 0.0463, Val Loss: 0.4504\n",
      "Epoch [31/1000], Loss: 0.0291, Val Loss: 0.4500\n",
      "Epoch [32/1000], Loss: 0.2411, Val Loss: 0.4499\n",
      "Epoch [33/1000], Loss: 0.5088, Val Loss: 0.4495\n",
      "Epoch [34/1000], Loss: 0.0048, Val Loss: 0.4500\n",
      "Epoch [35/1000], Loss: 0.4695, Val Loss: 0.4489\n",
      "Epoch [36/1000], Loss: 1.0890, Val Loss: 0.4489\n",
      "Epoch [37/1000], Loss: 1.2885, Val Loss: 0.4493\n",
      "Epoch [38/1000], Loss: 0.0461, Val Loss: 0.4490\n",
      "Epoch [39/1000], Loss: 0.0099, Val Loss: 0.4486\n",
      "Epoch [40/1000], Loss: 0.1984, Val Loss: 0.4487\n",
      "Epoch [41/1000], Loss: 0.6973, Val Loss: 0.4485\n",
      "Epoch [42/1000], Loss: 1.5095, Val Loss: 0.4489\n",
      "Epoch [43/1000], Loss: 0.2386, Val Loss: 0.4487\n",
      "Epoch [44/1000], Loss: 0.3951, Val Loss: 0.4487\n",
      "Epoch [45/1000], Loss: 0.2692, Val Loss: 0.4486\n",
      "Epoch [46/1000], Loss: 0.6505, Val Loss: 0.4487\n",
      "Epoch [47/1000], Loss: 0.2545, Val Loss: 0.4484\n",
      "Epoch [48/1000], Loss: 1.6024, Val Loss: 0.4477\n",
      "Epoch [49/1000], Loss: 0.1302, Val Loss: 0.4483\n",
      "Epoch [50/1000], Loss: 0.0357, Val Loss: 0.4483\n",
      "Epoch [51/1000], Loss: 0.2487, Val Loss: 0.4483\n",
      "Epoch [52/1000], Loss: 0.0587, Val Loss: 0.4479\n",
      "Epoch [53/1000], Loss: 0.0014, Val Loss: 0.4478\n",
      "Epoch [54/1000], Loss: 0.5971, Val Loss: 0.4475\n",
      "Epoch [55/1000], Loss: 0.2852, Val Loss: 0.4476\n",
      "Epoch [56/1000], Loss: 0.0240, Val Loss: 0.4473\n",
      "Epoch [57/1000], Loss: 0.1983, Val Loss: 0.4474\n",
      "Epoch [58/1000], Loss: 0.0538, Val Loss: 0.4476\n",
      "Epoch [59/1000], Loss: 0.4186, Val Loss: 0.4479\n",
      "Epoch [60/1000], Loss: 0.5385, Val Loss: 0.4473\n",
      "Epoch [61/1000], Loss: 0.0207, Val Loss: 0.4479\n",
      "Epoch [62/1000], Loss: 0.9051, Val Loss: 0.4481\n",
      "Epoch [63/1000], Loss: 0.3481, Val Loss: 0.4479\n",
      "Epoch [64/1000], Loss: 1.1857, Val Loss: 0.4474\n",
      "Epoch [65/1000], Loss: 0.1961, Val Loss: 0.4468\n",
      "Epoch [66/1000], Loss: 0.6919, Val Loss: 0.4468\n",
      "Epoch [67/1000], Loss: 0.3163, Val Loss: 0.4474\n",
      "Epoch [68/1000], Loss: 0.0095, Val Loss: 0.4469\n",
      "Epoch [69/1000], Loss: 0.0221, Val Loss: 0.4466\n",
      "Epoch [70/1000], Loss: 0.0953, Val Loss: 0.4477\n",
      "Epoch [71/1000], Loss: 0.0066, Val Loss: 0.4473\n",
      "Epoch [72/1000], Loss: 0.2756, Val Loss: 0.4474\n",
      "Epoch [73/1000], Loss: 0.0800, Val Loss: 0.4473\n",
      "Epoch [74/1000], Loss: 0.9577, Val Loss: 0.4478\n",
      "Epoch [75/1000], Loss: 0.0717, Val Loss: 0.4475\n",
      "Epoch [76/1000], Loss: 0.7286, Val Loss: 0.4478\n",
      "Epoch [77/1000], Loss: 0.3296, Val Loss: 0.4480\n",
      "Epoch [78/1000], Loss: 0.4165, Val Loss: 0.4479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:29:32,553] Trial 74 finished with value: 0.8236718546878264 and parameters: {'hidden_size': 101, 'dropout_prob': 0.730047513060943, 'learning_rate': 0.0002505263214613463, 'weight_decay': 4.773786300695314e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 79\n",
      "Epoch [1/1000], Loss: 0.9514, Val Loss: 0.5349\n",
      "Epoch [2/1000], Loss: 0.0283, Val Loss: 0.5040\n",
      "Epoch [3/1000], Loss: 0.4693, Val Loss: 0.4862\n",
      "Epoch [4/1000], Loss: 0.3450, Val Loss: 0.4773\n",
      "Epoch [5/1000], Loss: 0.0675, Val Loss: 0.4746\n",
      "Epoch [6/1000], Loss: 0.1858, Val Loss: 0.4712\n",
      "Epoch [7/1000], Loss: 0.0055, Val Loss: 0.4685\n",
      "Epoch [8/1000], Loss: 0.1587, Val Loss: 0.4661\n",
      "Epoch [9/1000], Loss: 0.5208, Val Loss: 0.4638\n",
      "Epoch [10/1000], Loss: 0.0325, Val Loss: 0.4612\n",
      "Epoch [11/1000], Loss: 0.5449, Val Loss: 0.4594\n",
      "Epoch [12/1000], Loss: 0.3146, Val Loss: 0.4582\n",
      "Epoch [13/1000], Loss: 0.0783, Val Loss: 0.4577\n",
      "Epoch [14/1000], Loss: 0.0253, Val Loss: 0.4567\n",
      "Epoch [15/1000], Loss: 0.9666, Val Loss: 0.4559\n",
      "Epoch [16/1000], Loss: 0.0484, Val Loss: 0.4556\n",
      "Epoch [17/1000], Loss: 0.2381, Val Loss: 0.4558\n",
      "Epoch [18/1000], Loss: 0.0435, Val Loss: 0.4547\n",
      "Epoch [19/1000], Loss: 0.7055, Val Loss: 0.4544\n",
      "Epoch [20/1000], Loss: 0.0514, Val Loss: 0.4540\n",
      "Epoch [21/1000], Loss: 1.6617, Val Loss: 0.4536\n",
      "Epoch [22/1000], Loss: 0.9344, Val Loss: 0.4534\n",
      "Epoch [23/1000], Loss: 0.7242, Val Loss: 0.4534\n",
      "Epoch [24/1000], Loss: 0.8759, Val Loss: 0.4531\n",
      "Epoch [25/1000], Loss: 0.1579, Val Loss: 0.4524\n",
      "Epoch [26/1000], Loss: 0.3264, Val Loss: 0.4519\n",
      "Epoch [27/1000], Loss: 0.2385, Val Loss: 0.4513\n",
      "Epoch [28/1000], Loss: 0.3512, Val Loss: 0.4511\n",
      "Epoch [29/1000], Loss: 0.0003, Val Loss: 0.4508\n",
      "Epoch [30/1000], Loss: 0.3877, Val Loss: 0.4499\n",
      "Epoch [31/1000], Loss: 0.8597, Val Loss: 0.4498\n",
      "Epoch [32/1000], Loss: 0.1783, Val Loss: 0.4500\n",
      "Epoch [33/1000], Loss: 0.0005, Val Loss: 0.4497\n",
      "Epoch [34/1000], Loss: 0.1978, Val Loss: 0.4489\n",
      "Epoch [35/1000], Loss: 0.1553, Val Loss: 0.4491\n",
      "Epoch [36/1000], Loss: 0.1180, Val Loss: 0.4496\n",
      "Epoch [37/1000], Loss: 0.6501, Val Loss: 0.4500\n",
      "Epoch [38/1000], Loss: 0.7586, Val Loss: 0.4494\n",
      "Epoch [39/1000], Loss: 0.7217, Val Loss: 0.4495\n",
      "Epoch [40/1000], Loss: 0.0110, Val Loss: 0.4486\n",
      "Epoch [41/1000], Loss: 0.3721, Val Loss: 0.4489\n",
      "Epoch [42/1000], Loss: 0.1974, Val Loss: 0.4479\n",
      "Epoch [43/1000], Loss: 1.4801, Val Loss: 0.4476\n",
      "Epoch [44/1000], Loss: 0.6133, Val Loss: 0.4479\n",
      "Epoch [45/1000], Loss: 0.8104, Val Loss: 0.4476\n",
      "Epoch [46/1000], Loss: 0.2864, Val Loss: 0.4473\n",
      "Epoch [47/1000], Loss: 0.0104, Val Loss: 0.4470\n",
      "Epoch [48/1000], Loss: 0.1271, Val Loss: 0.4467\n",
      "Epoch [49/1000], Loss: 0.5421, Val Loss: 0.4469\n",
      "Epoch [50/1000], Loss: 0.0014, Val Loss: 0.4468\n",
      "Epoch [51/1000], Loss: 1.0204, Val Loss: 0.4466\n",
      "Epoch [52/1000], Loss: 0.0548, Val Loss: 0.4471\n",
      "Epoch [53/1000], Loss: 0.3790, Val Loss: 0.4474\n",
      "Epoch [54/1000], Loss: 0.0083, Val Loss: 0.4469\n",
      "Epoch [55/1000], Loss: 0.3847, Val Loss: 0.4467\n",
      "Epoch [56/1000], Loss: 0.0459, Val Loss: 0.4471\n",
      "Epoch [57/1000], Loss: 0.0076, Val Loss: 0.4476\n",
      "Epoch [58/1000], Loss: 0.7000, Val Loss: 0.4473\n",
      "Epoch [59/1000], Loss: 0.1635, Val Loss: 0.4470\n",
      "Epoch [60/1000], Loss: 0.2910, Val Loss: 0.4464\n",
      "Epoch [61/1000], Loss: 0.0375, Val Loss: 0.4467\n",
      "Epoch [62/1000], Loss: 1.1840, Val Loss: 0.4466\n",
      "Epoch [63/1000], Loss: 0.2357, Val Loss: 0.4467\n",
      "Epoch [64/1000], Loss: 0.2355, Val Loss: 0.4476\n",
      "Epoch [65/1000], Loss: 0.9367, Val Loss: 0.4473\n",
      "Epoch [66/1000], Loss: 0.0040, Val Loss: 0.4472\n",
      "Epoch [67/1000], Loss: 0.4424, Val Loss: 0.4472\n",
      "Epoch [68/1000], Loss: 0.0213, Val Loss: 0.4463\n",
      "Epoch [69/1000], Loss: 0.3904, Val Loss: 0.4467\n",
      "Epoch [70/1000], Loss: 0.0754, Val Loss: 0.4465\n",
      "Epoch [71/1000], Loss: 0.7396, Val Loss: 0.4466\n",
      "Epoch [72/1000], Loss: 0.7409, Val Loss: 0.4468\n",
      "Epoch [73/1000], Loss: 0.2992, Val Loss: 0.4471\n",
      "Epoch [74/1000], Loss: 0.0835, Val Loss: 0.4466\n",
      "Epoch [75/1000], Loss: 0.2230, Val Loss: 0.4465\n",
      "Epoch [76/1000], Loss: 1.3649, Val Loss: 0.4469\n",
      "Epoch [77/1000], Loss: 0.4076, Val Loss: 0.4466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:29:55,380] Trial 75 finished with value: 0.8227083407678543 and parameters: {'hidden_size': 112, 'dropout_prob': 0.798005688068899, 'learning_rate': 0.00033711466909005296, 'weight_decay': 4.7377538265621356e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 78\n",
      "Epoch [1/1000], Loss: 0.5810, Val Loss: 0.5815\n",
      "Epoch [2/1000], Loss: 2.7266, Val Loss: 0.5224\n",
      "Epoch [3/1000], Loss: 1.0213, Val Loss: 0.5048\n",
      "Epoch [4/1000], Loss: 0.9401, Val Loss: 0.4916\n",
      "Epoch [5/1000], Loss: 0.3761, Val Loss: 0.4818\n",
      "Epoch [6/1000], Loss: 0.8180, Val Loss: 0.4754\n",
      "Epoch [7/1000], Loss: 0.0560, Val Loss: 0.4702\n",
      "Epoch [8/1000], Loss: 0.7451, Val Loss: 0.4663\n",
      "Epoch [9/1000], Loss: 0.0544, Val Loss: 0.4642\n",
      "Epoch [10/1000], Loss: 0.5016, Val Loss: 0.4621\n",
      "Epoch [11/1000], Loss: 0.1409, Val Loss: 0.4606\n",
      "Epoch [12/1000], Loss: 0.0971, Val Loss: 0.4594\n",
      "Epoch [13/1000], Loss: 0.6856, Val Loss: 0.4579\n",
      "Epoch [14/1000], Loss: 0.3922, Val Loss: 0.4573\n",
      "Epoch [15/1000], Loss: 0.8179, Val Loss: 0.4570\n",
      "Epoch [16/1000], Loss: 0.0059, Val Loss: 0.4559\n",
      "Epoch [17/1000], Loss: 0.0107, Val Loss: 0.4553\n",
      "Epoch [18/1000], Loss: 0.0314, Val Loss: 0.4549\n",
      "Epoch [19/1000], Loss: 0.8204, Val Loss: 0.4544\n",
      "Epoch [20/1000], Loss: 0.0744, Val Loss: 0.4542\n",
      "Epoch [21/1000], Loss: 0.3444, Val Loss: 0.4535\n",
      "Epoch [22/1000], Loss: 0.2037, Val Loss: 0.4527\n",
      "Epoch [23/1000], Loss: 0.5831, Val Loss: 0.4523\n",
      "Epoch [24/1000], Loss: 0.5018, Val Loss: 0.4524\n",
      "Epoch [25/1000], Loss: 0.0300, Val Loss: 0.4522\n",
      "Epoch [26/1000], Loss: 0.0694, Val Loss: 0.4517\n",
      "Epoch [27/1000], Loss: 0.5829, Val Loss: 0.4513\n",
      "Epoch [28/1000], Loss: 0.1192, Val Loss: 0.4515\n",
      "Epoch [29/1000], Loss: 0.0393, Val Loss: 0.4509\n",
      "Epoch [30/1000], Loss: 3.0689, Val Loss: 0.4505\n",
      "Epoch [31/1000], Loss: 0.2715, Val Loss: 0.4503\n",
      "Epoch [32/1000], Loss: 0.0670, Val Loss: 0.4498\n",
      "Epoch [33/1000], Loss: 1.0719, Val Loss: 0.4499\n",
      "Epoch [34/1000], Loss: 0.5084, Val Loss: 0.4499\n",
      "Epoch [35/1000], Loss: 0.3077, Val Loss: 0.4501\n",
      "Epoch [36/1000], Loss: 0.0006, Val Loss: 0.4502\n",
      "Epoch [37/1000], Loss: 0.2159, Val Loss: 0.4500\n",
      "Epoch [38/1000], Loss: 0.2093, Val Loss: 0.4496\n",
      "Epoch [39/1000], Loss: 0.3052, Val Loss: 0.4495\n",
      "Epoch [40/1000], Loss: 0.6829, Val Loss: 0.4496\n",
      "Epoch [41/1000], Loss: 0.0725, Val Loss: 0.4497\n",
      "Epoch [42/1000], Loss: 0.1512, Val Loss: 0.4494\n",
      "Epoch [43/1000], Loss: 0.4411, Val Loss: 0.4490\n",
      "Epoch [44/1000], Loss: 0.5173, Val Loss: 0.4491\n",
      "Epoch [45/1000], Loss: 0.7133, Val Loss: 0.4490\n",
      "Epoch [46/1000], Loss: 0.2523, Val Loss: 0.4487\n",
      "Epoch [47/1000], Loss: 1.7291, Val Loss: 0.4488\n",
      "Epoch [48/1000], Loss: 0.0226, Val Loss: 0.4489\n",
      "Epoch [49/1000], Loss: 0.1558, Val Loss: 0.4488\n",
      "Epoch [50/1000], Loss: 0.3284, Val Loss: 0.4488\n",
      "Epoch [51/1000], Loss: 0.5564, Val Loss: 0.4492\n",
      "Epoch [52/1000], Loss: 0.2619, Val Loss: 0.4488\n",
      "Epoch [53/1000], Loss: 0.0226, Val Loss: 0.4486\n",
      "Epoch [54/1000], Loss: 0.0570, Val Loss: 0.4486\n",
      "Epoch [55/1000], Loss: 0.9230, Val Loss: 0.4488\n",
      "Epoch [56/1000], Loss: 0.1137, Val Loss: 0.4485\n",
      "Epoch [57/1000], Loss: 2.3832, Val Loss: 0.4483\n",
      "Epoch [58/1000], Loss: 0.5131, Val Loss: 0.4481\n",
      "Epoch [59/1000], Loss: 0.3725, Val Loss: 0.4481\n",
      "Epoch [60/1000], Loss: 1.0996, Val Loss: 0.4483\n",
      "Epoch [61/1000], Loss: 2.0631, Val Loss: 0.4483\n",
      "Epoch [62/1000], Loss: 0.6901, Val Loss: 0.4484\n",
      "Epoch [63/1000], Loss: 0.3645, Val Loss: 0.4483\n",
      "Epoch [64/1000], Loss: 0.2359, Val Loss: 0.4486\n",
      "Epoch [65/1000], Loss: 0.0213, Val Loss: 0.4482\n",
      "Epoch [66/1000], Loss: 0.2872, Val Loss: 0.4482\n",
      "Epoch [67/1000], Loss: 0.9514, Val Loss: 0.4477\n",
      "Epoch [68/1000], Loss: 0.0087, Val Loss: 0.4478\n",
      "Epoch [69/1000], Loss: 0.5978, Val Loss: 0.4480\n",
      "Epoch [70/1000], Loss: 0.1613, Val Loss: 0.4486\n",
      "Epoch [71/1000], Loss: 0.1833, Val Loss: 0.4481\n",
      "Epoch [72/1000], Loss: 0.3233, Val Loss: 0.4478\n",
      "Epoch [73/1000], Loss: 0.1843, Val Loss: 0.4477\n",
      "Epoch [74/1000], Loss: 0.0413, Val Loss: 0.4474\n",
      "Epoch [75/1000], Loss: 0.0107, Val Loss: 0.4477\n",
      "Epoch [76/1000], Loss: 0.0651, Val Loss: 0.4476\n",
      "Epoch [77/1000], Loss: 1.2996, Val Loss: 0.4476\n",
      "Epoch [78/1000], Loss: 0.1059, Val Loss: 0.4477\n",
      "Epoch [79/1000], Loss: 0.4329, Val Loss: 0.4472\n",
      "Epoch [80/1000], Loss: 1.2646, Val Loss: 0.4470\n",
      "Epoch [81/1000], Loss: 0.1704, Val Loss: 0.4473\n",
      "Epoch [82/1000], Loss: 0.0210, Val Loss: 0.4475\n",
      "Epoch [83/1000], Loss: 0.5630, Val Loss: 0.4478\n",
      "Epoch [84/1000], Loss: 0.0300, Val Loss: 0.4478\n",
      "Epoch [85/1000], Loss: 6.5171, Val Loss: 0.4478\n",
      "Epoch [86/1000], Loss: 0.3209, Val Loss: 0.4472\n",
      "Epoch [87/1000], Loss: 0.5547, Val Loss: 0.4473\n",
      "Epoch [88/1000], Loss: 0.9730, Val Loss: 0.4472\n",
      "Epoch [89/1000], Loss: 0.1172, Val Loss: 0.4472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:30:22,421] Trial 76 finished with value: 0.8224534429054278 and parameters: {'hidden_size': 100, 'dropout_prob': 0.7758762556857498, 'learning_rate': 0.00023931966557029722, 'weight_decay': 4.428285969749004e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 90\n",
      "Epoch [1/1000], Loss: 0.7714, Val Loss: 0.6986\n",
      "Epoch [2/1000], Loss: 0.7093, Val Loss: 0.6785\n",
      "Epoch [3/1000], Loss: 0.6013, Val Loss: 0.6500\n",
      "Epoch [4/1000], Loss: 0.5303, Val Loss: 0.6113\n",
      "Epoch [5/1000], Loss: 0.4508, Val Loss: 0.5714\n",
      "Epoch [6/1000], Loss: 0.9208, Val Loss: 0.5452\n",
      "Epoch [7/1000], Loss: 0.9088, Val Loss: 0.5314\n",
      "Epoch [8/1000], Loss: 0.1380, Val Loss: 0.5223\n",
      "Epoch [9/1000], Loss: 0.9536, Val Loss: 0.5152\n",
      "Epoch [10/1000], Loss: 0.2625, Val Loss: 0.5102\n",
      "Epoch [11/1000], Loss: 1.3173, Val Loss: 0.5056\n",
      "Epoch [12/1000], Loss: 0.1238, Val Loss: 0.5015\n",
      "Epoch [13/1000], Loss: 0.6363, Val Loss: 0.4976\n",
      "Epoch [14/1000], Loss: 0.2809, Val Loss: 0.4942\n",
      "Epoch [15/1000], Loss: 0.6290, Val Loss: 0.4908\n",
      "Epoch [16/1000], Loss: 0.4362, Val Loss: 0.4877\n",
      "Epoch [17/1000], Loss: 0.2181, Val Loss: 0.4850\n",
      "Epoch [18/1000], Loss: 0.7848, Val Loss: 0.4826\n",
      "Epoch [19/1000], Loss: 0.2390, Val Loss: 0.4804\n",
      "Epoch [20/1000], Loss: 0.0635, Val Loss: 0.4785\n",
      "Epoch [21/1000], Loss: 0.4211, Val Loss: 0.4767\n",
      "Epoch [22/1000], Loss: 0.3049, Val Loss: 0.4751\n",
      "Epoch [23/1000], Loss: 0.7023, Val Loss: 0.4737\n",
      "Epoch [24/1000], Loss: 0.2335, Val Loss: 0.4725\n",
      "Epoch [25/1000], Loss: 0.6388, Val Loss: 0.4713\n",
      "Epoch [26/1000], Loss: 1.4457, Val Loss: 0.4701\n",
      "Epoch [27/1000], Loss: 0.5396, Val Loss: 0.4692\n",
      "Epoch [28/1000], Loss: 0.1811, Val Loss: 0.4683\n",
      "Epoch [29/1000], Loss: 0.6422, Val Loss: 0.4677\n",
      "Epoch [30/1000], Loss: 0.0177, Val Loss: 0.4671\n",
      "Epoch [31/1000], Loss: 0.0666, Val Loss: 0.4664\n",
      "Epoch [32/1000], Loss: 0.0640, Val Loss: 0.4657\n",
      "Epoch [33/1000], Loss: 0.9529, Val Loss: 0.4650\n",
      "Epoch [34/1000], Loss: 0.6858, Val Loss: 0.4646\n",
      "Epoch [35/1000], Loss: 0.3643, Val Loss: 0.4640\n",
      "Epoch [36/1000], Loss: 1.6485, Val Loss: 0.4636\n",
      "Epoch [37/1000], Loss: 0.6409, Val Loss: 0.4632\n",
      "Epoch [38/1000], Loss: 0.2933, Val Loss: 0.4628\n",
      "Epoch [39/1000], Loss: 0.0477, Val Loss: 0.4624\n",
      "Epoch [40/1000], Loss: 0.3305, Val Loss: 0.4621\n",
      "Epoch [41/1000], Loss: 0.0014, Val Loss: 0.4617\n",
      "Epoch [42/1000], Loss: 0.0390, Val Loss: 0.4614\n",
      "Epoch [43/1000], Loss: 0.0410, Val Loss: 0.4610\n",
      "Epoch [44/1000], Loss: 0.8656, Val Loss: 0.4608\n",
      "Epoch [45/1000], Loss: 0.0859, Val Loss: 0.4605\n",
      "Epoch [46/1000], Loss: 0.0903, Val Loss: 0.4602\n",
      "Epoch [47/1000], Loss: 0.6239, Val Loss: 0.4601\n",
      "Epoch [48/1000], Loss: 0.5470, Val Loss: 0.4597\n",
      "Epoch [49/1000], Loss: 0.4031, Val Loss: 0.4594\n",
      "Epoch [50/1000], Loss: 0.7452, Val Loss: 0.4591\n",
      "Epoch [51/1000], Loss: 0.6048, Val Loss: 0.4587\n",
      "Epoch [52/1000], Loss: 0.1529, Val Loss: 0.4586\n",
      "Epoch [53/1000], Loss: 0.0547, Val Loss: 0.4584\n",
      "Epoch [54/1000], Loss: 0.0207, Val Loss: 0.4582\n",
      "Epoch [55/1000], Loss: 0.3132, Val Loss: 0.4579\n",
      "Epoch [56/1000], Loss: 0.9805, Val Loss: 0.4577\n",
      "Epoch [57/1000], Loss: 1.3551, Val Loss: 0.4577\n",
      "Epoch [58/1000], Loss: 0.1012, Val Loss: 0.4574\n",
      "Epoch [59/1000], Loss: 0.1827, Val Loss: 0.4572\n",
      "Epoch [60/1000], Loss: 0.3774, Val Loss: 0.4570\n",
      "Epoch [61/1000], Loss: 0.2156, Val Loss: 0.4568\n",
      "Epoch [62/1000], Loss: 0.0483, Val Loss: 0.4566\n",
      "Epoch [63/1000], Loss: 0.2309, Val Loss: 0.4564\n",
      "Epoch [64/1000], Loss: 0.5932, Val Loss: 0.4562\n",
      "Epoch [65/1000], Loss: 0.2421, Val Loss: 0.4561\n",
      "Epoch [66/1000], Loss: 0.1324, Val Loss: 0.4558\n",
      "Epoch [67/1000], Loss: 0.0109, Val Loss: 0.4556\n",
      "Epoch [68/1000], Loss: 0.0024, Val Loss: 0.4555\n",
      "Epoch [69/1000], Loss: 0.7868, Val Loss: 0.4554\n",
      "Epoch [70/1000], Loss: 0.0655, Val Loss: 0.4553\n",
      "Epoch [71/1000], Loss: 0.6847, Val Loss: 0.4553\n",
      "Epoch [72/1000], Loss: 0.7315, Val Loss: 0.4554\n",
      "Epoch [73/1000], Loss: 0.5049, Val Loss: 0.4552\n",
      "Epoch [74/1000], Loss: 0.8928, Val Loss: 0.4549\n",
      "Epoch [75/1000], Loss: 0.5687, Val Loss: 0.4547\n",
      "Epoch [76/1000], Loss: 4.2303, Val Loss: 0.4547\n",
      "Epoch [77/1000], Loss: 0.1196, Val Loss: 0.4544\n",
      "Epoch [78/1000], Loss: 0.8344, Val Loss: 0.4544\n",
      "Epoch [79/1000], Loss: 0.7733, Val Loss: 0.4544\n",
      "Epoch [80/1000], Loss: 0.0658, Val Loss: 0.4543\n",
      "Epoch [81/1000], Loss: 0.3652, Val Loss: 0.4542\n",
      "Epoch [82/1000], Loss: 1.5826, Val Loss: 0.4542\n",
      "Epoch [83/1000], Loss: 0.1546, Val Loss: 0.4541\n",
      "Epoch [84/1000], Loss: 0.7309, Val Loss: 0.4540\n",
      "Epoch [85/1000], Loss: 0.4994, Val Loss: 0.4539\n",
      "Epoch [86/1000], Loss: 0.1362, Val Loss: 0.4538\n",
      "Epoch [87/1000], Loss: 0.0771, Val Loss: 0.4538\n",
      "Epoch [88/1000], Loss: 0.0222, Val Loss: 0.4536\n",
      "Epoch [89/1000], Loss: 0.0310, Val Loss: 0.4536\n",
      "Epoch [90/1000], Loss: 0.2235, Val Loss: 0.4536\n",
      "Epoch [91/1000], Loss: 0.2141, Val Loss: 0.4535\n",
      "Epoch [92/1000], Loss: 0.6052, Val Loss: 0.4534\n",
      "Epoch [93/1000], Loss: 0.7867, Val Loss: 0.4534\n",
      "Epoch [94/1000], Loss: 0.3276, Val Loss: 0.4533\n",
      "Epoch [95/1000], Loss: 2.8222, Val Loss: 0.4532\n",
      "Epoch [96/1000], Loss: 0.0348, Val Loss: 0.4531\n",
      "Epoch [97/1000], Loss: 0.1144, Val Loss: 0.4530\n",
      "Epoch [98/1000], Loss: 0.0557, Val Loss: 0.4529\n",
      "Epoch [99/1000], Loss: 0.9187, Val Loss: 0.4528\n",
      "Epoch [100/1000], Loss: 0.1575, Val Loss: 0.4528\n",
      "Epoch [101/1000], Loss: 0.9266, Val Loss: 0.4527\n",
      "Epoch [102/1000], Loss: 0.2598, Val Loss: 0.4525\n",
      "Epoch [103/1000], Loss: 0.0459, Val Loss: 0.4524\n",
      "Epoch [104/1000], Loss: 0.1515, Val Loss: 0.4523\n",
      "Epoch [105/1000], Loss: 0.5104, Val Loss: 0.4523\n",
      "Epoch [106/1000], Loss: 0.8449, Val Loss: 0.4522\n",
      "Epoch [107/1000], Loss: 0.0113, Val Loss: 0.4522\n",
      "Epoch [108/1000], Loss: 0.6601, Val Loss: 0.4522\n",
      "Epoch [109/1000], Loss: 1.1071, Val Loss: 0.4520\n",
      "Epoch [110/1000], Loss: 0.0471, Val Loss: 0.4521\n",
      "Epoch [111/1000], Loss: 0.5673, Val Loss: 0.4519\n",
      "Epoch [112/1000], Loss: 0.1972, Val Loss: 0.4520\n",
      "Epoch [113/1000], Loss: 2.3679, Val Loss: 0.4519\n",
      "Epoch [114/1000], Loss: 0.6084, Val Loss: 0.4518\n",
      "Epoch [115/1000], Loss: 0.1752, Val Loss: 0.4517\n",
      "Epoch [116/1000], Loss: 0.0871, Val Loss: 0.4517\n",
      "Epoch [117/1000], Loss: 0.3183, Val Loss: 0.4515\n",
      "Epoch [118/1000], Loss: 0.2537, Val Loss: 0.4515\n",
      "Epoch [119/1000], Loss: 0.7451, Val Loss: 0.4515\n",
      "Epoch [120/1000], Loss: 0.6053, Val Loss: 0.4514\n",
      "Epoch [121/1000], Loss: 0.2978, Val Loss: 0.4513\n",
      "Epoch [122/1000], Loss: 0.1942, Val Loss: 0.4512\n",
      "Epoch [123/1000], Loss: 0.0561, Val Loss: 0.4511\n",
      "Epoch [124/1000], Loss: 0.0167, Val Loss: 0.4511\n",
      "Epoch [125/1000], Loss: 0.0538, Val Loss: 0.4509\n",
      "Epoch [126/1000], Loss: 0.2322, Val Loss: 0.4508\n",
      "Epoch [127/1000], Loss: 0.0710, Val Loss: 0.4508\n",
      "Epoch [128/1000], Loss: 0.0141, Val Loss: 0.4508\n",
      "Epoch [129/1000], Loss: 0.5951, Val Loss: 0.4507\n",
      "Epoch [130/1000], Loss: 0.4930, Val Loss: 0.4506\n",
      "Epoch [131/1000], Loss: 0.4154, Val Loss: 0.4507\n",
      "Epoch [132/1000], Loss: 0.3356, Val Loss: 0.4506\n",
      "Epoch [133/1000], Loss: 0.1269, Val Loss: 0.4507\n",
      "Epoch [134/1000], Loss: 0.0156, Val Loss: 0.4506\n",
      "Epoch [135/1000], Loss: 0.1235, Val Loss: 0.4506\n",
      "Epoch [136/1000], Loss: 0.0691, Val Loss: 0.4506\n",
      "Epoch [137/1000], Loss: 0.9910, Val Loss: 0.4506\n",
      "Epoch [138/1000], Loss: 0.7610, Val Loss: 0.4505\n",
      "Epoch [139/1000], Loss: 0.1125, Val Loss: 0.4505\n",
      "Epoch [140/1000], Loss: 0.4548, Val Loss: 0.4505\n",
      "Epoch [141/1000], Loss: 0.1420, Val Loss: 0.4504\n",
      "Epoch [142/1000], Loss: 0.8691, Val Loss: 0.4505\n",
      "Epoch [143/1000], Loss: 0.6285, Val Loss: 0.4504\n",
      "Epoch [144/1000], Loss: 0.0295, Val Loss: 0.4503\n",
      "Epoch [145/1000], Loss: 0.0484, Val Loss: 0.4503\n",
      "Epoch [146/1000], Loss: 0.3050, Val Loss: 0.4503\n",
      "Epoch [147/1000], Loss: 0.9092, Val Loss: 0.4502\n",
      "Epoch [148/1000], Loss: 0.7794, Val Loss: 0.4502\n",
      "Epoch [149/1000], Loss: 1.3781, Val Loss: 0.4502\n",
      "Epoch [150/1000], Loss: 0.7744, Val Loss: 0.4502\n",
      "Epoch [151/1000], Loss: 0.9579, Val Loss: 0.4500\n",
      "Epoch [152/1000], Loss: 0.0074, Val Loss: 0.4500\n",
      "Epoch [153/1000], Loss: 0.0625, Val Loss: 0.4500\n",
      "Epoch [154/1000], Loss: 0.0299, Val Loss: 0.4498\n",
      "Epoch [155/1000], Loss: 0.2714, Val Loss: 0.4500\n",
      "Epoch [156/1000], Loss: 0.2592, Val Loss: 0.4499\n",
      "Epoch [157/1000], Loss: 1.4274, Val Loss: 0.4498\n",
      "Epoch [158/1000], Loss: 0.2267, Val Loss: 0.4497\n",
      "Epoch [159/1000], Loss: 0.0082, Val Loss: 0.4496\n",
      "Epoch [160/1000], Loss: 0.8046, Val Loss: 0.4496\n",
      "Epoch [161/1000], Loss: 0.9064, Val Loss: 0.4496\n",
      "Epoch [162/1000], Loss: 0.4186, Val Loss: 0.4497\n",
      "Epoch [163/1000], Loss: 0.9158, Val Loss: 0.4496\n",
      "Epoch [164/1000], Loss: 0.4065, Val Loss: 0.4498\n",
      "Epoch [165/1000], Loss: 0.7189, Val Loss: 0.4497\n",
      "Epoch [166/1000], Loss: 0.0136, Val Loss: 0.4496\n",
      "Epoch [167/1000], Loss: 0.5579, Val Loss: 0.4497\n",
      "Epoch [168/1000], Loss: 0.0537, Val Loss: 0.4496\n",
      "Epoch [169/1000], Loss: 0.9070, Val Loss: 0.4496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 0.7154, Val Loss: 0.4496\n",
      "Epoch [171/1000], Loss: 2.3714, Val Loss: 0.4496\n",
      "Epoch [172/1000], Loss: 0.6677, Val Loss: 0.4496\n",
      "Epoch [173/1000], Loss: 0.2065, Val Loss: 0.4494\n",
      "Epoch [174/1000], Loss: 0.1501, Val Loss: 0.4494\n",
      "Epoch [175/1000], Loss: 0.2830, Val Loss: 0.4495\n",
      "Epoch [176/1000], Loss: 0.1445, Val Loss: 0.4494\n",
      "Epoch [177/1000], Loss: 0.0083, Val Loss: 0.4494\n",
      "Epoch [178/1000], Loss: 0.0052, Val Loss: 0.4494\n",
      "Epoch [179/1000], Loss: 0.1623, Val Loss: 0.4494\n",
      "Epoch [180/1000], Loss: 0.7536, Val Loss: 0.4495\n",
      "Epoch [181/1000], Loss: 0.7194, Val Loss: 0.4494\n",
      "Epoch [182/1000], Loss: 0.1390, Val Loss: 0.4494\n",
      "Epoch [183/1000], Loss: 0.0958, Val Loss: 0.4494\n",
      "Epoch [184/1000], Loss: 0.0282, Val Loss: 0.4495\n",
      "Epoch [185/1000], Loss: 0.3753, Val Loss: 0.4495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:31:17,442] Trial 77 finished with value: 0.8217422778692578 and parameters: {'hidden_size': 111, 'dropout_prob': 0.7284083131618002, 'learning_rate': 5.509975074397314e-05, 'weight_decay': 8.057660167162636e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 186\n",
      "Epoch [1/1000], Loss: 0.2737, Val Loss: 0.4707\n",
      "Epoch [2/1000], Loss: 0.6602, Val Loss: 0.4586\n",
      "Epoch [3/1000], Loss: 0.5699, Val Loss: 0.4540\n",
      "Epoch [4/1000], Loss: 0.0082, Val Loss: 0.4516\n",
      "Epoch [5/1000], Loss: 0.8030, Val Loss: 0.4514\n",
      "Epoch [6/1000], Loss: 0.0804, Val Loss: 0.4484\n",
      "Epoch [7/1000], Loss: 0.9564, Val Loss: 0.4491\n",
      "Epoch [8/1000], Loss: 0.0889, Val Loss: 0.4539\n",
      "Epoch [9/1000], Loss: 3.6559, Val Loss: 0.4493\n",
      "Epoch [10/1000], Loss: 0.4721, Val Loss: 0.4476\n",
      "Epoch [11/1000], Loss: 1.0715, Val Loss: 0.4478\n",
      "Epoch [12/1000], Loss: 0.3410, Val Loss: 0.4469\n",
      "Epoch [13/1000], Loss: 0.6478, Val Loss: 0.4458\n",
      "Epoch [14/1000], Loss: 0.3161, Val Loss: 0.4455\n",
      "Epoch [15/1000], Loss: 0.0164, Val Loss: 0.4466\n",
      "Epoch [16/1000], Loss: 0.4391, Val Loss: 0.4466\n",
      "Epoch [17/1000], Loss: 0.1304, Val Loss: 0.4454\n",
      "Epoch [18/1000], Loss: 0.4639, Val Loss: 0.4458\n",
      "Epoch [19/1000], Loss: 1.5669, Val Loss: 0.4441\n",
      "Epoch [20/1000], Loss: 0.1614, Val Loss: 0.4494\n",
      "Epoch [21/1000], Loss: 0.5559, Val Loss: 0.4448\n",
      "Epoch [22/1000], Loss: 0.1074, Val Loss: 0.4432\n",
      "Epoch [23/1000], Loss: 0.1328, Val Loss: 0.4440\n",
      "Epoch [24/1000], Loss: 0.0632, Val Loss: 0.4460\n",
      "Epoch [25/1000], Loss: 0.4723, Val Loss: 0.4459\n",
      "Epoch [26/1000], Loss: 0.1038, Val Loss: 0.4459\n",
      "Epoch [27/1000], Loss: 0.0275, Val Loss: 0.4461\n",
      "Epoch [28/1000], Loss: 0.0774, Val Loss: 0.4471\n",
      "Epoch [29/1000], Loss: 0.3391, Val Loss: 0.4451\n",
      "Epoch [30/1000], Loss: 0.1554, Val Loss: 0.4475\n",
      "Epoch [31/1000], Loss: 1.1166, Val Loss: 0.4464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:31:27,268] Trial 78 finished with value: 0.8212069923581621 and parameters: {'hidden_size': 106, 'dropout_prob': 0.3313977419900027, 'learning_rate': 0.0007399589135789085, 'weight_decay': 7.461174226072842e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 32\n",
      "Epoch [1/1000], Loss: 0.2147, Val Loss: 0.5034\n",
      "Epoch [2/1000], Loss: 1.3557, Val Loss: 0.4728\n",
      "Epoch [3/1000], Loss: 0.6693, Val Loss: 0.4635\n",
      "Epoch [4/1000], Loss: 0.3829, Val Loss: 0.4580\n",
      "Epoch [5/1000], Loss: 0.7202, Val Loss: 0.4551\n",
      "Epoch [6/1000], Loss: 0.0130, Val Loss: 0.4540\n",
      "Epoch [7/1000], Loss: 0.3635, Val Loss: 0.4539\n",
      "Epoch [8/1000], Loss: 1.8223, Val Loss: 0.4526\n",
      "Epoch [9/1000], Loss: 0.9641, Val Loss: 0.4518\n",
      "Epoch [10/1000], Loss: 0.5397, Val Loss: 0.4503\n",
      "Epoch [11/1000], Loss: 3.6934, Val Loss: 0.4508\n",
      "Epoch [12/1000], Loss: 0.1898, Val Loss: 0.4511\n",
      "Epoch [13/1000], Loss: 0.0466, Val Loss: 0.4510\n",
      "Epoch [14/1000], Loss: 0.8122, Val Loss: 0.4483\n",
      "Epoch [15/1000], Loss: 0.1682, Val Loss: 0.4496\n",
      "Epoch [16/1000], Loss: 0.0236, Val Loss: 0.4499\n",
      "Epoch [17/1000], Loss: 0.0234, Val Loss: 0.4494\n",
      "Epoch [18/1000], Loss: 0.3290, Val Loss: 0.4498\n",
      "Epoch [19/1000], Loss: 0.2423, Val Loss: 0.4497\n",
      "Epoch [20/1000], Loss: 0.1039, Val Loss: 0.4487\n",
      "Epoch [21/1000], Loss: 0.3063, Val Loss: 0.4467\n",
      "Epoch [22/1000], Loss: 0.3157, Val Loss: 0.4488\n",
      "Epoch [23/1000], Loss: 0.4937, Val Loss: 0.4476\n",
      "Epoch [24/1000], Loss: 0.1170, Val Loss: 0.4486\n",
      "Epoch [25/1000], Loss: 0.6892, Val Loss: 0.4488\n",
      "Epoch [26/1000], Loss: 0.1295, Val Loss: 0.4496\n",
      "Epoch [27/1000], Loss: 0.2080, Val Loss: 0.4479\n",
      "Epoch [28/1000], Loss: 0.1801, Val Loss: 0.4473\n",
      "Epoch [29/1000], Loss: 0.9556, Val Loss: 0.4461\n",
      "Epoch [30/1000], Loss: 0.3288, Val Loss: 0.4472\n",
      "Epoch [31/1000], Loss: 0.2165, Val Loss: 0.4470\n",
      "Epoch [32/1000], Loss: 0.6600, Val Loss: 0.4486\n",
      "Epoch [33/1000], Loss: 0.1018, Val Loss: 0.4481\n",
      "Epoch [34/1000], Loss: 0.0006, Val Loss: 0.4484\n",
      "Epoch [35/1000], Loss: 1.1145, Val Loss: 0.4489\n",
      "Epoch [36/1000], Loss: 0.0021, Val Loss: 0.4491\n",
      "Epoch [37/1000], Loss: 0.2524, Val Loss: 0.4480\n",
      "Epoch [38/1000], Loss: 0.3986, Val Loss: 0.4475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:31:38,617] Trial 79 finished with value: 0.8219971757316844 and parameters: {'hidden_size': 117, 'dropout_prob': 0.699824501647082, 'learning_rate': 0.0005949303371498417, 'weight_decay': 4.220444371933708e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 39\n",
      "Epoch [1/1000], Loss: 0.2103, Val Loss: 0.4797\n",
      "Epoch [2/1000], Loss: 0.1414, Val Loss: 0.4628\n",
      "Epoch [3/1000], Loss: 0.6951, Val Loss: 0.4601\n",
      "Epoch [4/1000], Loss: 0.4598, Val Loss: 0.4583\n",
      "Epoch [5/1000], Loss: 0.0257, Val Loss: 0.4532\n",
      "Epoch [6/1000], Loss: 0.4948, Val Loss: 0.4543\n",
      "Epoch [7/1000], Loss: 0.1577, Val Loss: 0.4536\n",
      "Epoch [8/1000], Loss: 0.8987, Val Loss: 0.4524\n",
      "Epoch [9/1000], Loss: 0.0037, Val Loss: 0.4508\n",
      "Epoch [10/1000], Loss: 0.0025, Val Loss: 0.4488\n",
      "Epoch [11/1000], Loss: 0.0658, Val Loss: 0.4486\n",
      "Epoch [12/1000], Loss: 0.0223, Val Loss: 0.4555\n",
      "Epoch [13/1000], Loss: 0.1293, Val Loss: 0.4500\n",
      "Epoch [14/1000], Loss: 0.0056, Val Loss: 0.4517\n",
      "Epoch [15/1000], Loss: 0.0006, Val Loss: 0.4495\n",
      "Epoch [16/1000], Loss: 0.3639, Val Loss: 0.4510\n",
      "Epoch [17/1000], Loss: 0.1872, Val Loss: 0.4500\n",
      "Epoch [18/1000], Loss: 0.1038, Val Loss: 0.4508\n",
      "Epoch [19/1000], Loss: 0.0506, Val Loss: 0.4492\n",
      "Epoch [20/1000], Loss: 0.0323, Val Loss: 0.4503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:31:44,762] Trial 80 finished with value: 0.8224203061833123 and parameters: {'hidden_size': 105, 'dropout_prob': 0.7619803862974726, 'learning_rate': 0.0016196339184937976, 'weight_decay': 2.7786365523120724e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 21\n",
      "Epoch [1/1000], Loss: 0.2455, Val Loss: 0.5228\n",
      "Epoch [2/1000], Loss: 1.0493, Val Loss: 0.4957\n",
      "Epoch [3/1000], Loss: 0.7660, Val Loss: 0.4788\n",
      "Epoch [4/1000], Loss: 0.0641, Val Loss: 0.4720\n",
      "Epoch [5/1000], Loss: 0.0061, Val Loss: 0.4678\n",
      "Epoch [6/1000], Loss: 0.3895, Val Loss: 0.4658\n",
      "Epoch [7/1000], Loss: 0.7007, Val Loss: 0.4638\n",
      "Epoch [8/1000], Loss: 0.1017, Val Loss: 0.4613\n",
      "Epoch [9/1000], Loss: 0.5087, Val Loss: 0.4600\n",
      "Epoch [10/1000], Loss: 0.0770, Val Loss: 0.4591\n",
      "Epoch [11/1000], Loss: 0.4564, Val Loss: 0.4573\n",
      "Epoch [12/1000], Loss: 0.1312, Val Loss: 0.4566\n",
      "Epoch [13/1000], Loss: 0.5662, Val Loss: 0.4544\n",
      "Epoch [14/1000], Loss: 0.6549, Val Loss: 0.4546\n",
      "Epoch [15/1000], Loss: 0.3767, Val Loss: 0.4547\n",
      "Epoch [16/1000], Loss: 0.0525, Val Loss: 0.4543\n",
      "Epoch [17/1000], Loss: 1.0260, Val Loss: 0.4538\n",
      "Epoch [18/1000], Loss: 0.0128, Val Loss: 0.4525\n",
      "Epoch [19/1000], Loss: 1.1761, Val Loss: 0.4512\n",
      "Epoch [20/1000], Loss: 0.0888, Val Loss: 0.4509\n",
      "Epoch [21/1000], Loss: 0.1342, Val Loss: 0.4516\n",
      "Epoch [22/1000], Loss: 0.0098, Val Loss: 0.4513\n",
      "Epoch [23/1000], Loss: 0.5981, Val Loss: 0.4507\n",
      "Epoch [24/1000], Loss: 0.0690, Val Loss: 0.4507\n",
      "Epoch [25/1000], Loss: 0.0230, Val Loss: 0.4500\n",
      "Epoch [26/1000], Loss: 0.0065, Val Loss: 0.4493\n",
      "Epoch [27/1000], Loss: 0.0083, Val Loss: 0.4495\n",
      "Epoch [28/1000], Loss: 0.6495, Val Loss: 0.4500\n",
      "Epoch [29/1000], Loss: 1.0074, Val Loss: 0.4491\n",
      "Epoch [30/1000], Loss: 0.2182, Val Loss: 0.4486\n",
      "Epoch [31/1000], Loss: 0.1932, Val Loss: 0.4487\n",
      "Epoch [32/1000], Loss: 0.0377, Val Loss: 0.4480\n",
      "Epoch [33/1000], Loss: 0.1556, Val Loss: 0.4491\n",
      "Epoch [34/1000], Loss: 0.2390, Val Loss: 0.4485\n",
      "Epoch [35/1000], Loss: 0.2572, Val Loss: 0.4488\n",
      "Epoch [36/1000], Loss: 0.3072, Val Loss: 0.4489\n",
      "Epoch [37/1000], Loss: 0.0525, Val Loss: 0.4486\n",
      "Epoch [38/1000], Loss: 0.8668, Val Loss: 0.4483\n",
      "Epoch [39/1000], Loss: 0.8084, Val Loss: 0.4484\n",
      "Epoch [40/1000], Loss: 0.6464, Val Loss: 0.4483\n",
      "Epoch [41/1000], Loss: 0.2850, Val Loss: 0.4481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:31:56,929] Trial 81 finished with value: 0.8225808918366411 and parameters: {'hidden_size': 99, 'dropout_prob': 0.7102942627851749, 'learning_rate': 0.0003248535219851833, 'weight_decay': 5.902692343971749e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 42\n",
      "Epoch [1/1000], Loss: 0.4780, Val Loss: 0.5100\n",
      "Epoch [2/1000], Loss: 0.3829, Val Loss: 0.4760\n",
      "Epoch [3/1000], Loss: 0.7347, Val Loss: 0.4649\n",
      "Epoch [4/1000], Loss: 0.8551, Val Loss: 0.4611\n",
      "Epoch [5/1000], Loss: 0.2819, Val Loss: 0.4597\n",
      "Epoch [6/1000], Loss: 0.1368, Val Loss: 0.4566\n",
      "Epoch [7/1000], Loss: 0.7211, Val Loss: 0.4552\n",
      "Epoch [8/1000], Loss: 0.0321, Val Loss: 0.4543\n",
      "Epoch [9/1000], Loss: 0.4368, Val Loss: 0.4524\n",
      "Epoch [10/1000], Loss: 1.1526, Val Loss: 0.4519\n",
      "Epoch [11/1000], Loss: 0.4890, Val Loss: 0.4513\n",
      "Epoch [12/1000], Loss: 0.7055, Val Loss: 0.4491\n",
      "Epoch [13/1000], Loss: 0.1489, Val Loss: 0.4491\n",
      "Epoch [14/1000], Loss: 0.2891, Val Loss: 0.4481\n",
      "Epoch [15/1000], Loss: 0.0159, Val Loss: 0.4478\n",
      "Epoch [16/1000], Loss: 1.0666, Val Loss: 0.4480\n",
      "Epoch [17/1000], Loss: 0.5324, Val Loss: 0.4473\n",
      "Epoch [18/1000], Loss: 0.0023, Val Loss: 0.4476\n",
      "Epoch [19/1000], Loss: 0.6241, Val Loss: 0.4480\n",
      "Epoch [20/1000], Loss: 0.8710, Val Loss: 0.4471\n",
      "Epoch [21/1000], Loss: 0.2689, Val Loss: 0.4468\n",
      "Epoch [22/1000], Loss: 0.9557, Val Loss: 0.4475\n",
      "Epoch [23/1000], Loss: 0.1799, Val Loss: 0.4484\n",
      "Epoch [24/1000], Loss: 0.2350, Val Loss: 0.4483\n",
      "Epoch [25/1000], Loss: 0.5713, Val Loss: 0.4474\n",
      "Epoch [26/1000], Loss: 0.2781, Val Loss: 0.4468\n",
      "Epoch [27/1000], Loss: 0.3126, Val Loss: 0.4483\n",
      "Epoch [28/1000], Loss: 0.5354, Val Loss: 0.4465\n",
      "Epoch [29/1000], Loss: 0.0392, Val Loss: 0.4474\n",
      "Epoch [30/1000], Loss: 0.4534, Val Loss: 0.4469\n",
      "Epoch [31/1000], Loss: 0.3417, Val Loss: 0.4475\n",
      "Epoch [32/1000], Loss: 0.8177, Val Loss: 0.4473\n",
      "Epoch [33/1000], Loss: 0.0004, Val Loss: 0.4474\n",
      "Epoch [34/1000], Loss: 1.4612, Val Loss: 0.4477\n",
      "Epoch [35/1000], Loss: 0.0089, Val Loss: 0.4486\n",
      "Epoch [36/1000], Loss: 0.3908, Val Loss: 0.4491\n",
      "Epoch [37/1000], Loss: 0.0062, Val Loss: 0.4485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:32:08,105] Trial 82 finished with value: 0.8227287325968484 and parameters: {'hidden_size': 94, 'dropout_prob': 0.6837045040346963, 'learning_rate': 0.00047939592693021637, 'weight_decay': 5.469298789753015e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 38\n",
      "Epoch [1/1000], Loss: 1.0705, Val Loss: 0.4937\n",
      "Epoch [2/1000], Loss: 0.1517, Val Loss: 0.4711\n",
      "Epoch [3/1000], Loss: 1.7721, Val Loss: 0.4606\n",
      "Epoch [4/1000], Loss: 0.7148, Val Loss: 0.4552\n",
      "Epoch [5/1000], Loss: 0.4908, Val Loss: 0.4535\n",
      "Epoch [6/1000], Loss: 0.6494, Val Loss: 0.4511\n",
      "Epoch [7/1000], Loss: 0.5537, Val Loss: 0.4508\n",
      "Epoch [8/1000], Loss: 0.0028, Val Loss: 0.4509\n",
      "Epoch [9/1000], Loss: 0.0080, Val Loss: 0.4501\n",
      "Epoch [10/1000], Loss: 0.3658, Val Loss: 0.4492\n",
      "Epoch [11/1000], Loss: 0.0957, Val Loss: 0.4486\n",
      "Epoch [12/1000], Loss: 0.0637, Val Loss: 0.4500\n",
      "Epoch [13/1000], Loss: 0.6295, Val Loss: 0.4511\n",
      "Epoch [14/1000], Loss: 0.0374, Val Loss: 0.4489\n",
      "Epoch [15/1000], Loss: 0.0165, Val Loss: 0.4491\n",
      "Epoch [16/1000], Loss: 1.6692, Val Loss: 0.4507\n",
      "Epoch [17/1000], Loss: 0.8075, Val Loss: 0.4518\n",
      "Epoch [18/1000], Loss: 0.7293, Val Loss: 0.4510\n",
      "Epoch [19/1000], Loss: 0.0541, Val Loss: 0.4506\n",
      "Epoch [20/1000], Loss: 0.0002, Val Loss: 0.4519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:32:14,240] Trial 83 finished with value: 0.8214465963488431 and parameters: {'hidden_size': 102, 'dropout_prob': 0.731225613092916, 'learning_rate': 0.0009166945013840317, 'weight_decay': 3.8459978315006316e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 21\n",
      "Epoch [1/1000], Loss: 0.9529, Val Loss: 0.5482\n",
      "Epoch [2/1000], Loss: 0.0642, Val Loss: 0.4973\n",
      "Epoch [3/1000], Loss: 0.8775, Val Loss: 0.4833\n",
      "Epoch [4/1000], Loss: 0.5438, Val Loss: 0.4737\n",
      "Epoch [5/1000], Loss: 0.4272, Val Loss: 0.4683\n",
      "Epoch [6/1000], Loss: 0.3101, Val Loss: 0.4655\n",
      "Epoch [7/1000], Loss: 0.1364, Val Loss: 0.4633\n",
      "Epoch [8/1000], Loss: 0.5846, Val Loss: 0.4619\n",
      "Epoch [9/1000], Loss: 0.7971, Val Loss: 0.4599\n",
      "Epoch [10/1000], Loss: 0.4741, Val Loss: 0.4582\n",
      "Epoch [11/1000], Loss: 0.6990, Val Loss: 0.4566\n",
      "Epoch [12/1000], Loss: 0.2251, Val Loss: 0.4558\n",
      "Epoch [13/1000], Loss: 0.6813, Val Loss: 0.4546\n",
      "Epoch [14/1000], Loss: 0.0896, Val Loss: 0.4540\n",
      "Epoch [15/1000], Loss: 0.3779, Val Loss: 0.4538\n",
      "Epoch [16/1000], Loss: 0.0078, Val Loss: 0.4536\n",
      "Epoch [17/1000], Loss: 0.0538, Val Loss: 0.4526\n",
      "Epoch [18/1000], Loss: 0.8574, Val Loss: 0.4521\n",
      "Epoch [19/1000], Loss: 0.3282, Val Loss: 0.4529\n",
      "Epoch [20/1000], Loss: 0.0036, Val Loss: 0.4520\n",
      "Epoch [21/1000], Loss: 0.5648, Val Loss: 0.4514\n",
      "Epoch [22/1000], Loss: 0.8491, Val Loss: 0.4508\n",
      "Epoch [23/1000], Loss: 0.0087, Val Loss: 0.4510\n",
      "Epoch [24/1000], Loss: 0.8462, Val Loss: 0.4514\n",
      "Epoch [25/1000], Loss: 0.5078, Val Loss: 0.4504\n",
      "Epoch [26/1000], Loss: 0.0025, Val Loss: 0.4504\n",
      "Epoch [27/1000], Loss: 0.0667, Val Loss: 0.4502\n",
      "Epoch [28/1000], Loss: 0.0344, Val Loss: 0.4505\n",
      "Epoch [29/1000], Loss: 0.2028, Val Loss: 0.4495\n",
      "Epoch [30/1000], Loss: 0.6676, Val Loss: 0.4498\n",
      "Epoch [31/1000], Loss: 0.4860, Val Loss: 0.4488\n",
      "Epoch [32/1000], Loss: 0.0058, Val Loss: 0.4482\n",
      "Epoch [33/1000], Loss: 0.5246, Val Loss: 0.4489\n",
      "Epoch [34/1000], Loss: 1.6604, Val Loss: 0.4490\n",
      "Epoch [35/1000], Loss: 0.6308, Val Loss: 0.4487\n",
      "Epoch [36/1000], Loss: 0.0400, Val Loss: 0.4487\n",
      "Epoch [37/1000], Loss: 0.0563, Val Loss: 0.4478\n",
      "Epoch [38/1000], Loss: 0.4972, Val Loss: 0.4475\n",
      "Epoch [39/1000], Loss: 0.6045, Val Loss: 0.4477\n",
      "Epoch [40/1000], Loss: 0.4335, Val Loss: 0.4480\n",
      "Epoch [41/1000], Loss: 0.0815, Val Loss: 0.4475\n",
      "Epoch [42/1000], Loss: 0.0133, Val Loss: 0.4474\n",
      "Epoch [43/1000], Loss: 0.1004, Val Loss: 0.4469\n",
      "Epoch [44/1000], Loss: 0.0080, Val Loss: 0.4472\n",
      "Epoch [45/1000], Loss: 0.0217, Val Loss: 0.4475\n",
      "Epoch [46/1000], Loss: 0.5074, Val Loss: 0.4472\n",
      "Epoch [47/1000], Loss: 0.3717, Val Loss: 0.4483\n",
      "Epoch [48/1000], Loss: 0.3376, Val Loss: 0.4476\n",
      "Epoch [49/1000], Loss: 0.0068, Val Loss: 0.4472\n",
      "Epoch [50/1000], Loss: 0.0025, Val Loss: 0.4476\n",
      "Epoch [51/1000], Loss: 0.6677, Val Loss: 0.4475\n",
      "Epoch [52/1000], Loss: 0.8594, Val Loss: 0.4467\n",
      "Epoch [53/1000], Loss: 0.4113, Val Loss: 0.4471\n",
      "Epoch [54/1000], Loss: 0.1249, Val Loss: 0.4473\n",
      "Epoch [55/1000], Loss: 0.2800, Val Loss: 0.4469\n",
      "Epoch [56/1000], Loss: 0.9721, Val Loss: 0.4468\n",
      "Epoch [57/1000], Loss: 0.0787, Val Loss: 0.4471\n",
      "Epoch [58/1000], Loss: 0.4947, Val Loss: 0.4468\n",
      "Epoch [59/1000], Loss: 0.5039, Val Loss: 0.4469\n",
      "Epoch [60/1000], Loss: 0.2494, Val Loss: 0.4469\n",
      "Epoch [61/1000], Loss: 0.0265, Val Loss: 0.4466\n",
      "Epoch [62/1000], Loss: 0.2075, Val Loss: 0.4464\n",
      "Epoch [63/1000], Loss: 0.0988, Val Loss: 0.4464\n",
      "Epoch [64/1000], Loss: 0.5174, Val Loss: 0.4469\n",
      "Epoch [65/1000], Loss: 1.0871, Val Loss: 0.4467\n",
      "Epoch [66/1000], Loss: 0.4703, Val Loss: 0.4465\n",
      "Epoch [67/1000], Loss: 0.9770, Val Loss: 0.4467\n",
      "Epoch [68/1000], Loss: 1.6537, Val Loss: 0.4465\n",
      "Epoch [69/1000], Loss: 0.3154, Val Loss: 0.4465\n",
      "Epoch [70/1000], Loss: 0.1734, Val Loss: 0.4465\n",
      "Epoch [71/1000], Loss: 1.0957, Val Loss: 0.4464\n",
      "Epoch [72/1000], Loss: 0.0857, Val Loss: 0.4462\n",
      "Epoch [73/1000], Loss: 0.0120, Val Loss: 0.4460\n",
      "Epoch [74/1000], Loss: 0.3992, Val Loss: 0.4460\n",
      "Epoch [75/1000], Loss: 1.2409, Val Loss: 0.4450\n",
      "Epoch [76/1000], Loss: 0.3682, Val Loss: 0.4455\n",
      "Epoch [77/1000], Loss: 0.1030, Val Loss: 0.4462\n",
      "Epoch [78/1000], Loss: 0.5525, Val Loss: 0.4462\n",
      "Epoch [79/1000], Loss: 1.9165, Val Loss: 0.4465\n",
      "Epoch [80/1000], Loss: 0.4005, Val Loss: 0.4463\n",
      "Epoch [81/1000], Loss: 0.0190, Val Loss: 0.4462\n",
      "Epoch [82/1000], Loss: 0.5818, Val Loss: 0.4465\n",
      "Epoch [83/1000], Loss: 0.3175, Val Loss: 0.4469\n",
      "Epoch [84/1000], Loss: 0.8319, Val Loss: 0.4463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:32:40,408] Trial 84 finished with value: 0.8246226237146775 and parameters: {'hidden_size': 92, 'dropout_prob': 0.7669968285179096, 'learning_rate': 0.00027959665557968387, 'weight_decay': 4.969319205117978e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 85\n",
      "Epoch [1/1000], Loss: 0.5632, Val Loss: 0.6573\n",
      "Epoch [2/1000], Loss: 0.5350, Val Loss: 0.6399\n",
      "Epoch [3/1000], Loss: 1.1045, Val Loss: 0.6176\n",
      "Epoch [4/1000], Loss: 0.3538, Val Loss: 0.5929\n",
      "Epoch [5/1000], Loss: 0.3435, Val Loss: 0.5683\n",
      "Epoch [6/1000], Loss: 0.3312, Val Loss: 0.5502\n",
      "Epoch [7/1000], Loss: 0.4222, Val Loss: 0.5386\n",
      "Epoch [8/1000], Loss: 0.4377, Val Loss: 0.5305\n",
      "Epoch [9/1000], Loss: 0.3798, Val Loss: 0.5248\n",
      "Epoch [10/1000], Loss: 0.2215, Val Loss: 0.5198\n",
      "Epoch [11/1000], Loss: 0.9973, Val Loss: 0.5151\n",
      "Epoch [12/1000], Loss: 2.2714, Val Loss: 0.5111\n",
      "Epoch [13/1000], Loss: 0.2803, Val Loss: 0.5081\n",
      "Epoch [14/1000], Loss: 0.7813, Val Loss: 0.5046\n",
      "Epoch [15/1000], Loss: 0.6036, Val Loss: 0.5013\n",
      "Epoch [16/1000], Loss: 0.9990, Val Loss: 0.4986\n",
      "Epoch [17/1000], Loss: 0.2734, Val Loss: 0.4957\n",
      "Epoch [18/1000], Loss: 0.2697, Val Loss: 0.4926\n",
      "Epoch [19/1000], Loss: 1.5088, Val Loss: 0.4901\n",
      "Epoch [20/1000], Loss: 0.3590, Val Loss: 0.4880\n",
      "Epoch [21/1000], Loss: 0.0410, Val Loss: 0.4860\n",
      "Epoch [22/1000], Loss: 0.9835, Val Loss: 0.4842\n",
      "Epoch [23/1000], Loss: 0.7707, Val Loss: 0.4822\n",
      "Epoch [24/1000], Loss: 0.4503, Val Loss: 0.4802\n",
      "Epoch [25/1000], Loss: 0.2202, Val Loss: 0.4786\n",
      "Epoch [26/1000], Loss: 0.2731, Val Loss: 0.4771\n",
      "Epoch [27/1000], Loss: 1.3753, Val Loss: 0.4755\n",
      "Epoch [28/1000], Loss: 0.3768, Val Loss: 0.4745\n",
      "Epoch [29/1000], Loss: 0.3299, Val Loss: 0.4731\n",
      "Epoch [30/1000], Loss: 0.5918, Val Loss: 0.4719\n",
      "Epoch [31/1000], Loss: 0.7643, Val Loss: 0.4709\n",
      "Epoch [32/1000], Loss: 0.0374, Val Loss: 0.4699\n",
      "Epoch [33/1000], Loss: 0.4919, Val Loss: 0.4693\n",
      "Epoch [34/1000], Loss: 0.3655, Val Loss: 0.4686\n",
      "Epoch [35/1000], Loss: 0.3703, Val Loss: 0.4678\n",
      "Epoch [36/1000], Loss: 0.3386, Val Loss: 0.4671\n",
      "Epoch [37/1000], Loss: 0.7635, Val Loss: 0.4667\n",
      "Epoch [38/1000], Loss: 2.3696, Val Loss: 0.4660\n",
      "Epoch [39/1000], Loss: 0.2170, Val Loss: 0.4656\n",
      "Epoch [40/1000], Loss: 0.5117, Val Loss: 0.4653\n",
      "Epoch [41/1000], Loss: 0.1457, Val Loss: 0.4648\n",
      "Epoch [42/1000], Loss: 0.0189, Val Loss: 0.4644\n",
      "Epoch [43/1000], Loss: 0.6993, Val Loss: 0.4642\n",
      "Epoch [44/1000], Loss: 0.0311, Val Loss: 0.4638\n",
      "Epoch [45/1000], Loss: 0.5897, Val Loss: 0.4634\n",
      "Epoch [46/1000], Loss: 0.0302, Val Loss: 0.4631\n",
      "Epoch [47/1000], Loss: 0.7323, Val Loss: 0.4628\n",
      "Epoch [48/1000], Loss: 0.0131, Val Loss: 0.4625\n",
      "Epoch [49/1000], Loss: 0.1665, Val Loss: 0.4622\n",
      "Epoch [50/1000], Loss: 0.1950, Val Loss: 0.4619\n",
      "Epoch [51/1000], Loss: 0.3254, Val Loss: 0.4618\n",
      "Epoch [52/1000], Loss: 0.0410, Val Loss: 0.4617\n",
      "Epoch [53/1000], Loss: 0.0589, Val Loss: 0.4612\n",
      "Epoch [54/1000], Loss: 0.6575, Val Loss: 0.4611\n",
      "Epoch [55/1000], Loss: 0.7040, Val Loss: 0.4609\n",
      "Epoch [56/1000], Loss: 0.6913, Val Loss: 0.4607\n",
      "Epoch [57/1000], Loss: 0.2594, Val Loss: 0.4605\n",
      "Epoch [58/1000], Loss: 0.0028, Val Loss: 0.4603\n",
      "Epoch [59/1000], Loss: 0.0060, Val Loss: 0.4599\n",
      "Epoch [60/1000], Loss: 0.6742, Val Loss: 0.4598\n",
      "Epoch [61/1000], Loss: 0.0096, Val Loss: 0.4596\n",
      "Epoch [62/1000], Loss: 0.6481, Val Loss: 0.4594\n",
      "Epoch [63/1000], Loss: 0.7037, Val Loss: 0.4592\n",
      "Epoch [64/1000], Loss: 0.0159, Val Loss: 0.4591\n",
      "Epoch [65/1000], Loss: 0.0032, Val Loss: 0.4590\n",
      "Epoch [66/1000], Loss: 0.0400, Val Loss: 0.4586\n",
      "Epoch [67/1000], Loss: 0.5706, Val Loss: 0.4583\n",
      "Epoch [68/1000], Loss: 0.0395, Val Loss: 0.4581\n",
      "Epoch [69/1000], Loss: 0.2401, Val Loss: 0.4579\n",
      "Epoch [70/1000], Loss: 2.8151, Val Loss: 0.4579\n",
      "Epoch [71/1000], Loss: 0.6362, Val Loss: 0.4579\n",
      "Epoch [72/1000], Loss: 0.4158, Val Loss: 0.4577\n",
      "Epoch [73/1000], Loss: 0.1296, Val Loss: 0.4575\n",
      "Epoch [74/1000], Loss: 0.0052, Val Loss: 0.4573\n",
      "Epoch [75/1000], Loss: 0.2411, Val Loss: 0.4570\n",
      "Epoch [76/1000], Loss: 0.1822, Val Loss: 0.4569\n",
      "Epoch [77/1000], Loss: 0.1254, Val Loss: 0.4567\n",
      "Epoch [78/1000], Loss: 0.3931, Val Loss: 0.4565\n",
      "Epoch [79/1000], Loss: 0.6064, Val Loss: 0.4564\n",
      "Epoch [80/1000], Loss: 0.9936, Val Loss: 0.4562\n",
      "Epoch [81/1000], Loss: 0.0257, Val Loss: 0.4561\n",
      "Epoch [82/1000], Loss: 0.1702, Val Loss: 0.4561\n",
      "Epoch [83/1000], Loss: 0.5871, Val Loss: 0.4558\n",
      "Epoch [84/1000], Loss: 0.0333, Val Loss: 0.4556\n",
      "Epoch [85/1000], Loss: 0.1021, Val Loss: 0.4555\n",
      "Epoch [86/1000], Loss: 0.0104, Val Loss: 0.4556\n",
      "Epoch [87/1000], Loss: 0.4700, Val Loss: 0.4554\n",
      "Epoch [88/1000], Loss: 0.0138, Val Loss: 0.4552\n",
      "Epoch [89/1000], Loss: 0.1003, Val Loss: 0.4551\n",
      "Epoch [90/1000], Loss: 0.3451, Val Loss: 0.4549\n",
      "Epoch [91/1000], Loss: 0.0066, Val Loss: 0.4548\n",
      "Epoch [92/1000], Loss: 0.0364, Val Loss: 0.4546\n",
      "Epoch [93/1000], Loss: 0.7150, Val Loss: 0.4546\n",
      "Epoch [94/1000], Loss: 0.6333, Val Loss: 0.4544\n",
      "Epoch [95/1000], Loss: 0.1287, Val Loss: 0.4544\n",
      "Epoch [96/1000], Loss: 0.0955, Val Loss: 0.4542\n",
      "Epoch [97/1000], Loss: 0.0070, Val Loss: 0.4542\n",
      "Epoch [98/1000], Loss: 0.1337, Val Loss: 0.4541\n",
      "Epoch [99/1000], Loss: 0.0033, Val Loss: 0.4539\n",
      "Epoch [100/1000], Loss: 0.3674, Val Loss: 0.4539\n",
      "Epoch [101/1000], Loss: 0.0621, Val Loss: 0.4538\n",
      "Epoch [102/1000], Loss: 0.0368, Val Loss: 0.4537\n",
      "Epoch [103/1000], Loss: 0.0142, Val Loss: 0.4536\n",
      "Epoch [104/1000], Loss: 0.3274, Val Loss: 0.4535\n",
      "Epoch [105/1000], Loss: 0.0107, Val Loss: 0.4535\n",
      "Epoch [106/1000], Loss: 0.7460, Val Loss: 0.4534\n",
      "Epoch [107/1000], Loss: 0.0801, Val Loss: 0.4532\n",
      "Epoch [108/1000], Loss: 0.0170, Val Loss: 0.4532\n",
      "Epoch [109/1000], Loss: 0.1384, Val Loss: 0.4531\n",
      "Epoch [110/1000], Loss: 0.8532, Val Loss: 0.4529\n",
      "Epoch [111/1000], Loss: 0.8117, Val Loss: 0.4527\n",
      "Epoch [112/1000], Loss: 0.0906, Val Loss: 0.4526\n",
      "Epoch [113/1000], Loss: 0.0245, Val Loss: 0.4525\n",
      "Epoch [114/1000], Loss: 0.7443, Val Loss: 0.4526\n",
      "Epoch [115/1000], Loss: 0.2844, Val Loss: 0.4526\n",
      "Epoch [116/1000], Loss: 0.3390, Val Loss: 0.4524\n",
      "Epoch [117/1000], Loss: 0.5064, Val Loss: 0.4525\n",
      "Epoch [118/1000], Loss: 0.0993, Val Loss: 0.4523\n",
      "Epoch [119/1000], Loss: 1.3249, Val Loss: 0.4523\n",
      "Epoch [120/1000], Loss: 0.1783, Val Loss: 0.4522\n",
      "Epoch [121/1000], Loss: 0.8621, Val Loss: 0.4523\n",
      "Epoch [122/1000], Loss: 0.3905, Val Loss: 0.4521\n",
      "Epoch [123/1000], Loss: 0.4576, Val Loss: 0.4520\n",
      "Epoch [124/1000], Loss: 0.0042, Val Loss: 0.4519\n",
      "Epoch [125/1000], Loss: 0.4945, Val Loss: 0.4518\n",
      "Epoch [126/1000], Loss: 0.5949, Val Loss: 0.4517\n",
      "Epoch [127/1000], Loss: 0.0775, Val Loss: 0.4516\n",
      "Epoch [128/1000], Loss: 0.8465, Val Loss: 0.4516\n",
      "Epoch [129/1000], Loss: 0.0117, Val Loss: 0.4517\n",
      "Epoch [130/1000], Loss: 0.3530, Val Loss: 0.4515\n",
      "Epoch [131/1000], Loss: 0.0626, Val Loss: 0.4514\n",
      "Epoch [132/1000], Loss: 0.2701, Val Loss: 0.4514\n",
      "Epoch [133/1000], Loss: 0.1246, Val Loss: 0.4514\n",
      "Epoch [134/1000], Loss: 0.1593, Val Loss: 0.4513\n",
      "Epoch [135/1000], Loss: 0.2334, Val Loss: 0.4512\n",
      "Epoch [136/1000], Loss: 0.3810, Val Loss: 0.4512\n",
      "Epoch [137/1000], Loss: 1.7073, Val Loss: 0.4512\n",
      "Epoch [138/1000], Loss: 0.8534, Val Loss: 0.4512\n",
      "Epoch [139/1000], Loss: 0.4931, Val Loss: 0.4512\n",
      "Epoch [140/1000], Loss: 0.8425, Val Loss: 0.4512\n",
      "Epoch [141/1000], Loss: 0.3977, Val Loss: 0.4511\n",
      "Epoch [142/1000], Loss: 0.6747, Val Loss: 0.4511\n",
      "Epoch [143/1000], Loss: 0.5856, Val Loss: 0.4511\n",
      "Epoch [144/1000], Loss: 0.0318, Val Loss: 0.4510\n",
      "Epoch [145/1000], Loss: 0.4425, Val Loss: 0.4510\n",
      "Epoch [146/1000], Loss: 0.5617, Val Loss: 0.4509\n",
      "Epoch [147/1000], Loss: 0.3425, Val Loss: 0.4509\n",
      "Epoch [148/1000], Loss: 0.8921, Val Loss: 0.4509\n",
      "Epoch [149/1000], Loss: 0.1569, Val Loss: 0.4508\n",
      "Epoch [150/1000], Loss: 1.2815, Val Loss: 0.4507\n",
      "Epoch [151/1000], Loss: 0.5914, Val Loss: 0.4507\n",
      "Epoch [152/1000], Loss: 0.6155, Val Loss: 0.4507\n",
      "Epoch [153/1000], Loss: 0.5072, Val Loss: 0.4507\n",
      "Epoch [154/1000], Loss: 0.7308, Val Loss: 0.4507\n",
      "Epoch [155/1000], Loss: 0.2963, Val Loss: 0.4506\n",
      "Epoch [156/1000], Loss: 0.1079, Val Loss: 0.4507\n",
      "Epoch [157/1000], Loss: 0.5433, Val Loss: 0.4506\n",
      "Epoch [158/1000], Loss: 0.1374, Val Loss: 0.4505\n",
      "Epoch [159/1000], Loss: 0.1913, Val Loss: 0.4504\n",
      "Epoch [160/1000], Loss: 0.5299, Val Loss: 0.4504\n",
      "Epoch [161/1000], Loss: 1.7182, Val Loss: 0.4502\n",
      "Epoch [162/1000], Loss: 0.9534, Val Loss: 0.4502\n",
      "Epoch [163/1000], Loss: 0.2466, Val Loss: 0.4501\n",
      "Epoch [164/1000], Loss: 0.7031, Val Loss: 0.4500\n",
      "Epoch [165/1000], Loss: 0.7584, Val Loss: 0.4499\n",
      "Epoch [166/1000], Loss: 0.0069, Val Loss: 0.4499\n",
      "Epoch [167/1000], Loss: 0.0301, Val Loss: 0.4499\n",
      "Epoch [168/1000], Loss: 0.8734, Val Loss: 0.4499\n",
      "Epoch [169/1000], Loss: 0.2262, Val Loss: 0.4499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 0.2491, Val Loss: 0.4499\n",
      "Epoch [171/1000], Loss: 0.1125, Val Loss: 0.4498\n",
      "Epoch [172/1000], Loss: 1.1946, Val Loss: 0.4498\n",
      "Epoch [173/1000], Loss: 0.0144, Val Loss: 0.4498\n",
      "Epoch [174/1000], Loss: 0.0026, Val Loss: 0.4498\n",
      "Epoch [175/1000], Loss: 0.7014, Val Loss: 0.4499\n",
      "Epoch [176/1000], Loss: 0.0699, Val Loss: 0.4498\n",
      "Epoch [177/1000], Loss: 0.5744, Val Loss: 0.4498\n",
      "Epoch [178/1000], Loss: 0.3192, Val Loss: 0.4497\n",
      "Epoch [179/1000], Loss: 0.0747, Val Loss: 0.4497\n",
      "Epoch [180/1000], Loss: 0.4847, Val Loss: 0.4497\n",
      "Epoch [181/1000], Loss: 0.8442, Val Loss: 0.4496\n",
      "Epoch [182/1000], Loss: 0.8431, Val Loss: 0.4495\n",
      "Epoch [183/1000], Loss: 0.1417, Val Loss: 0.4494\n",
      "Epoch [184/1000], Loss: 1.3850, Val Loss: 0.4495\n",
      "Epoch [185/1000], Loss: 0.4373, Val Loss: 0.4495\n",
      "Epoch [186/1000], Loss: 0.2986, Val Loss: 0.4496\n",
      "Epoch [187/1000], Loss: 0.2569, Val Loss: 0.4495\n",
      "Epoch [188/1000], Loss: 0.0530, Val Loss: 0.4495\n",
      "Epoch [189/1000], Loss: 0.0532, Val Loss: 0.4494\n",
      "Epoch [190/1000], Loss: 0.6261, Val Loss: 0.4494\n",
      "Epoch [191/1000], Loss: 3.5552, Val Loss: 0.4493\n",
      "Epoch [192/1000], Loss: 0.1311, Val Loss: 0.4493\n",
      "Epoch [193/1000], Loss: 0.4771, Val Loss: 0.4492\n",
      "Epoch [194/1000], Loss: 0.4920, Val Loss: 0.4492\n",
      "Epoch [195/1000], Loss: 0.6736, Val Loss: 0.4492\n",
      "Epoch [196/1000], Loss: 0.0454, Val Loss: 0.4493\n",
      "Epoch [197/1000], Loss: 0.0289, Val Loss: 0.4493\n",
      "Epoch [198/1000], Loss: 0.7513, Val Loss: 0.4493\n",
      "Epoch [199/1000], Loss: 0.8480, Val Loss: 0.4492\n",
      "Epoch [200/1000], Loss: 0.0074, Val Loss: 0.4492\n",
      "Epoch [201/1000], Loss: 0.6378, Val Loss: 0.4493\n",
      "Epoch [202/1000], Loss: 0.8649, Val Loss: 0.4492\n",
      "Epoch [203/1000], Loss: 0.1030, Val Loss: 0.4493\n",
      "Epoch [204/1000], Loss: 0.3402, Val Loss: 0.4493\n",
      "Epoch [205/1000], Loss: 0.9888, Val Loss: 0.4493\n",
      "Epoch [206/1000], Loss: 0.0751, Val Loss: 0.4492\n",
      "Epoch [207/1000], Loss: 0.0807, Val Loss: 0.4492\n",
      "Epoch [208/1000], Loss: 1.2030, Val Loss: 0.4492\n",
      "Epoch [209/1000], Loss: 0.1229, Val Loss: 0.4493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:33:41,109] Trial 85 finished with value: 0.8215867901731777 and parameters: {'hidden_size': 92, 'dropout_prob': 0.7832875152560874, 'learning_rate': 5.7216187728389734e-05, 'weight_decay': 6.868170839611574e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 210\n",
      "Epoch [1/1000], Loss: 0.2894, Val Loss: 0.5378\n",
      "Epoch [2/1000], Loss: 0.5116, Val Loss: 0.5057\n",
      "Epoch [3/1000], Loss: 0.4360, Val Loss: 0.4952\n",
      "Epoch [4/1000], Loss: 0.3051, Val Loss: 0.4831\n",
      "Epoch [5/1000], Loss: 0.4226, Val Loss: 0.4792\n",
      "Epoch [6/1000], Loss: 0.5285, Val Loss: 0.4778\n",
      "Epoch [7/1000], Loss: 0.0088, Val Loss: 0.4763\n",
      "Epoch [8/1000], Loss: 0.0742, Val Loss: 0.4711\n",
      "Epoch [9/1000], Loss: 0.0255, Val Loss: 0.4706\n",
      "Epoch [10/1000], Loss: 0.0011, Val Loss: 0.4719\n",
      "Epoch [11/1000], Loss: 0.4517, Val Loss: 0.4666\n",
      "Epoch [12/1000], Loss: 0.5136, Val Loss: 0.4771\n",
      "Epoch [13/1000], Loss: 0.0356, Val Loss: 0.4672\n",
      "Epoch [14/1000], Loss: 0.8448, Val Loss: 0.4684\n",
      "Epoch [15/1000], Loss: 0.6008, Val Loss: 0.4682\n",
      "Epoch [16/1000], Loss: 0.0383, Val Loss: 0.4678\n",
      "Epoch [17/1000], Loss: 0.0840, Val Loss: 0.4713\n",
      "Epoch [18/1000], Loss: 1.3443, Val Loss: 0.4647\n",
      "Epoch [19/1000], Loss: 0.6053, Val Loss: 0.4666\n",
      "Epoch [20/1000], Loss: 0.0724, Val Loss: 0.4684\n",
      "Epoch [21/1000], Loss: 0.1923, Val Loss: 0.4676\n",
      "Epoch [22/1000], Loss: 0.4290, Val Loss: 0.4598\n",
      "Epoch [23/1000], Loss: 0.0381, Val Loss: 0.4660\n",
      "Epoch [24/1000], Loss: 0.0350, Val Loss: 0.4650\n",
      "Epoch [25/1000], Loss: 0.7076, Val Loss: 0.4666\n",
      "Epoch [26/1000], Loss: 0.6602, Val Loss: 0.4607\n",
      "Epoch [27/1000], Loss: 0.5699, Val Loss: 0.4713\n",
      "Epoch [28/1000], Loss: 0.0022, Val Loss: 0.4686\n",
      "Epoch [29/1000], Loss: 0.0360, Val Loss: 0.4679\n",
      "Epoch [30/1000], Loss: 0.2295, Val Loss: 0.4659\n",
      "Epoch [31/1000], Loss: 0.7305, Val Loss: 0.4684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:33:49,484] Trial 86 finished with value: 0.8207558231416672 and parameters: {'hidden_size': 21, 'dropout_prob': 0.7696242164886132, 'learning_rate': 0.0006815811942210974, 'weight_decay': 6.136457290584222e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 32\n",
      "Epoch [1/1000], Loss: 0.1196, Val Loss: 0.4742\n",
      "Epoch [2/1000], Loss: 0.1085, Val Loss: 0.4580\n",
      "Epoch [3/1000], Loss: 0.7523, Val Loss: 0.4507\n",
      "Epoch [4/1000], Loss: 0.4010, Val Loss: 0.4514\n",
      "Epoch [5/1000], Loss: 2.9147, Val Loss: 0.4522\n",
      "Epoch [6/1000], Loss: 0.0382, Val Loss: 0.4484\n",
      "Epoch [7/1000], Loss: 0.0242, Val Loss: 0.4499\n",
      "Epoch [8/1000], Loss: 0.9340, Val Loss: 0.4496\n",
      "Epoch [9/1000], Loss: 1.4701, Val Loss: 0.4493\n",
      "Epoch [10/1000], Loss: 1.1144, Val Loss: 0.4486\n",
      "Epoch [11/1000], Loss: 0.4064, Val Loss: 0.4492\n",
      "Epoch [12/1000], Loss: 0.2834, Val Loss: 0.4489\n",
      "Epoch [13/1000], Loss: 0.6516, Val Loss: 0.4494\n",
      "Epoch [14/1000], Loss: 0.6076, Val Loss: 0.4496\n",
      "Epoch [15/1000], Loss: 0.1051, Val Loss: 0.4507\n",
      "Epoch [16/1000], Loss: 0.0298, Val Loss: 0.4475\n",
      "Epoch [17/1000], Loss: 0.2077, Val Loss: 0.4482\n",
      "Epoch [18/1000], Loss: 1.4673, Val Loss: 0.4482\n",
      "Epoch [19/1000], Loss: 0.4099, Val Loss: 0.4475\n",
      "Epoch [20/1000], Loss: 0.1005, Val Loss: 0.4486\n",
      "Epoch [21/1000], Loss: 0.8544, Val Loss: 0.4496\n",
      "Epoch [22/1000], Loss: 0.0219, Val Loss: 0.4493\n",
      "Epoch [23/1000], Loss: 0.6818, Val Loss: 0.4478\n",
      "Epoch [24/1000], Loss: 0.7650, Val Loss: 0.4478\n",
      "Epoch [25/1000], Loss: 1.0472, Val Loss: 0.4472\n",
      "Epoch [26/1000], Loss: 1.6426, Val Loss: 0.4477\n",
      "Epoch [27/1000], Loss: 0.3151, Val Loss: 0.4478\n",
      "Epoch [28/1000], Loss: 0.0962, Val Loss: 0.4474\n",
      "Epoch [29/1000], Loss: 0.0684, Val Loss: 0.4500\n",
      "Epoch [30/1000], Loss: 0.1081, Val Loss: 0.4520\n",
      "Epoch [31/1000], Loss: 0.3572, Val Loss: 0.4484\n",
      "Epoch [32/1000], Loss: 0.3271, Val Loss: 0.4504\n",
      "Epoch [33/1000], Loss: 0.1862, Val Loss: 0.4480\n",
      "Epoch [34/1000], Loss: 0.2820, Val Loss: 0.4485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:33:59,791] Trial 87 finished with value: 0.8203021049465479 and parameters: {'hidden_size': 109, 'dropout_prob': 0.7511965706165273, 'learning_rate': 0.002160312025809908, 'weight_decay': 8.94898507636519e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 35\n",
      "Epoch [1/1000], Loss: 0.5946, Val Loss: 0.5954\n",
      "Epoch [2/1000], Loss: 0.1123, Val Loss: 0.5156\n",
      "Epoch [3/1000], Loss: 0.0496, Val Loss: 0.4968\n",
      "Epoch [4/1000], Loss: 0.5518, Val Loss: 0.4838\n",
      "Epoch [5/1000], Loss: 0.7862, Val Loss: 0.4743\n",
      "Epoch [6/1000], Loss: 0.2410, Val Loss: 0.4688\n",
      "Epoch [7/1000], Loss: 0.0375, Val Loss: 0.4647\n",
      "Epoch [8/1000], Loss: 0.8811, Val Loss: 0.4621\n",
      "Epoch [9/1000], Loss: 0.9755, Val Loss: 0.4612\n",
      "Epoch [10/1000], Loss: 0.0471, Val Loss: 0.4600\n",
      "Epoch [11/1000], Loss: 0.5567, Val Loss: 0.4588\n",
      "Epoch [12/1000], Loss: 0.8110, Val Loss: 0.4574\n",
      "Epoch [13/1000], Loss: 1.7454, Val Loss: 0.4567\n",
      "Epoch [14/1000], Loss: 0.0048, Val Loss: 0.4562\n",
      "Epoch [15/1000], Loss: 0.7804, Val Loss: 0.4556\n",
      "Epoch [16/1000], Loss: 0.2363, Val Loss: 0.4551\n",
      "Epoch [17/1000], Loss: 0.2250, Val Loss: 0.4551\n",
      "Epoch [18/1000], Loss: 0.0120, Val Loss: 0.4547\n",
      "Epoch [19/1000], Loss: 0.2013, Val Loss: 0.4541\n",
      "Epoch [20/1000], Loss: 0.0008, Val Loss: 0.4540\n",
      "Epoch [21/1000], Loss: 0.9146, Val Loss: 0.4536\n",
      "Epoch [22/1000], Loss: 0.0011, Val Loss: 0.4536\n",
      "Epoch [23/1000], Loss: 0.4956, Val Loss: 0.4531\n",
      "Epoch [24/1000], Loss: 0.0486, Val Loss: 0.4528\n",
      "Epoch [25/1000], Loss: 0.0418, Val Loss: 0.4527\n",
      "Epoch [26/1000], Loss: 0.0448, Val Loss: 0.4523\n",
      "Epoch [27/1000], Loss: 0.4144, Val Loss: 0.4522\n",
      "Epoch [28/1000], Loss: 0.0039, Val Loss: 0.4521\n",
      "Epoch [29/1000], Loss: 0.4589, Val Loss: 0.4517\n",
      "Epoch [30/1000], Loss: 0.4945, Val Loss: 0.4513\n",
      "Epoch [31/1000], Loss: 0.7805, Val Loss: 0.4509\n",
      "Epoch [32/1000], Loss: 0.5008, Val Loss: 0.4507\n",
      "Epoch [33/1000], Loss: 0.0738, Val Loss: 0.4503\n",
      "Epoch [34/1000], Loss: 0.3238, Val Loss: 0.4502\n",
      "Epoch [35/1000], Loss: 0.6861, Val Loss: 0.4498\n",
      "Epoch [36/1000], Loss: 0.1155, Val Loss: 0.4494\n",
      "Epoch [37/1000], Loss: 0.1970, Val Loss: 0.4494\n",
      "Epoch [38/1000], Loss: 0.0150, Val Loss: 0.4494\n",
      "Epoch [39/1000], Loss: 0.0063, Val Loss: 0.4492\n",
      "Epoch [40/1000], Loss: 0.8170, Val Loss: 0.4493\n",
      "Epoch [41/1000], Loss: 0.8239, Val Loss: 0.4495\n",
      "Epoch [42/1000], Loss: 0.4705, Val Loss: 0.4496\n",
      "Epoch [43/1000], Loss: 0.6259, Val Loss: 0.4498\n",
      "Epoch [44/1000], Loss: 0.3982, Val Loss: 0.4491\n",
      "Epoch [45/1000], Loss: 0.0253, Val Loss: 0.4496\n",
      "Epoch [46/1000], Loss: 1.0325, Val Loss: 0.4490\n",
      "Epoch [47/1000], Loss: 0.1698, Val Loss: 0.4490\n",
      "Epoch [48/1000], Loss: 0.4137, Val Loss: 0.4488\n",
      "Epoch [49/1000], Loss: 0.3133, Val Loss: 0.4490\n",
      "Epoch [50/1000], Loss: 0.4022, Val Loss: 0.4489\n",
      "Epoch [51/1000], Loss: 0.6283, Val Loss: 0.4485\n",
      "Epoch [52/1000], Loss: 0.0244, Val Loss: 0.4484\n",
      "Epoch [53/1000], Loss: 0.9593, Val Loss: 0.4485\n",
      "Epoch [54/1000], Loss: 0.2382, Val Loss: 0.4485\n",
      "Epoch [55/1000], Loss: 0.0442, Val Loss: 0.4485\n",
      "Epoch [56/1000], Loss: 0.5924, Val Loss: 0.4483\n",
      "Epoch [57/1000], Loss: 0.0089, Val Loss: 0.4486\n",
      "Epoch [58/1000], Loss: 1.1245, Val Loss: 0.4492\n",
      "Epoch [59/1000], Loss: 0.3166, Val Loss: 0.4488\n",
      "Epoch [60/1000], Loss: 0.4728, Val Loss: 0.4490\n",
      "Epoch [61/1000], Loss: 0.0627, Val Loss: 0.4485\n",
      "Epoch [62/1000], Loss: 0.0428, Val Loss: 0.4486\n",
      "Epoch [63/1000], Loss: 0.5673, Val Loss: 0.4485\n",
      "Epoch [64/1000], Loss: 1.0432, Val Loss: 0.4483\n",
      "Epoch [65/1000], Loss: 0.2393, Val Loss: 0.4487\n",
      "Epoch [66/1000], Loss: 0.3396, Val Loss: 0.4487\n",
      "Epoch [67/1000], Loss: 0.6098, Val Loss: 0.4486\n",
      "Epoch [68/1000], Loss: 0.5917, Val Loss: 0.4482\n",
      "Epoch [69/1000], Loss: 0.0705, Val Loss: 0.4483\n",
      "Epoch [70/1000], Loss: 0.0259, Val Loss: 0.4484\n",
      "Epoch [71/1000], Loss: 0.7801, Val Loss: 0.4484\n",
      "Epoch [72/1000], Loss: 0.7124, Val Loss: 0.4485\n",
      "Epoch [73/1000], Loss: 1.0452, Val Loss: 0.4483\n",
      "Epoch [74/1000], Loss: 0.0696, Val Loss: 0.4482\n",
      "Epoch [75/1000], Loss: 0.0663, Val Loss: 0.4484\n",
      "Epoch [76/1000], Loss: 0.9754, Val Loss: 0.4485\n",
      "Epoch [77/1000], Loss: 0.3838, Val Loss: 0.4484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:34:22,186] Trial 88 finished with value: 0.8207252353981759 and parameters: {'hidden_size': 96, 'dropout_prob': 0.695858577937616, 'learning_rate': 0.0002126492199016807, 'weight_decay': 4.921142192411208e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 78\n",
      "Epoch [1/1000], Loss: 0.6542, Val Loss: 0.4588\n",
      "Epoch [2/1000], Loss: 0.5184, Val Loss: 0.4483\n",
      "Epoch [3/1000], Loss: 0.0958, Val Loss: 0.4503\n",
      "Epoch [4/1000], Loss: 2.0699, Val Loss: 0.4471\n",
      "Epoch [5/1000], Loss: 0.0181, Val Loss: 0.4591\n",
      "Epoch [6/1000], Loss: 0.0323, Val Loss: 0.4485\n",
      "Epoch [7/1000], Loss: 0.0045, Val Loss: 0.4490\n",
      "Epoch [8/1000], Loss: 0.0274, Val Loss: 0.4495\n",
      "Epoch [9/1000], Loss: 0.0607, Val Loss: 0.4487\n",
      "Epoch [10/1000], Loss: 0.0151, Val Loss: 0.4465\n",
      "Epoch [11/1000], Loss: 0.0244, Val Loss: 0.4462\n",
      "Epoch [12/1000], Loss: 0.3227, Val Loss: 0.4486\n",
      "Epoch [13/1000], Loss: 0.0862, Val Loss: 0.4474\n",
      "Epoch [14/1000], Loss: 0.3555, Val Loss: 0.4462\n",
      "Epoch [15/1000], Loss: 0.0160, Val Loss: 0.4465\n",
      "Epoch [16/1000], Loss: 0.0023, Val Loss: 0.4523\n",
      "Epoch [17/1000], Loss: 0.1632, Val Loss: 0.4563\n",
      "Epoch [18/1000], Loss: 0.0299, Val Loss: 0.4493\n",
      "Epoch [19/1000], Loss: 0.8780, Val Loss: 0.4494\n",
      "Epoch [20/1000], Loss: 0.3463, Val Loss: 0.4468\n",
      "Epoch [21/1000], Loss: 0.4484, Val Loss: 0.4461\n",
      "Epoch [22/1000], Loss: 0.3091, Val Loss: 0.4473\n",
      "Epoch [23/1000], Loss: 0.1747, Val Loss: 0.4454\n",
      "Epoch [24/1000], Loss: 0.1841, Val Loss: 0.4544\n",
      "Epoch [25/1000], Loss: 0.1498, Val Loss: 0.4452\n",
      "Epoch [26/1000], Loss: 0.7534, Val Loss: 0.4437\n",
      "Epoch [27/1000], Loss: 0.2198, Val Loss: 0.4498\n",
      "Epoch [28/1000], Loss: 0.1683, Val Loss: 0.4471\n",
      "Epoch [29/1000], Loss: 0.0048, Val Loss: 0.4564\n",
      "Epoch [30/1000], Loss: 0.0689, Val Loss: 0.4469\n",
      "Epoch [31/1000], Loss: 0.3038, Val Loss: 0.4472\n",
      "Epoch [32/1000], Loss: 0.5511, Val Loss: 0.4598\n",
      "Epoch [33/1000], Loss: 0.0942, Val Loss: 0.4488\n",
      "Epoch [34/1000], Loss: 0.0002, Val Loss: 0.4531\n",
      "Epoch [35/1000], Loss: 0.0244, Val Loss: 0.4469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:34:32,748] Trial 89 finished with value: 0.813509076912881 and parameters: {'hidden_size': 82, 'dropout_prob': 0.23863555951661236, 'learning_rate': 0.0025532696734938192, 'weight_decay': 7.677365441405606e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 36\n",
      "Epoch [1/1000], Loss: 0.5829, Val Loss: 0.4845\n",
      "Epoch [2/1000], Loss: 0.2076, Val Loss: 0.4686\n",
      "Epoch [3/1000], Loss: 0.1029, Val Loss: 0.4604\n",
      "Epoch [4/1000], Loss: 0.0080, Val Loss: 0.4554\n",
      "Epoch [5/1000], Loss: 0.4434, Val Loss: 0.4547\n",
      "Epoch [6/1000], Loss: 0.0392, Val Loss: 0.4524\n",
      "Epoch [7/1000], Loss: 0.0040, Val Loss: 0.4509\n",
      "Epoch [8/1000], Loss: 0.0689, Val Loss: 0.4514\n",
      "Epoch [9/1000], Loss: 0.3055, Val Loss: 0.4498\n",
      "Epoch [10/1000], Loss: 0.0747, Val Loss: 0.4503\n",
      "Epoch [11/1000], Loss: 0.3992, Val Loss: 0.4504\n",
      "Epoch [12/1000], Loss: 0.0140, Val Loss: 0.4502\n",
      "Epoch [13/1000], Loss: 0.0126, Val Loss: 0.4488\n",
      "Epoch [14/1000], Loss: 0.2642, Val Loss: 0.4496\n",
      "Epoch [15/1000], Loss: 1.2031, Val Loss: 0.4491\n",
      "Epoch [16/1000], Loss: 0.0050, Val Loss: 0.4512\n",
      "Epoch [17/1000], Loss: 0.0093, Val Loss: 0.4516\n",
      "Epoch [18/1000], Loss: 0.6593, Val Loss: 0.4473\n",
      "Epoch [19/1000], Loss: 0.4458, Val Loss: 0.4480\n",
      "Epoch [20/1000], Loss: 0.0193, Val Loss: 0.4486\n",
      "Epoch [21/1000], Loss: 0.3546, Val Loss: 0.4478\n",
      "Epoch [22/1000], Loss: 0.0182, Val Loss: 0.4471\n",
      "Epoch [23/1000], Loss: 0.2755, Val Loss: 0.4488\n",
      "Epoch [24/1000], Loss: 1.1272, Val Loss: 0.4484\n",
      "Epoch [25/1000], Loss: 0.0197, Val Loss: 0.4494\n",
      "Epoch [26/1000], Loss: 0.0089, Val Loss: 0.4509\n",
      "Epoch [27/1000], Loss: 0.3771, Val Loss: 0.4497\n",
      "Epoch [28/1000], Loss: 0.0007, Val Loss: 0.4478\n",
      "Epoch [29/1000], Loss: 0.7102, Val Loss: 0.4502\n",
      "Epoch [30/1000], Loss: 0.8696, Val Loss: 0.4499\n",
      "Epoch [31/1000], Loss: 3.6224, Val Loss: 0.4501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:34:41,987] Trial 90 finished with value: 0.8197234867988397 and parameters: {'hidden_size': 88, 'dropout_prob': 0.6760415144583136, 'learning_rate': 0.0013458777580876607, 'weight_decay': 8.616467196593016e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 32\n",
      "Epoch [1/1000], Loss: 1.4259, Val Loss: 0.5361\n",
      "Epoch [2/1000], Loss: 0.7614, Val Loss: 0.4933\n",
      "Epoch [3/1000], Loss: 0.1607, Val Loss: 0.4767\n",
      "Epoch [4/1000], Loss: 0.0145, Val Loss: 0.4686\n",
      "Epoch [5/1000], Loss: 0.7084, Val Loss: 0.4642\n",
      "Epoch [6/1000], Loss: 0.7032, Val Loss: 0.4609\n",
      "Epoch [7/1000], Loss: 0.5783, Val Loss: 0.4582\n",
      "Epoch [8/1000], Loss: 0.3412, Val Loss: 0.4569\n",
      "Epoch [9/1000], Loss: 0.0723, Val Loss: 0.4554\n",
      "Epoch [10/1000], Loss: 0.0046, Val Loss: 0.4550\n",
      "Epoch [11/1000], Loss: 0.5971, Val Loss: 0.4540\n",
      "Epoch [12/1000], Loss: 0.9813, Val Loss: 0.4536\n",
      "Epoch [13/1000], Loss: 0.3796, Val Loss: 0.4534\n",
      "Epoch [14/1000], Loss: 0.0140, Val Loss: 0.4535\n",
      "Epoch [15/1000], Loss: 4.2464, Val Loss: 0.4528\n",
      "Epoch [16/1000], Loss: 0.0991, Val Loss: 0.4524\n",
      "Epoch [17/1000], Loss: 0.2145, Val Loss: 0.4517\n",
      "Epoch [18/1000], Loss: 0.0020, Val Loss: 0.4511\n",
      "Epoch [19/1000], Loss: 0.4073, Val Loss: 0.4509\n",
      "Epoch [20/1000], Loss: 0.1393, Val Loss: 0.4511\n",
      "Epoch [21/1000], Loss: 0.5299, Val Loss: 0.4499\n",
      "Epoch [22/1000], Loss: 0.4759, Val Loss: 0.4490\n",
      "Epoch [23/1000], Loss: 0.3380, Val Loss: 0.4485\n",
      "Epoch [24/1000], Loss: 0.5090, Val Loss: 0.4483\n",
      "Epoch [25/1000], Loss: 0.0288, Val Loss: 0.4490\n",
      "Epoch [26/1000], Loss: 0.7936, Val Loss: 0.4489\n",
      "Epoch [27/1000], Loss: 0.0203, Val Loss: 0.4489\n",
      "Epoch [28/1000], Loss: 0.4572, Val Loss: 0.4481\n",
      "Epoch [29/1000], Loss: 0.3802, Val Loss: 0.4480\n",
      "Epoch [30/1000], Loss: 0.2769, Val Loss: 0.4478\n",
      "Epoch [31/1000], Loss: 0.5330, Val Loss: 0.4471\n",
      "Epoch [32/1000], Loss: 0.6533, Val Loss: 0.4482\n",
      "Epoch [33/1000], Loss: 0.7709, Val Loss: 0.4479\n",
      "Epoch [34/1000], Loss: 0.4973, Val Loss: 0.4471\n",
      "Epoch [35/1000], Loss: 0.0132, Val Loss: 0.4466\n",
      "Epoch [36/1000], Loss: 0.2920, Val Loss: 0.4473\n",
      "Epoch [37/1000], Loss: 0.6384, Val Loss: 0.4465\n",
      "Epoch [38/1000], Loss: 0.2796, Val Loss: 0.4466\n",
      "Epoch [39/1000], Loss: 0.0035, Val Loss: 0.4470\n",
      "Epoch [40/1000], Loss: 0.3847, Val Loss: 0.4469\n",
      "Epoch [41/1000], Loss: 0.0442, Val Loss: 0.4472\n",
      "Epoch [42/1000], Loss: 0.0297, Val Loss: 0.4472\n",
      "Epoch [43/1000], Loss: 0.0406, Val Loss: 0.4465\n",
      "Epoch [44/1000], Loss: 0.4877, Val Loss: 0.4480\n",
      "Epoch [45/1000], Loss: 0.0063, Val Loss: 0.4468\n",
      "Epoch [46/1000], Loss: 0.0561, Val Loss: 0.4469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:34:54,422] Trial 91 finished with value: 0.8220099206248056 and parameters: {'hidden_size': 104, 'dropout_prob': 0.7371225681559175, 'learning_rate': 0.0004209945032440873, 'weight_decay': 5.1001399190158266e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 47\n",
      "Epoch [1/1000], Loss: 0.0148, Val Loss: 0.4915\n",
      "Epoch [2/1000], Loss: 1.0484, Val Loss: 0.4716\n",
      "Epoch [3/1000], Loss: 0.0387, Val Loss: 0.4653\n",
      "Epoch [4/1000], Loss: 0.7144, Val Loss: 0.4629\n",
      "Epoch [5/1000], Loss: 0.0590, Val Loss: 0.4613\n",
      "Epoch [6/1000], Loss: 0.4387, Val Loss: 0.4573\n",
      "Epoch [7/1000], Loss: 0.5253, Val Loss: 0.4561\n",
      "Epoch [8/1000], Loss: 0.4165, Val Loss: 0.4542\n",
      "Epoch [9/1000], Loss: 0.0818, Val Loss: 0.4526\n",
      "Epoch [10/1000], Loss: 0.1121, Val Loss: 0.4530\n",
      "Epoch [11/1000], Loss: 0.2837, Val Loss: 0.4498\n",
      "Epoch [12/1000], Loss: 0.6590, Val Loss: 0.4491\n",
      "Epoch [13/1000], Loss: 0.0064, Val Loss: 0.4502\n",
      "Epoch [14/1000], Loss: 0.2583, Val Loss: 0.4483\n",
      "Epoch [15/1000], Loss: 0.5701, Val Loss: 0.4481\n",
      "Epoch [16/1000], Loss: 0.4535, Val Loss: 0.4496\n",
      "Epoch [17/1000], Loss: 0.5049, Val Loss: 0.4490\n",
      "Epoch [18/1000], Loss: 1.3866, Val Loss: 0.4477\n",
      "Epoch [19/1000], Loss: 0.0642, Val Loss: 0.4486\n",
      "Epoch [20/1000], Loss: 0.5678, Val Loss: 0.4487\n",
      "Epoch [21/1000], Loss: 0.3701, Val Loss: 0.4462\n",
      "Epoch [22/1000], Loss: 0.9852, Val Loss: 0.4465\n",
      "Epoch [23/1000], Loss: 0.2656, Val Loss: 0.4488\n",
      "Epoch [24/1000], Loss: 0.1024, Val Loss: 0.4470\n",
      "Epoch [25/1000], Loss: 0.3149, Val Loss: 0.4465\n",
      "Epoch [26/1000], Loss: 0.3845, Val Loss: 0.4464\n",
      "Epoch [27/1000], Loss: 0.4490, Val Loss: 0.4477\n",
      "Epoch [28/1000], Loss: 1.0277, Val Loss: 0.4458\n",
      "Epoch [29/1000], Loss: 0.2149, Val Loss: 0.4471\n",
      "Epoch [30/1000], Loss: 0.1345, Val Loss: 0.4446\n",
      "Epoch [31/1000], Loss: 0.0087, Val Loss: 0.4453\n",
      "Epoch [32/1000], Loss: 0.2606, Val Loss: 0.4472\n",
      "Epoch [33/1000], Loss: 0.3267, Val Loss: 0.4467\n",
      "Epoch [34/1000], Loss: 0.2486, Val Loss: 0.4455\n",
      "Epoch [35/1000], Loss: 0.0044, Val Loss: 0.4470\n",
      "Epoch [36/1000], Loss: 1.1775, Val Loss: 0.4456\n",
      "Epoch [37/1000], Loss: 1.2526, Val Loss: 0.4465\n",
      "Epoch [38/1000], Loss: 1.1255, Val Loss: 0.4464\n",
      "Epoch [39/1000], Loss: 0.2026, Val Loss: 0.4462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:35:05,310] Trial 92 finished with value: 0.8237228342603119 and parameters: {'hidden_size': 101, 'dropout_prob': 0.7142138196384304, 'learning_rate': 0.0006828760513031254, 'weight_decay': 4.4262513307918066e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 40\n",
      "Epoch [1/1000], Loss: 0.0122, Val Loss: 0.5073\n",
      "Epoch [2/1000], Loss: 0.5326, Val Loss: 0.4725\n",
      "Epoch [3/1000], Loss: 0.0494, Val Loss: 0.4641\n",
      "Epoch [4/1000], Loss: 0.0673, Val Loss: 0.4600\n",
      "Epoch [5/1000], Loss: 1.0408, Val Loss: 0.4561\n",
      "Epoch [6/1000], Loss: 0.2353, Val Loss: 0.4555\n",
      "Epoch [7/1000], Loss: 0.0232, Val Loss: 0.4534\n",
      "Epoch [8/1000], Loss: 0.7126, Val Loss: 0.4530\n",
      "Epoch [9/1000], Loss: 0.0495, Val Loss: 0.4506\n",
      "Epoch [10/1000], Loss: 0.3008, Val Loss: 0.4499\n",
      "Epoch [11/1000], Loss: 0.0580, Val Loss: 0.4503\n",
      "Epoch [12/1000], Loss: 0.0531, Val Loss: 0.4494\n",
      "Epoch [13/1000], Loss: 0.4258, Val Loss: 0.4505\n",
      "Epoch [14/1000], Loss: 0.3373, Val Loss: 0.4497\n",
      "Epoch [15/1000], Loss: 0.3846, Val Loss: 0.4506\n",
      "Epoch [16/1000], Loss: 0.0874, Val Loss: 0.4502\n",
      "Epoch [17/1000], Loss: 0.0055, Val Loss: 0.4501\n",
      "Epoch [18/1000], Loss: 0.0033, Val Loss: 0.4478\n",
      "Epoch [19/1000], Loss: 0.0791, Val Loss: 0.4487\n",
      "Epoch [20/1000], Loss: 0.5104, Val Loss: 0.4501\n",
      "Epoch [21/1000], Loss: 0.0350, Val Loss: 0.4499\n",
      "Epoch [22/1000], Loss: 0.2641, Val Loss: 0.4524\n",
      "Epoch [23/1000], Loss: 0.0987, Val Loss: 0.4498\n",
      "Epoch [24/1000], Loss: 0.4880, Val Loss: 0.4514\n",
      "Epoch [25/1000], Loss: 1.7492, Val Loss: 0.4493\n",
      "Epoch [26/1000], Loss: 0.0991, Val Loss: 0.4488\n",
      "Epoch [27/1000], Loss: 0.3683, Val Loss: 0.4492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:35:13,031] Trial 93 finished with value: 0.8214950269427042 and parameters: {'hidden_size': 53, 'dropout_prob': 0.7196321223405883, 'learning_rate': 0.0009065248281454171, 'weight_decay': 4.2563744295131664e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 28\n",
      "Epoch [1/1000], Loss: 0.4179, Val Loss: 0.5087\n",
      "Epoch [2/1000], Loss: 0.1575, Val Loss: 0.4757\n",
      "Epoch [3/1000], Loss: 0.8381, Val Loss: 0.4656\n",
      "Epoch [4/1000], Loss: 0.0162, Val Loss: 0.4608\n",
      "Epoch [5/1000], Loss: 0.2629, Val Loss: 0.4582\n",
      "Epoch [6/1000], Loss: 0.4758, Val Loss: 0.4567\n",
      "Epoch [7/1000], Loss: 0.0004, Val Loss: 0.4545\n",
      "Epoch [8/1000], Loss: 0.1587, Val Loss: 0.4528\n",
      "Epoch [9/1000], Loss: 0.0679, Val Loss: 0.4514\n",
      "Epoch [10/1000], Loss: 0.4457, Val Loss: 0.4505\n",
      "Epoch [11/1000], Loss: 0.7576, Val Loss: 0.4491\n",
      "Epoch [12/1000], Loss: 0.0443, Val Loss: 0.4500\n",
      "Epoch [13/1000], Loss: 0.3274, Val Loss: 0.4499\n",
      "Epoch [14/1000], Loss: 0.1765, Val Loss: 0.4495\n",
      "Epoch [15/1000], Loss: 0.8610, Val Loss: 0.4490\n",
      "Epoch [16/1000], Loss: 0.1062, Val Loss: 0.4498\n",
      "Epoch [17/1000], Loss: 0.2366, Val Loss: 0.4497\n",
      "Epoch [18/1000], Loss: 0.0998, Val Loss: 0.4486\n",
      "Epoch [19/1000], Loss: 0.0062, Val Loss: 0.4491\n",
      "Epoch [20/1000], Loss: 0.1652, Val Loss: 0.4484\n",
      "Epoch [21/1000], Loss: 0.1927, Val Loss: 0.4481\n",
      "Epoch [22/1000], Loss: 0.3500, Val Loss: 0.4487\n",
      "Epoch [23/1000], Loss: 0.1231, Val Loss: 0.4484\n",
      "Epoch [24/1000], Loss: 0.1847, Val Loss: 0.4490\n",
      "Epoch [25/1000], Loss: 1.2278, Val Loss: 0.4494\n",
      "Epoch [26/1000], Loss: 0.0005, Val Loss: 0.4499\n",
      "Epoch [27/1000], Loss: 0.3289, Val Loss: 0.4494\n",
      "Epoch [28/1000], Loss: 0.0318, Val Loss: 0.4503\n",
      "Epoch [29/1000], Loss: 0.7999, Val Loss: 0.4500\n",
      "Epoch [30/1000], Loss: 0.5718, Val Loss: 0.4495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:35:22,013] Trial 94 finished with value: 0.8209928781537239 and parameters: {'hidden_size': 108, 'dropout_prob': 0.784866608299629, 'learning_rate': 0.0006669804024667346, 'weight_decay': 3.9500026168343864e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 31\n",
      "Epoch [1/1000], Loss: 0.1076, Val Loss: 0.4666\n",
      "Epoch [2/1000], Loss: 0.0472, Val Loss: 0.4569\n",
      "Epoch [3/1000], Loss: 2.3638, Val Loss: 0.4552\n",
      "Epoch [4/1000], Loss: 0.0330, Val Loss: 0.4540\n",
      "Epoch [5/1000], Loss: 0.0604, Val Loss: 0.4503\n",
      "Epoch [6/1000], Loss: 0.6696, Val Loss: 0.4512\n",
      "Epoch [7/1000], Loss: 0.0128, Val Loss: 0.4526\n",
      "Epoch [8/1000], Loss: 1.5345, Val Loss: 0.4509\n",
      "Epoch [9/1000], Loss: 0.0822, Val Loss: 0.4527\n",
      "Epoch [10/1000], Loss: 0.0024, Val Loss: 0.4512\n",
      "Epoch [11/1000], Loss: 0.0956, Val Loss: 0.4510\n",
      "Epoch [12/1000], Loss: 0.1998, Val Loss: 0.4522\n",
      "Epoch [13/1000], Loss: 0.2499, Val Loss: 0.4502\n",
      "Epoch [14/1000], Loss: 0.4608, Val Loss: 0.4486\n",
      "Epoch [15/1000], Loss: 0.5238, Val Loss: 0.4582\n",
      "Epoch [16/1000], Loss: 0.0643, Val Loss: 0.4536\n",
      "Epoch [17/1000], Loss: 1.5686, Val Loss: 0.4544\n",
      "Epoch [18/1000], Loss: 0.1745, Val Loss: 0.4544\n",
      "Epoch [19/1000], Loss: 0.0150, Val Loss: 0.4489\n",
      "Epoch [20/1000], Loss: 0.2268, Val Loss: 0.4496\n",
      "Epoch [21/1000], Loss: 0.0932, Val Loss: 0.4529\n",
      "Epoch [22/1000], Loss: 0.0179, Val Loss: 0.4556\n",
      "Epoch [23/1000], Loss: 0.6860, Val Loss: 0.4524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:35:29,262] Trial 95 finished with value: 0.8193921195776851 and parameters: {'hidden_size': 98, 'dropout_prob': 0.7605426055268227, 'learning_rate': 0.0031900978570436854, 'weight_decay': 7.21302006606187e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 24\n",
      "Epoch [1/1000], Loss: 0.3956, Val Loss: 0.5713\n",
      "Epoch [2/1000], Loss: 0.4859, Val Loss: 0.5086\n",
      "Epoch [3/1000], Loss: 0.0871, Val Loss: 0.4910\n",
      "Epoch [4/1000], Loss: 0.2418, Val Loss: 0.4794\n",
      "Epoch [5/1000], Loss: 0.0860, Val Loss: 0.4718\n",
      "Epoch [6/1000], Loss: 0.9825, Val Loss: 0.4675\n",
      "Epoch [7/1000], Loss: 0.6104, Val Loss: 0.4647\n",
      "Epoch [8/1000], Loss: 0.0402, Val Loss: 0.4620\n",
      "Epoch [9/1000], Loss: 0.1691, Val Loss: 0.4602\n",
      "Epoch [10/1000], Loss: 0.4127, Val Loss: 0.4587\n",
      "Epoch [11/1000], Loss: 0.2643, Val Loss: 0.4567\n",
      "Epoch [12/1000], Loss: 0.5026, Val Loss: 0.4560\n",
      "Epoch [13/1000], Loss: 0.7085, Val Loss: 0.4552\n",
      "Epoch [14/1000], Loss: 0.0595, Val Loss: 0.4549\n",
      "Epoch [15/1000], Loss: 3.0173, Val Loss: 0.4539\n",
      "Epoch [16/1000], Loss: 0.0184, Val Loss: 0.4541\n",
      "Epoch [17/1000], Loss: 0.6306, Val Loss: 0.4539\n",
      "Epoch [18/1000], Loss: 0.9310, Val Loss: 0.4531\n",
      "Epoch [19/1000], Loss: 0.0351, Val Loss: 0.4529\n",
      "Epoch [20/1000], Loss: 0.5882, Val Loss: 0.4530\n",
      "Epoch [21/1000], Loss: 0.0460, Val Loss: 0.4527\n",
      "Epoch [22/1000], Loss: 0.5188, Val Loss: 0.4524\n",
      "Epoch [23/1000], Loss: 0.0011, Val Loss: 0.4521\n",
      "Epoch [24/1000], Loss: 0.0349, Val Loss: 0.4515\n",
      "Epoch [25/1000], Loss: 0.1843, Val Loss: 0.4515\n",
      "Epoch [26/1000], Loss: 0.9877, Val Loss: 0.4507\n",
      "Epoch [27/1000], Loss: 0.4001, Val Loss: 0.4513\n",
      "Epoch [28/1000], Loss: 0.0717, Val Loss: 0.4505\n",
      "Epoch [29/1000], Loss: 0.7265, Val Loss: 0.4505\n",
      "Epoch [30/1000], Loss: 0.7469, Val Loss: 0.4502\n",
      "Epoch [31/1000], Loss: 0.2041, Val Loss: 0.4504\n",
      "Epoch [32/1000], Loss: 0.9888, Val Loss: 0.4504\n",
      "Epoch [33/1000], Loss: 0.4411, Val Loss: 0.4504\n",
      "Epoch [34/1000], Loss: 1.1746, Val Loss: 0.4496\n",
      "Epoch [35/1000], Loss: 0.1296, Val Loss: 0.4499\n",
      "Epoch [36/1000], Loss: 0.0859, Val Loss: 0.4500\n",
      "Epoch [37/1000], Loss: 0.9251, Val Loss: 0.4504\n",
      "Epoch [38/1000], Loss: 0.1796, Val Loss: 0.4507\n",
      "Epoch [39/1000], Loss: 0.2102, Val Loss: 0.4507\n",
      "Epoch [40/1000], Loss: 0.0192, Val Loss: 0.4509\n",
      "Epoch [41/1000], Loss: 0.0305, Val Loss: 0.4509\n",
      "Epoch [42/1000], Loss: 0.0206, Val Loss: 0.4507\n",
      "Epoch [43/1000], Loss: 0.0659, Val Loss: 0.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:35:42,022] Trial 96 finished with value: 0.8192417298388537 and parameters: {'hidden_size': 95, 'dropout_prob': 0.6526179262553902, 'learning_rate': 0.0002695334599349213, 'weight_decay': 4.489563402055263e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 44\n",
      "Epoch [1/1000], Loss: 0.4862, Val Loss: 0.4742\n",
      "Epoch [2/1000], Loss: 0.7653, Val Loss: 0.4631\n",
      "Epoch [3/1000], Loss: 0.7817, Val Loss: 0.4665\n",
      "Epoch [4/1000], Loss: 0.0046, Val Loss: 0.4595\n",
      "Epoch [5/1000], Loss: 0.1226, Val Loss: 0.4561\n",
      "Epoch [6/1000], Loss: 0.0227, Val Loss: 0.4567\n",
      "Epoch [7/1000], Loss: 0.1060, Val Loss: 0.4554\n",
      "Epoch [8/1000], Loss: 0.1366, Val Loss: 0.4547\n",
      "Epoch [9/1000], Loss: 0.0857, Val Loss: 0.4531\n",
      "Epoch [10/1000], Loss: 0.0346, Val Loss: 0.4497\n",
      "Epoch [11/1000], Loss: 0.8970, Val Loss: 0.4486\n",
      "Epoch [12/1000], Loss: 0.8104, Val Loss: 0.4476\n",
      "Epoch [13/1000], Loss: 0.0697, Val Loss: 0.4473\n",
      "Epoch [14/1000], Loss: 0.2623, Val Loss: 0.4486\n",
      "Epoch [15/1000], Loss: 0.0062, Val Loss: 0.4468\n",
      "Epoch [16/1000], Loss: 0.5356, Val Loss: 0.4545\n",
      "Epoch [17/1000], Loss: 0.6023, Val Loss: 0.4503\n",
      "Epoch [18/1000], Loss: 0.0266, Val Loss: 0.4503\n",
      "Epoch [19/1000], Loss: 0.0019, Val Loss: 0.4520\n",
      "Epoch [20/1000], Loss: 0.1182, Val Loss: 0.4521\n",
      "Epoch [21/1000], Loss: 0.0769, Val Loss: 0.4484\n",
      "Epoch [22/1000], Loss: 0.3844, Val Loss: 0.4491\n",
      "Epoch [23/1000], Loss: 0.8336, Val Loss: 0.4500\n",
      "Epoch [24/1000], Loss: 0.1621, Val Loss: 0.4497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:35:49,150] Trial 97 finished with value: 0.8214644391992127 and parameters: {'hidden_size': 39, 'dropout_prob': 0.6153442845557915, 'learning_rate': 0.0017469835822999835, 'weight_decay': 3.526369107104548e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 25\n",
      "Epoch [1/1000], Loss: 0.0076, Val Loss: 0.4821\n",
      "Epoch [2/1000], Loss: 0.6755, Val Loss: 0.4655\n",
      "Epoch [3/1000], Loss: 0.8320, Val Loss: 0.4569\n",
      "Epoch [4/1000], Loss: 0.1168, Val Loss: 0.4540\n",
      "Epoch [5/1000], Loss: 0.0131, Val Loss: 0.4533\n",
      "Epoch [6/1000], Loss: 0.0216, Val Loss: 0.4491\n",
      "Epoch [7/1000], Loss: 0.2765, Val Loss: 0.4493\n",
      "Epoch [8/1000], Loss: 0.4768, Val Loss: 0.4495\n",
      "Epoch [9/1000], Loss: 0.0027, Val Loss: 0.4491\n",
      "Epoch [10/1000], Loss: 0.6750, Val Loss: 0.4467\n",
      "Epoch [11/1000], Loss: 0.7674, Val Loss: 0.4462\n",
      "Epoch [12/1000], Loss: 0.0034, Val Loss: 0.4474\n",
      "Epoch [13/1000], Loss: 0.4991, Val Loss: 0.4479\n",
      "Epoch [14/1000], Loss: 0.2933, Val Loss: 0.4488\n",
      "Epoch [15/1000], Loss: 0.3854, Val Loss: 0.4482\n",
      "Epoch [16/1000], Loss: 0.0008, Val Loss: 0.4472\n",
      "Epoch [17/1000], Loss: 1.4311, Val Loss: 0.4467\n",
      "Epoch [18/1000], Loss: 1.0851, Val Loss: 0.4474\n",
      "Epoch [19/1000], Loss: 0.2526, Val Loss: 0.4480\n",
      "Epoch [20/1000], Loss: 0.0380, Val Loss: 0.4481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:35:54,783] Trial 98 finished with value: 0.822494226563416 and parameters: {'hidden_size': 88, 'dropout_prob': 0.7073360525292793, 'learning_rate': 0.0011860700587656334, 'weight_decay': 9.284073263322623e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 21\n",
      "Epoch [1/1000], Loss: 0.6297, Val Loss: 0.6711\n",
      "Epoch [2/1000], Loss: 0.7132, Val Loss: 0.6638\n",
      "Epoch [3/1000], Loss: 0.5871, Val Loss: 0.6575\n",
      "Epoch [4/1000], Loss: 0.6258, Val Loss: 0.6513\n",
      "Epoch [5/1000], Loss: 0.6343, Val Loss: 0.6450\n",
      "Epoch [6/1000], Loss: 0.7828, Val Loss: 0.6370\n",
      "Epoch [7/1000], Loss: 0.4826, Val Loss: 0.6270\n",
      "Epoch [8/1000], Loss: 0.5885, Val Loss: 0.6157\n",
      "Epoch [9/1000], Loss: 0.8120, Val Loss: 0.6033\n",
      "Epoch [10/1000], Loss: 0.3679, Val Loss: 0.5906\n",
      "Epoch [11/1000], Loss: 0.3810, Val Loss: 0.5767\n",
      "Epoch [12/1000], Loss: 0.4076, Val Loss: 0.5647\n",
      "Epoch [13/1000], Loss: 0.3479, Val Loss: 0.5538\n",
      "Epoch [14/1000], Loss: 0.4350, Val Loss: 0.5446\n",
      "Epoch [15/1000], Loss: 0.3975, Val Loss: 0.5368\n",
      "Epoch [16/1000], Loss: 1.0721, Val Loss: 0.5305\n",
      "Epoch [17/1000], Loss: 0.4345, Val Loss: 0.5249\n",
      "Epoch [18/1000], Loss: 0.2824, Val Loss: 0.5201\n",
      "Epoch [19/1000], Loss: 1.3860, Val Loss: 0.5159\n",
      "Epoch [20/1000], Loss: 0.1763, Val Loss: 0.5130\n",
      "Epoch [21/1000], Loss: 1.0390, Val Loss: 0.5098\n",
      "Epoch [22/1000], Loss: 0.2032, Val Loss: 0.5072\n",
      "Epoch [23/1000], Loss: 0.2672, Val Loss: 0.5047\n",
      "Epoch [24/1000], Loss: 0.2837, Val Loss: 0.5024\n",
      "Epoch [25/1000], Loss: 0.1834, Val Loss: 0.5005\n",
      "Epoch [26/1000], Loss: 0.2294, Val Loss: 0.4986\n",
      "Epoch [27/1000], Loss: 0.1483, Val Loss: 0.4970\n",
      "Epoch [28/1000], Loss: 0.0818, Val Loss: 0.4954\n",
      "Epoch [29/1000], Loss: 1.0910, Val Loss: 0.4939\n",
      "Epoch [30/1000], Loss: 0.3317, Val Loss: 0.4924\n",
      "Epoch [31/1000], Loss: 0.0631, Val Loss: 0.4909\n",
      "Epoch [32/1000], Loss: 0.3366, Val Loss: 0.4894\n",
      "Epoch [33/1000], Loss: 0.3719, Val Loss: 0.4878\n",
      "Epoch [34/1000], Loss: 0.4276, Val Loss: 0.4864\n",
      "Epoch [35/1000], Loss: 0.5516, Val Loss: 0.4851\n",
      "Epoch [36/1000], Loss: 0.5398, Val Loss: 0.4842\n",
      "Epoch [37/1000], Loss: 0.8847, Val Loss: 0.4830\n",
      "Epoch [38/1000], Loss: 0.2662, Val Loss: 0.4821\n",
      "Epoch [39/1000], Loss: 0.2961, Val Loss: 0.4812\n",
      "Epoch [40/1000], Loss: 0.4172, Val Loss: 0.4802\n",
      "Epoch [41/1000], Loss: 0.2123, Val Loss: 0.4793\n",
      "Epoch [42/1000], Loss: 0.0551, Val Loss: 0.4783\n",
      "Epoch [43/1000], Loss: 0.2312, Val Loss: 0.4774\n",
      "Epoch [44/1000], Loss: 0.2992, Val Loss: 0.4766\n",
      "Epoch [45/1000], Loss: 3.1594, Val Loss: 0.4758\n",
      "Epoch [46/1000], Loss: 0.1654, Val Loss: 0.4753\n",
      "Epoch [47/1000], Loss: 0.1673, Val Loss: 0.4747\n",
      "Epoch [48/1000], Loss: 0.1962, Val Loss: 0.4742\n",
      "Epoch [49/1000], Loss: 0.1913, Val Loss: 0.4736\n",
      "Epoch [50/1000], Loss: 0.0188, Val Loss: 0.4729\n",
      "Epoch [51/1000], Loss: 1.2337, Val Loss: 0.4724\n",
      "Epoch [52/1000], Loss: 1.0348, Val Loss: 0.4717\n",
      "Epoch [53/1000], Loss: 0.0593, Val Loss: 0.4712\n",
      "Epoch [54/1000], Loss: 0.0790, Val Loss: 0.4708\n",
      "Epoch [55/1000], Loss: 0.2643, Val Loss: 0.4705\n",
      "Epoch [56/1000], Loss: 0.0709, Val Loss: 0.4700\n",
      "Epoch [57/1000], Loss: 0.5404, Val Loss: 0.4695\n",
      "Epoch [58/1000], Loss: 0.1145, Val Loss: 0.4691\n",
      "Epoch [59/1000], Loss: 1.3106, Val Loss: 0.4686\n",
      "Epoch [60/1000], Loss: 0.7897, Val Loss: 0.4685\n",
      "Epoch [61/1000], Loss: 0.0110, Val Loss: 0.4681\n",
      "Epoch [62/1000], Loss: 0.8753, Val Loss: 0.4678\n",
      "Epoch [63/1000], Loss: 0.8529, Val Loss: 0.4675\n",
      "Epoch [64/1000], Loss: 0.6261, Val Loss: 0.4673\n",
      "Epoch [65/1000], Loss: 0.4738, Val Loss: 0.4671\n",
      "Epoch [66/1000], Loss: 0.1047, Val Loss: 0.4668\n",
      "Epoch [67/1000], Loss: 0.6988, Val Loss: 0.4665\n",
      "Epoch [68/1000], Loss: 0.6064, Val Loss: 0.4663\n",
      "Epoch [69/1000], Loss: 0.6668, Val Loss: 0.4659\n",
      "Epoch [70/1000], Loss: 0.8926, Val Loss: 0.4657\n",
      "Epoch [71/1000], Loss: 0.1157, Val Loss: 0.4657\n",
      "Epoch [72/1000], Loss: 0.1759, Val Loss: 0.4654\n",
      "Epoch [73/1000], Loss: 2.4106, Val Loss: 0.4652\n",
      "Epoch [74/1000], Loss: 0.0561, Val Loss: 0.4651\n",
      "Epoch [75/1000], Loss: 0.0476, Val Loss: 0.4650\n",
      "Epoch [76/1000], Loss: 0.2851, Val Loss: 0.4647\n",
      "Epoch [77/1000], Loss: 0.5942, Val Loss: 0.4644\n",
      "Epoch [78/1000], Loss: 0.0503, Val Loss: 0.4643\n",
      "Epoch [79/1000], Loss: 0.6472, Val Loss: 0.4642\n",
      "Epoch [80/1000], Loss: 0.4362, Val Loss: 0.4640\n",
      "Epoch [81/1000], Loss: 0.0472, Val Loss: 0.4638\n",
      "Epoch [82/1000], Loss: 0.3593, Val Loss: 0.4637\n",
      "Epoch [83/1000], Loss: 0.0858, Val Loss: 0.4635\n",
      "Epoch [84/1000], Loss: 0.5622, Val Loss: 0.4634\n",
      "Epoch [85/1000], Loss: 0.3429, Val Loss: 0.4633\n",
      "Epoch [86/1000], Loss: 0.3119, Val Loss: 0.4632\n",
      "Epoch [87/1000], Loss: 0.9920, Val Loss: 0.4629\n",
      "Epoch [88/1000], Loss: 0.4693, Val Loss: 0.4628\n",
      "Epoch [89/1000], Loss: 0.4076, Val Loss: 0.4626\n",
      "Epoch [90/1000], Loss: 0.2639, Val Loss: 0.4625\n",
      "Epoch [91/1000], Loss: 0.0738, Val Loss: 0.4624\n",
      "Epoch [92/1000], Loss: 0.1811, Val Loss: 0.4622\n",
      "Epoch [93/1000], Loss: 0.0472, Val Loss: 0.4621\n",
      "Epoch [94/1000], Loss: 0.4572, Val Loss: 0.4619\n",
      "Epoch [95/1000], Loss: 0.3627, Val Loss: 0.4618\n",
      "Epoch [96/1000], Loss: 0.1438, Val Loss: 0.4616\n",
      "Epoch [97/1000], Loss: 0.6261, Val Loss: 0.4615\n",
      "Epoch [98/1000], Loss: 0.0973, Val Loss: 0.4615\n",
      "Epoch [99/1000], Loss: 1.1072, Val Loss: 0.4613\n",
      "Epoch [100/1000], Loss: 0.5007, Val Loss: 0.4614\n",
      "Epoch [101/1000], Loss: 0.7757, Val Loss: 0.4612\n",
      "Epoch [102/1000], Loss: 0.5343, Val Loss: 0.4610\n",
      "Epoch [103/1000], Loss: 0.0510, Val Loss: 0.4610\n",
      "Epoch [104/1000], Loss: 0.0616, Val Loss: 0.4609\n",
      "Epoch [105/1000], Loss: 0.0492, Val Loss: 0.4607\n",
      "Epoch [106/1000], Loss: 0.7263, Val Loss: 0.4606\n",
      "Epoch [107/1000], Loss: 0.0233, Val Loss: 0.4605\n",
      "Epoch [108/1000], Loss: 0.1613, Val Loss: 0.4604\n",
      "Epoch [109/1000], Loss: 0.7358, Val Loss: 0.4602\n",
      "Epoch [110/1000], Loss: 0.0796, Val Loss: 0.4602\n",
      "Epoch [111/1000], Loss: 0.3381, Val Loss: 0.4601\n",
      "Epoch [112/1000], Loss: 0.7746, Val Loss: 0.4600\n",
      "Epoch [113/1000], Loss: 0.0350, Val Loss: 0.4599\n",
      "Epoch [114/1000], Loss: 0.7626, Val Loss: 0.4597\n",
      "Epoch [115/1000], Loss: 0.1492, Val Loss: 0.4597\n",
      "Epoch [116/1000], Loss: 0.0422, Val Loss: 0.4596\n",
      "Epoch [117/1000], Loss: 0.1936, Val Loss: 0.4596\n",
      "Epoch [118/1000], Loss: 0.3540, Val Loss: 0.4595\n",
      "Epoch [119/1000], Loss: 0.7669, Val Loss: 0.4593\n",
      "Epoch [120/1000], Loss: 0.1481, Val Loss: 0.4593\n",
      "Epoch [121/1000], Loss: 0.7353, Val Loss: 0.4591\n",
      "Epoch [122/1000], Loss: 0.3917, Val Loss: 0.4590\n",
      "Epoch [123/1000], Loss: 0.7187, Val Loss: 0.4589\n",
      "Epoch [124/1000], Loss: 0.1213, Val Loss: 0.4589\n",
      "Epoch [125/1000], Loss: 0.2557, Val Loss: 0.4587\n",
      "Epoch [126/1000], Loss: 1.1136, Val Loss: 0.4586\n",
      "Epoch [127/1000], Loss: 0.9259, Val Loss: 0.4586\n",
      "Epoch [128/1000], Loss: 1.7889, Val Loss: 0.4586\n",
      "Epoch [129/1000], Loss: 0.2770, Val Loss: 0.4585\n",
      "Epoch [130/1000], Loss: 0.0038, Val Loss: 0.4583\n",
      "Epoch [131/1000], Loss: 0.8802, Val Loss: 0.4583\n",
      "Epoch [132/1000], Loss: 0.4608, Val Loss: 0.4582\n",
      "Epoch [133/1000], Loss: 0.3099, Val Loss: 0.4581\n",
      "Epoch [134/1000], Loss: 0.0158, Val Loss: 0.4580\n",
      "Epoch [135/1000], Loss: 1.4174, Val Loss: 0.4579\n",
      "Epoch [136/1000], Loss: 0.1000, Val Loss: 0.4579\n",
      "Epoch [137/1000], Loss: 0.1080, Val Loss: 0.4578\n",
      "Epoch [138/1000], Loss: 0.1486, Val Loss: 0.4577\n",
      "Epoch [139/1000], Loss: 0.7673, Val Loss: 0.4576\n",
      "Epoch [140/1000], Loss: 0.0575, Val Loss: 0.4576\n",
      "Epoch [141/1000], Loss: 0.9907, Val Loss: 0.4575\n",
      "Epoch [142/1000], Loss: 0.0172, Val Loss: 0.4574\n",
      "Epoch [143/1000], Loss: 0.0012, Val Loss: 0.4574\n",
      "Epoch [144/1000], Loss: 0.2209, Val Loss: 0.4573\n",
      "Epoch [145/1000], Loss: 1.0458, Val Loss: 0.4572\n",
      "Epoch [146/1000], Loss: 0.0027, Val Loss: 0.4571\n",
      "Epoch [147/1000], Loss: 0.2566, Val Loss: 0.4570\n",
      "Epoch [148/1000], Loss: 0.0500, Val Loss: 0.4569\n",
      "Epoch [149/1000], Loss: 0.0026, Val Loss: 0.4568\n",
      "Epoch [150/1000], Loss: 0.4912, Val Loss: 0.4567\n",
      "Epoch [151/1000], Loss: 0.7453, Val Loss: 0.4566\n",
      "Epoch [152/1000], Loss: 0.4843, Val Loss: 0.4565\n",
      "Epoch [153/1000], Loss: 0.5365, Val Loss: 0.4565\n",
      "Epoch [154/1000], Loss: 0.7757, Val Loss: 0.4565\n",
      "Epoch [155/1000], Loss: 1.5064, Val Loss: 0.4564\n",
      "Epoch [156/1000], Loss: 0.0728, Val Loss: 0.4564\n",
      "Epoch [157/1000], Loss: 0.7714, Val Loss: 0.4564\n",
      "Epoch [158/1000], Loss: 0.0110, Val Loss: 0.4563\n",
      "Epoch [159/1000], Loss: 0.1185, Val Loss: 0.4563\n",
      "Epoch [160/1000], Loss: 0.3736, Val Loss: 0.4563\n",
      "Epoch [161/1000], Loss: 0.0400, Val Loss: 0.4562\n",
      "Epoch [162/1000], Loss: 0.0116, Val Loss: 0.4561\n",
      "Epoch [163/1000], Loss: 0.7463, Val Loss: 0.4561\n",
      "Epoch [164/1000], Loss: 0.2581, Val Loss: 0.4562\n",
      "Epoch [165/1000], Loss: 0.2958, Val Loss: 0.4561\n",
      "Epoch [166/1000], Loss: 0.5574, Val Loss: 0.4560\n",
      "Epoch [167/1000], Loss: 0.0105, Val Loss: 0.4559\n",
      "Epoch [168/1000], Loss: 0.3195, Val Loss: 0.4559\n",
      "Epoch [169/1000], Loss: 0.6078, Val Loss: 0.4558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 0.5840, Val Loss: 0.4557\n",
      "Epoch [171/1000], Loss: 0.0666, Val Loss: 0.4557\n",
      "Epoch [172/1000], Loss: 0.8474, Val Loss: 0.4556\n",
      "Epoch [173/1000], Loss: 1.0415, Val Loss: 0.4555\n",
      "Epoch [174/1000], Loss: 0.8064, Val Loss: 0.4554\n",
      "Epoch [175/1000], Loss: 0.2522, Val Loss: 0.4554\n",
      "Epoch [176/1000], Loss: 0.7318, Val Loss: 0.4552\n",
      "Epoch [177/1000], Loss: 0.0190, Val Loss: 0.4552\n",
      "Epoch [178/1000], Loss: 0.1768, Val Loss: 0.4551\n",
      "Epoch [179/1000], Loss: 0.1314, Val Loss: 0.4550\n",
      "Epoch [180/1000], Loss: 0.0506, Val Loss: 0.4549\n",
      "Epoch [181/1000], Loss: 0.3285, Val Loss: 0.4548\n",
      "Epoch [182/1000], Loss: 0.0077, Val Loss: 0.4548\n",
      "Epoch [183/1000], Loss: 0.8577, Val Loss: 0.4548\n",
      "Epoch [184/1000], Loss: 0.0060, Val Loss: 0.4548\n",
      "Epoch [185/1000], Loss: 0.6845, Val Loss: 0.4548\n",
      "Epoch [186/1000], Loss: 0.7466, Val Loss: 0.4548\n",
      "Epoch [187/1000], Loss: 0.4715, Val Loss: 0.4548\n",
      "Epoch [188/1000], Loss: 0.0337, Val Loss: 0.4547\n",
      "Epoch [189/1000], Loss: 0.4959, Val Loss: 0.4547\n",
      "Epoch [190/1000], Loss: 0.0709, Val Loss: 0.4546\n",
      "Epoch [191/1000], Loss: 0.8931, Val Loss: 0.4545\n",
      "Epoch [192/1000], Loss: 1.1154, Val Loss: 0.4545\n",
      "Epoch [193/1000], Loss: 0.4998, Val Loss: 0.4544\n",
      "Epoch [194/1000], Loss: 1.0070, Val Loss: 0.4543\n",
      "Epoch [195/1000], Loss: 0.0656, Val Loss: 0.4543\n",
      "Epoch [196/1000], Loss: 0.1390, Val Loss: 0.4543\n",
      "Epoch [197/1000], Loss: 0.4618, Val Loss: 0.4542\n",
      "Epoch [198/1000], Loss: 0.0388, Val Loss: 0.4541\n",
      "Epoch [199/1000], Loss: 0.5034, Val Loss: 0.4541\n",
      "Epoch [200/1000], Loss: 1.1697, Val Loss: 0.4540\n",
      "Epoch [201/1000], Loss: 0.0029, Val Loss: 0.4540\n",
      "Epoch [202/1000], Loss: 1.1285, Val Loss: 0.4539\n",
      "Epoch [203/1000], Loss: 0.5702, Val Loss: 0.4538\n",
      "Epoch [204/1000], Loss: 0.7746, Val Loss: 0.4537\n",
      "Epoch [205/1000], Loss: 0.1292, Val Loss: 0.4537\n",
      "Epoch [206/1000], Loss: 0.0435, Val Loss: 0.4536\n",
      "Epoch [207/1000], Loss: 0.4707, Val Loss: 0.4536\n",
      "Epoch [208/1000], Loss: 1.8458, Val Loss: 0.4536\n",
      "Epoch [209/1000], Loss: 0.0951, Val Loss: 0.4535\n",
      "Epoch [210/1000], Loss: 0.4298, Val Loss: 0.4535\n",
      "Epoch [211/1000], Loss: 0.3594, Val Loss: 0.4535\n",
      "Epoch [212/1000], Loss: 0.4013, Val Loss: 0.4534\n",
      "Epoch [213/1000], Loss: 0.6745, Val Loss: 0.4533\n",
      "Epoch [214/1000], Loss: 1.2617, Val Loss: 0.4533\n",
      "Epoch [215/1000], Loss: 0.5228, Val Loss: 0.4533\n",
      "Epoch [216/1000], Loss: 0.1689, Val Loss: 0.4533\n",
      "Epoch [217/1000], Loss: 0.7047, Val Loss: 0.4532\n",
      "Epoch [218/1000], Loss: 1.2922, Val Loss: 0.4532\n",
      "Epoch [219/1000], Loss: 0.1445, Val Loss: 0.4531\n",
      "Epoch [220/1000], Loss: 0.0740, Val Loss: 0.4531\n",
      "Epoch [221/1000], Loss: 0.0286, Val Loss: 0.4531\n",
      "Epoch [222/1000], Loss: 0.0241, Val Loss: 0.4530\n",
      "Epoch [223/1000], Loss: 1.8779, Val Loss: 0.4530\n",
      "Epoch [224/1000], Loss: 0.9157, Val Loss: 0.4530\n",
      "Epoch [225/1000], Loss: 0.4097, Val Loss: 0.4530\n",
      "Epoch [226/1000], Loss: 0.4862, Val Loss: 0.4529\n",
      "Epoch [227/1000], Loss: 0.1222, Val Loss: 0.4529\n",
      "Epoch [228/1000], Loss: 0.0141, Val Loss: 0.4529\n",
      "Epoch [229/1000], Loss: 0.7812, Val Loss: 0.4528\n",
      "Epoch [230/1000], Loss: 0.6307, Val Loss: 0.4528\n",
      "Epoch [231/1000], Loss: 0.1413, Val Loss: 0.4527\n",
      "Epoch [232/1000], Loss: 0.6643, Val Loss: 0.4527\n",
      "Epoch [233/1000], Loss: 0.1230, Val Loss: 0.4527\n",
      "Epoch [234/1000], Loss: 0.0007, Val Loss: 0.4526\n",
      "Epoch [235/1000], Loss: 0.3242, Val Loss: 0.4526\n",
      "Epoch [236/1000], Loss: 0.7938, Val Loss: 0.4525\n",
      "Epoch [237/1000], Loss: 0.5597, Val Loss: 0.4526\n",
      "Epoch [238/1000], Loss: 0.4750, Val Loss: 0.4526\n",
      "Epoch [239/1000], Loss: 0.0051, Val Loss: 0.4524\n",
      "Epoch [240/1000], Loss: 0.1045, Val Loss: 0.4524\n",
      "Epoch [241/1000], Loss: 0.0292, Val Loss: 0.4524\n",
      "Epoch [242/1000], Loss: 0.3183, Val Loss: 0.4523\n",
      "Epoch [243/1000], Loss: 0.8867, Val Loss: 0.4523\n",
      "Epoch [244/1000], Loss: 0.5097, Val Loss: 0.4523\n",
      "Epoch [245/1000], Loss: 0.4715, Val Loss: 0.4522\n",
      "Epoch [246/1000], Loss: 0.2630, Val Loss: 0.4522\n",
      "Epoch [247/1000], Loss: 0.5144, Val Loss: 0.4522\n",
      "Epoch [248/1000], Loss: 0.0388, Val Loss: 0.4522\n",
      "Epoch [249/1000], Loss: 0.4861, Val Loss: 0.4522\n",
      "Epoch [250/1000], Loss: 0.0362, Val Loss: 0.4522\n",
      "Epoch [251/1000], Loss: 0.1710, Val Loss: 0.4522\n",
      "Epoch [252/1000], Loss: 0.7985, Val Loss: 0.4521\n",
      "Epoch [253/1000], Loss: 0.0380, Val Loss: 0.4520\n",
      "Epoch [254/1000], Loss: 0.1741, Val Loss: 0.4520\n",
      "Epoch [255/1000], Loss: 0.0432, Val Loss: 0.4520\n",
      "Epoch [256/1000], Loss: 0.1348, Val Loss: 0.4520\n",
      "Epoch [257/1000], Loss: 0.3655, Val Loss: 0.4520\n",
      "Epoch [258/1000], Loss: 0.3511, Val Loss: 0.4519\n",
      "Epoch [259/1000], Loss: 1.2702, Val Loss: 0.4519\n",
      "Epoch [260/1000], Loss: 0.2174, Val Loss: 0.4519\n",
      "Epoch [261/1000], Loss: 0.0901, Val Loss: 0.4519\n",
      "Epoch [262/1000], Loss: 0.4829, Val Loss: 0.4519\n",
      "Epoch [263/1000], Loss: 0.0989, Val Loss: 0.4519\n",
      "Epoch [264/1000], Loss: 2.2859, Val Loss: 0.4518\n",
      "Epoch [265/1000], Loss: 0.1668, Val Loss: 0.4519\n",
      "Epoch [266/1000], Loss: 0.1434, Val Loss: 0.4518\n",
      "Epoch [267/1000], Loss: 0.7860, Val Loss: 0.4518\n",
      "Epoch [268/1000], Loss: 0.0822, Val Loss: 0.4518\n",
      "Epoch [269/1000], Loss: 0.2481, Val Loss: 0.4518\n",
      "Epoch [270/1000], Loss: 0.0108, Val Loss: 0.4518\n",
      "Epoch [271/1000], Loss: 0.3352, Val Loss: 0.4517\n",
      "Epoch [272/1000], Loss: 0.6284, Val Loss: 0.4517\n",
      "Epoch [273/1000], Loss: 0.2776, Val Loss: 0.4516\n",
      "Epoch [274/1000], Loss: 1.0729, Val Loss: 0.4516\n",
      "Epoch [275/1000], Loss: 0.7888, Val Loss: 0.4517\n",
      "Epoch [276/1000], Loss: 1.4219, Val Loss: 0.4517\n",
      "Epoch [277/1000], Loss: 0.3348, Val Loss: 0.4518\n",
      "Epoch [278/1000], Loss: 0.4831, Val Loss: 0.4518\n",
      "Epoch [279/1000], Loss: 0.0216, Val Loss: 0.4518\n",
      "Epoch [280/1000], Loss: 0.3277, Val Loss: 0.4517\n",
      "Epoch [281/1000], Loss: 0.2103, Val Loss: 0.4517\n",
      "Epoch [282/1000], Loss: 0.4166, Val Loss: 0.4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-16 18:37:11,622] Trial 99 finished with value: 0.8187701687933644 and parameters: {'hidden_size': 46, 'dropout_prob': 0.741514149256248, 'learning_rate': 3.4675354798151744e-05, 'weight_decay': 8.039186475334685e-05}. Best is trial 31 with value: 0.8248367379191159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 283\n",
      "Best Hyperparameters: {'hidden_size': 80, 'dropout_prob': 0.6637967701094895, 'learning_rate': 0.0015734040867304835, 'weight_decay': 5.296599401924851e-05}\n",
      "Best ROC-AUC Score: 0.8248367379191159\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_prob):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size, 32)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "def objective(trial: Trial):\n",
    "    # Предложим гиперпараметры\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128)  # Отрегулируйте диапазон по мере необходимости\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.2, 0.8)  # Отрегулируйте диапазон по мере необходимости\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2)  # Отрегулируйте диапазон по мере необходимости\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4)  # Отрегулируйте диапазон по мере необходимости\n",
    "\n",
    "    # Определим свою модель нейронной сети с помощью предложенных гиперпараметров.\n",
    "    model = NeuralNetwork(X_train.shape[1], hidden_size, dropout_prob).to(device)\n",
    "\n",
    "    # Определим оптимизатор с предлагаемой скоростью обучения и затуханием веса.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Определите критерий (например, BCELoss) вне нашей целевой функции.\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Определим параметры ранней остановки\n",
    "    early_stopping_patience = 10\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Тренировочный цикл\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Потеря валидации и преждевременная остановка\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val_tensor.to(device))\n",
    "            val_loss = criterion(y_val_pred, y_val_tensor.to(device).unsqueeze(1))\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == early_stopping_patience:\n",
    "                print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Рассчитаем оценку ROC-AUC для данных валидации.\n",
    "    y_val_pred = y_val_pred.cpu().numpy()\n",
    "    val_roc_auc = roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "    # Вернем оценку ROC-AUC валидации в качестве цели оптимизации.\n",
    "    return val_roc_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")  # Мы хотим максимизировать ROC-AUC\n",
    "study.optimize(objective, n_trials=100)  # При необходимости отрегулируйте количество испытаний.\n",
    "\n",
    "best_params = study.best_params\n",
    "best_roc_auc = study.best_value\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best ROC-AUC Score: {best_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже, что мы успешно использовали Optuna, чтобы найти лучшие гиперпараметры для нашей модели нейронной сети. Лучшие гиперпараметры, которые мы получили, следующие:\n",
    "\n",
    "- **'hidden_size'**: 80\n",
    "- **'dropout_prob'**: 0,6638\n",
    "- **'learning_rate'**: 0,0016\n",
    "- **'weight_decay'**: 5.2966e-05\n",
    "\n",
    "Соответствующий показатель **ROC-AUC** для этих гиперпараметров составляет **примерно 0.8248**, что является хорошим результатом.\n",
    "\n",
    "Теперь можно приступить к обучению окончательной модели нейронной сети, используя эти лучшие гиперпараметры для всего набора обучающих данных, и оценить ее производительность на тестовом наборе данных, чтобы проверить, соответствует ли она указанному порогу ROC-AUC, и вычислить точность, как указано в описании задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.3800\n",
      "Epoch [2/1000], Loss: 0.0413\n",
      "Epoch [3/1000], Loss: 0.0042\n",
      "Epoch [4/1000], Loss: 0.5129\n",
      "Epoch [5/1000], Loss: 0.2139\n",
      "Epoch [6/1000], Loss: 1.1003\n",
      "Epoch [7/1000], Loss: 0.4040\n",
      "Epoch [8/1000], Loss: 0.2807\n",
      "Epoch [9/1000], Loss: 0.5898\n",
      "Epoch [10/1000], Loss: 0.0200\n",
      "Epoch [11/1000], Loss: 0.5932\n",
      "Epoch [12/1000], Loss: 1.1318\n",
      "Epoch [13/1000], Loss: 0.0225\n",
      "Epoch [14/1000], Loss: 0.1930\n",
      "Epoch [15/1000], Loss: 1.7447\n",
      "Epoch [16/1000], Loss: 0.0816\n",
      "Epoch [17/1000], Loss: 0.0055\n",
      "Epoch [18/1000], Loss: 0.5638\n",
      "Epoch [19/1000], Loss: 0.3498\n",
      "Epoch [20/1000], Loss: 0.8685\n",
      "Epoch [21/1000], Loss: 0.1814\n",
      "Epoch [22/1000], Loss: 0.0671\n",
      "Epoch [23/1000], Loss: 0.5709\n",
      "Early stopping triggered at epoch 23\n",
      "\n",
      "Test ROC-AUC Score: 0.8582\n",
      "Test Accuracy: 0.8226\n"
     ]
    }
   ],
   "source": [
    "# Объединим наборы данных обучения и проверки\n",
    "X_train_val = torch.cat((X_train_tensor, X_val_tensor), dim=0)\n",
    "y_train_val = torch.cat((y_train_tensor, y_val_tensor), dim=0)\n",
    "\n",
    "# Используем лучшие гиперпараметры, полученные от Optuna.\n",
    "best_hidden_size = 80\n",
    "best_dropout_prob = 0.6638\n",
    "best_learning_rate = 0.0016\n",
    "best_weight_decay = 5.2966e-05\n",
    "\n",
    "# Определим свою модель нейронной сети с лучшими гиперпараметрами\n",
    "final_model = NeuralNetwork(X_train.shape[1], best_hidden_size, best_dropout_prob).to(device)\n",
    "\n",
    "# Определим оптимизатор с лучшей скоростью обучения и снижением веса\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "\n",
    "# Определим критерий (например, BCELoss) вне цикла обучения.\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Параметры ранней остановки\n",
    "early_stopping_patience = 10\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Цикл обучения на комбинированном наборе данных обучения и проверки\n",
    "num_epochs = 1000  # Можно настроить количество эпох по мере необходимости\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = final_model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # После каждой эпохи можно распечатать или записать потери обучения, если это необходимо.\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Потеря валидации и ранняя остановка\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = final_model(X_val_tensor.to(device))\n",
    "        val_loss = criterion(y_val_pred, y_val_tensor.to(device).unsqueeze(1))\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Оценка Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC Score: 0.8582\n",
      "Test Accuracy: 0.8226\n"
     ]
    }
   ],
   "source": [
    "# Оценим окончательную модель на тестовом наборе данных.\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = final_model(X_test_tensor.to(device))\n",
    "    y_test_pred = y_test_pred.cpu().numpy()\n",
    "    test_roc_score = roc_auc_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Вычисление точности (при условии, что пороговое значение равно 0,5)\n",
    "    y_test_pred_binary = (y_test_pred >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_test_pred_binary)\n",
    "\n",
    "print(f\"Test ROC-AUC Score: {test_roc_score:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Целевая проектная метрика достигнута!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Преобразование предсказанных вероятностей в двоичные предсказания\n",
    "y_test_pred_binary = (y_test_pred >= 0.5).astype(int)\n",
    "\n",
    "# Вычислим матрицу путаницы для двоичных предсказаний\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred_binary)\n",
    "\n",
    "# Вычислим точность\n",
    "accuracy = accuracy_score(y_test, y_test_pred_binary)\n",
    "\n",
    "# Визуализируем матрицу путаницы\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, square=True, annot_kws={\"size\": 12})\n",
    "plt.xlabel('Спрогнозированное значение')\n",
    "plt.ylabel('Фактическое значение')\n",
    "plt.title('Матрица ошибок')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На представленном изображении показана матрица путаницы (ошибок), полученная на основе модели классификации. Вот интерпретация:\n",
    "\n",
    "Матрица путаницы:\n",
    "- Матрица показывает эффективность модели классификации на тестовых данных.\n",
    "- Есть два класса, которые мы можем принять за «0» (отрицательный класс, клиент не ушел) и «1» (положительный класс, клиент ушел).\n",
    "- Верхний левый квадрат (темно-синий) с номером 970 представляет собой **Истинные негативы (TN)**: случаи, когда модель правильно предсказала отрицательный класс.\n",
    "- Правый нижний квадрат (голубой) с номером 189 представляет собой **Истинные положительные результаты (TP)**: случаи, когда модель правильно предсказала положительный класс.\n",
    "- Верхний правый квадрат (средний синий) с номером 88 представляет собой **Ложные положительные результаты (FP)**: случаи, когда модель неправильно предсказала положительный класс.\n",
    "\n",
    "Для нашей модели:\n",
    "\n",
    "- **Истинно отрицательные числа (TN):** 970 – модель правильно предсказала отрицательный класс (клиенты, которые не ушли).\n",
    "- **Ложно-отрицательные результаты (FN):** 162 – Модель неправильно предсказала отрицательный класс (клиенты, которые ушли, но модель предсказывала, что этого не произойдет).\n",
    "- **Истинные положительные результаты (TP):** 189 – Модель правильно предсказала положительный класс (ушедших клиентов).\n",
    "- **Ложные положительные результаты (FP):** 88. Модель неправильно предсказала положительный класс (клиенты, которые не ушли, но модель предсказала, что это произойдет).\n",
    "\n",
    "**На основе матрицы ошибок сделаем следующие выводы:**\n",
    "\n",
    "- Модель имеет тенденцию иметь больше ложноотрицательных результатов, чем ложноположительных, что может указывать на консервативный прогноз для положительного класса (например, она более осторожна в прогнозировании оттока).\n",
    "- Число истинных положительных результатов относительно невелико по сравнению с истинными отрицательными, что может свидетельствовать о том, что модель лучше идентифицирует отрицательный класс или о наличии дисбаланса классов.\n",
    "\n",
    "Чтобы лучше понять производительность модели, также следует взглянуть на кривую precision-recall и кривую ROC, особенно в такой области, как прогнозирование оттока, где стоимость ложноотрицательных результатов может быть выше, чем ложноположительных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Вычислим Precision-Recall кривую\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_test_pred)\n",
    "\n",
    "# Построим кривую Precision-Recall\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.title('Кривая Precision-Recall')\n",
    "plt.xlabel('Полнота')\n",
    "plt.ylabel('Точность')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На изображении показана кривая Precision-Recall для нашей модели классификации. Кривая Precision-Recall — это график, который отображает компромисс между точностью (ось Y) и полнотой (ось X) для различных порогов вероятности.\n",
    "\n",
    "Вот как интерпретировать кривую Precision-Recall на графике:\n",
    "\n",
    "- Кривая начинается с высокой точностью при низких уровнях полноты, что указывает на то, что, когда модель настроена на очень избирательную работу (предсказывает мало положительных результатов), она очень точна в этих прогнозах.\n",
    "- По мере увеличения полноты точность постепенно снижается. Это ожидаемо, потому что, поскольку модель пытается охватить больше положительных случаев (увеличение полноты), она также с большей вероятностью неправильно пометит некоторые отрицательные случаи как положительные (снижение точности).\n",
    "- Кривая не касается правого верхнего угла, что соответствует идеальной модели (100% точность и полнота). Тем не менее, похоже, что модель сохраняет относительно высокую точность для уровней полноты примерно до 0,6, что является хорошим признаком эффективности модели, особенно в контексте несбалансированного набора данных, что типично для сценариев прогнозирования оттока.\n",
    "\n",
    "На изображении показана кривая точности-отзыва для вашей модели классификации. Кривая точности-отзыва — это график, который отображает компромисс между точностью (ось Y) и полнотой отзыва (ось X) для различных порогов вероятности.\n",
    "\n",
    "Вот как интерпретировать кривую точности-отзыва на графике:\n",
    "\n",
    "- Кривая начинается с высокой точностью при низких уровнях полноты, что указывает на то, что, когда модель настроена на очень избирательную работу (предсказывает мало положительных результатов), она очень точна в этих прогнозах.\n",
    "- По мере увеличения полноты точность постепенно снижается. Это ожидаемо, потому что, поскольку модель пытается охватить больше положительных случаев (увеличение отзыва), она также с большей вероятностью неправильно пометит некоторые отрицательные случаи как положительные (снижение точности).\n",
    "- Кривая не касается правого верхнего угла, что соответствует идеальной модели (100% точность и полнота). Тем не менее, похоже, что модель сохраняет относительно высокую точность для уровней отзыва примерно до 0,6, что является хорошим признаком эффективности модели, особенно в контексте несбалансированного набора данных, что типично для сценариев прогнозирования оттока, где количество ушедших клиентов может быть намного меньше, чем не ушедших клиентов. Более высокая площадь под кривой Precision-Recall указывает на лучшую модель.\n",
    "\n",
    "Тестовый показатель ROC-AUC составляет 0.86, что показывает, что модель обладает хорошей способностью различать классы. Объединив это с кривой Precision-Recall, можно сказать что модель, по-видимому, работает хорошо не только в целом, но и в отношении баланса между точностью и полнотой, что может иметь решающее значение для бизнес-решений, основанных на правильном выявлении оттока.\n",
    "\n",
    "Для принятия действенных бизнес-решений можно учитывать стоимость ложных положительных результатов (предложение скидок клиентам, которые не собираются уходить) и ложных негативов (упущение возможности удержать уходящего клиента). В зависимости от этих затрат можно отдать приоритет точности над полнотой или наоборот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
